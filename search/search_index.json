{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to Chango Private!"},{"location":"components/components/","title":"Chango Private Components","text":""},{"location":"components/components/#components","title":"Components","text":"Component Version Apache Ozone 1.3.0 Kafka 3.4.x Zookeeper 3.6.4 Schema Registry 7.4.0 Spark 3.4.0 / Scala 2.12 Trino 422 Iceberg 1.3.0 Open JDK 8 / 11 / 17 Azkaban 3.69.0-8 Azkaban CLI 0.9.14 MySQL 5.7.42-1 Prometheus 2.45.0 Superset 0.38.1 dbt 0.21.0 PostgreSQL 9.2.24 Redis 6.2.12 Chango Private Admin 2.0.0 Chango Private Streaming 2.0.0 Chango Private Data API 2.0.0 Chango Private Trino Gateway 2.0.0 Chango Private Authorizer 2.0.0 Chango Private REST Catalog 2.0.0"},{"location":"examples/example/","title":"Home","text":"<p>New Version For full documentation visit mkdocs.org.</p>"},{"location":"examples/example/#code-blocks","title":"Code Blocks","text":""},{"location":"examples/example/#code-blocks-example","title":"Code Blocks Example","text":"<pre><code>     public String callGetRestAPI(String url) throws IOException {\nLOG.info(\"ready to call [{}]\", url);\nRequest request = new Request.Builder()\n.url(url)\n.addHeader(\"Authorization\", \"Bearer \" + accessToken)\n.build();\nRestResponse restResponse = ResponseHandler.doCall(simpleHttpClient.getClient(), request);\nif(restResponse.getStatusCode() == STATUS_OK) {\nreturn restResponse.getSuccessMessage();\n} else {\nLOG.error(\"response for [{}]: {}\", url, restResponse.getErrorMessage());\nthrow new RuntimeException(restResponse.getErrorMessage());\n}\n}\n</code></pre>"},{"location":"examples/example/#code-blocks-example2","title":"Code Blocks Example2","text":"AnyClass.java<pre><code>     private static SparkStreamingInstance streamingContext(String metastoreUrl, String bucket, String accessKey, String secretKey, String endpoint, String region) {\nSparkConf sparkConf = new SparkConf().setAppName(\"disruptor streaming context\");\nsparkConf.setMaster(\"local[2]\");\nSparkSession sparkSession = (spark == null) ? SparkSession\n.builder()\n.config(sparkConf)\n.enableHiveSupport()\n.getOrCreate() : spark;\nspark = sparkSession;\n// iceberg catalog from hive metastore.\nsparkConf = sparkSession.sparkContext().conf();\nsparkConf.set(\"spark.sql.catalog.hive_prod\", \"org.apache.iceberg.spark.SparkCatalog\");\nsparkConf.set(\"spark.sql.catalog.hive_prod.type\", \"hive\");\nsparkConf.set(\"spark.sql.catalog.hive_prod.uri\", \"thrift://\" + metastoreUrl);\nsparkConf.set(\"spark.sql.catalog.hive_prod.warehouse\", \"s3a://\" + bucket + \"/warehouse\");\nJavaStreamingContext scc = new JavaStreamingContext(JavaSparkContext.fromSparkContext(spark.sparkContext()), new Duration(1000));\norg.apache.hadoop.conf.Configuration hadoopConfiguration = scc.sparkContext().hadoopConfiguration();\nhadoopConfiguration.set(\"fs.defaultFS\", \"s3a://\" + bucket);\nhadoopConfiguration.set(\"fs.s3a.endpoint.region\", region);\nhadoopConfiguration.set(\"fs.s3a.endpoint\", endpoint);\nhadoopConfiguration.set(\"fs.s3a.access.key\", accessKey);\nhadoopConfiguration.set(\"fs.s3a.secret.key\", secretKey);\nhadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\");\nhadoopConfiguration.set(\"fs.s3a.change.detection.mode\", \"warn\");\nhadoopConfiguration.set(\"fs.s3a.change.detection.version.required\", \"false\");\nhadoopConfiguration.set(\"fs.s3a.multiobjectdelete.enable\", \"true\");\nhadoopConfiguration.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\");\nhadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\");\nhadoopConfiguration.set(\"hive.metastore.uris\", \"thrift://\" + metastoreUrl);\nhadoopConfiguration.set(\"hive.server2.enable.doAs\", \"false\");\nhadoopConfiguration.set(\"hive.metastore.client.socket.timeout\", \"1800\");\nhadoopConfiguration.set(\"hive.execution.engine\", \"spark\");\nreturn new SparkStreamingInstance(scc, sparkSession);\n}\n}\n</code></pre>"},{"location":"examples/example/#code-blocks-example3","title":"Code Blocks Example3","text":"<pre><code>     public String callGetRestAPI(String url) throws IOException {\nLOG.info(\"ready to call [{}]\", url);\nRequest request = new Request.Builder()\n.url(url)\n.addHeader(\"Authorization\", \"Bearer \" + accessToken)\n.build();\nRestResponse restResponse = ResponseHandler.doCall(simpleHttpClient.getClient(), request);\nif(restResponse.getStatusCode() == STATUS_OK) {\nreturn restResponse.getSuccessMessage();\n} else {\nLOG.error(\"response for [{}]: {}\", url, restResponse.getErrorMessage());\nthrow new RuntimeException(restResponse.getErrorMessage());\n}\n}\n</code></pre>"},{"location":"features/ingestion/","title":"Chango Private Data Ingestion","text":"<p>External data sources like CSV, JSON, Excel can be inserted into iceberg tables in chango directly.</p>"},{"location":"features/ingestion/#insert-external-data-sources-to-chango","title":"Insert External Data Sources to Chango","text":"<p>As data analytics engineer, you don\u2019t have to struggle with long row Excel to analyze data. SQL is better to analyze data.  External data like CSV, Excel, JSON can be inserted directly to iceberg table in chango,  then you can explore and analyze iceberg tables with trino queries.</p>"},{"location":"features/rbac/","title":"Chango Private Storage Security","text":"<p>Fine-grained data access control using RBAC to chango storage.</p>"},{"location":"features/rbac/#overview","title":"Overview","text":"<p>There are several ways to access data in chango storage. For example,</p> <ul> <li>External streaming applications want to produce streaming events to chango.</li> <li>Spark jobs want to read / write data in iceberg tables in chango.</li> <li>Trino wants to read / write data in iceberg tables in chango.</li> </ul> <p>Such data accesses to chango need to be controlled by Chango Storage RBAC. With Chango Storage RBAC, all data accesses will be controlled in the fine-grained manner like catalog level, schema level and table level. </p>"},{"location":"features/rbac/#credential-role-and-privileges","title":"Credential, Role and Privileges","text":"<p>A role can have many credentials and many privileges. There are <code>READ</code> and <code>WRITE</code> type in privilege.  Each privilege has storage access path with the convention of <code>&lt;catalog&gt;</code>.<code>&lt;schema&gt;</code>.<code>&lt;table&gt;</code>.</p> <ul> <li><code>iceberg.events.behavior</code> with <code>WRITE</code> : user / credential has the <code>WRITE</code> privilege to table behavior in <code>events</code> schema of <code>iceberg</code> catalog.</li> <li><code>iceberg.events.*</code> with <code>READ</code>: user / credential has the <code>READ</code> privilege to all the tables in <code>events</code> schema of <code>iceberg</code> catalog.</li> <li><code>mysql.*</code> with <code>READ</code>: user / credential has the <code>READ</code> privilege to all the tables in all schemas of <code>mysql</code> catalog.</li> <li><code>*</code> with <code>WRITE</code>: user / credential has the <code>WRITE</code> privilege to all the tables in all schemas of all catalogs.</li> </ul>"},{"location":"features/streaming/","title":"Chango Private Streaming","text":"<p>External event streaming application can insert streaming events into iceberg tables in chango without building streaming platform and writing streaming jobs.</p>"},{"location":"features/streaming/#no-streaming-platform-no-streaming-jobs","title":"No Streaming Platform, No Streaming Jobs","text":"<p>If you want to insert streaming events like user behavior events, logs, IoT events to data lakehouses, you need to build event streaming platform like kafka and write streaming jobs like spark streaming jobs in most cases. But in chango, you don't have to do so. </p> <p></p> <p>External streaming application can insert streaming events to iceberg tables in chango directly without streaming platform and streaming jobs.</p>"},{"location":"features/trino-gw/","title":"Chango Private Trino Gateway","text":"<p>Trino queries will be routed to the backend trino clusters by Chango Private Trino Gateway dynamically.</p>"},{"location":"features/trino-gw/#understand-trino-gateway-concept","title":"Understand Trino Gateway Concept","text":"<p>Chango has the concept of trino gateway which routes trino queries to upstream backend trino clusters dynamically.  If one of the backend trino clusters has exhausted, then trino gateway will route queries to the trino cluster  which is executing less requested queries. Trino does not support HA because trino coordinator has single point failure.  In order to support HA of trino, we need to use trino gateway.</p> <p>Let\u2019s say, there is only one large trino clusters(100 ~ 1000 workers) in the company. Many people like BI experts, data scientists,  and data engineers are running trino queries on this large cluster intensively.  These trino queries can be interactive or ETL. For example, because long running ETL queries have occupied most of the resources  which trino cluster needs to execute another queries, there can be little resources remaining trino cluster can use  to execute another interactive queries. The people who are running the interactive queries need to wait  until the long running ETL queries will finish. Such conflict problems can also happen in reverse case.</p> <p>Such monolithic approach with large trino workers can lead to be problematic. We need to separate a large trino cluster  to small trino clusters for the groups like BI team, data scientist team, and data engineer team individually.</p> <p>Let\u2019s say, BI team has 3 backeend trino clusters. If one of trino clusters needs to be scaled out, and trino catalogs needs  to be added to this trino cluster for new external data sources, or the trino cluster needs to be reinstalled with new trino version,  then  first just deactivate this trino cluster to which the queries will not be routed without down time problem of trino query execution.  After finishing scaling workers ,updating catalogs or reinstalling the trino cluster, activate this trino cluster to which the queries  will be routed again. With trino gateway, the activation and deactivation of backend trino clusters can be done with ease. </p>"},{"location":"intro/intro/","title":"Introduction","text":""},{"location":"intro/intro/#what-is-chango-private","title":"What is Chango Private?","text":"<p>Chango Private is a Data Lakehouse Platform for offline / disconnected environment.  All the data will be saved into iceberg tables on Ozone as s3 compatible object storage.</p>"},{"location":"intro/intro/#chango-private-data-lakehouse-platform","title":"Chango Private Data Lakehouse Platform","text":"<p>Chango Private provides most popular open source tools for data lake and data lakehouse with chango private specific tools.</p> <ul> <li><code>Ingestion</code>: spark and trino are used to insert external data sources to iceberg in apache ozone object storage. Kafka is used for external streaming events which will be inserted into iceberg in ozone.</li> <li><code>Storage</code>: apache ozone is used as s3 compatible object storage on which iceberg tables will be created.</li> <li><code>Transformation</code>: spark and trino are used to execute etl jobs which will read iceberg tables and write data to iceberg tables on ozone.</li> <li><code>Workflow</code>: azkaban orchestrates all data pipelines in ingestion and transformation layers.</li> <li><code>Analytics</code>: superset is used as bi tool with connecting to trino directly or through chango private trino gateway. Trino is used to read all iceberg tables on ozone.</li> </ul> <p>Chango Private also provides the following features.</p> <ul> <li><code>Chango Private Storage Security</code>: RBAC to control fine-grained data access(catalog, schema and table level control) to chango storage. </li> <li><code>Chango Private Trino Gateway</code>: chango private trino gateway routes trino queries to upstream backend trino clusters dynamically.</li> <li><code>Chango Private Streaming</code>: streaming events are saved to iceberg tables without streaming platform and streaming job.</li> <li><code>Chango Private Data Ingestion</code>: external data like csv, json and excel are inserted into iceberg tables with ease.</li> </ul>"}]}