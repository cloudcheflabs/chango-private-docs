{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to Chango Private! <p>       Chango Private is Data Lakehouse Platform for both online and disconnected environment.     </p> <p>       Let's get started with Chango Private.     </p> <p>       Join Chango Discussion to discuss Chango Private.     </p>"},{"location":"components/components/","title":"Chango Private Components","text":""},{"location":"components/components/#component-version","title":"Component Version","text":"Component Version Apache Ozone 1.3.0 Kafka 3.4.x Zookeeper 3.6.4 Schema Registry 7.4.0 Spark 3.4.0 / Scala 2.12 Trino 422 Iceberg 1.3.0 Open JDK 8 / 11 / 17 Azkaban 3.69.0-8 Azkaban CLI 0.9.14 Superset 0.38.1 dbt dbt-core: 1.2.1, dbt-trino: 1.2.2 Chango Private Admin 2.1.1 Chango Private Streaming 2.1.1 Chango Private Data API 2.1.1 Chango Private Trino Gateway 2.1.1 Chango Private Authorizer 2.1.1 Chango Private REST Catalog 2.1.1"},{"location":"components/components/#component-ports","title":"Component Ports","text":""},{"location":"components/components/#azkaban","title":"Azkaban","text":"<ul> <li>28081: Azkaban Web Server</li> <li>28082: Azkaban Executor Server</li> </ul>"},{"location":"components/components/#kafka","title":"Kafka","text":"<ul> <li>9092: Kafka Server</li> </ul>"},{"location":"components/components/#schema-registry","title":"Schema Registry","text":"<ul> <li>8081: Registry Server</li> </ul>"},{"location":"components/components/#mysql","title":"MySQL","text":"<ul> <li>3306: MySQL Server</li> </ul>"},{"location":"components/components/#ozone","title":"Ozone","text":"<ul> <li>9862: OM RPC</li> <li>9872: OM Ratis</li> <li>9874: OM HTTP</li> <li>9875: OM HTTPS</li> <li>9861: SCM Data Node</li> <li>9863: SCM Block Client</li> <li>9860: SCM Client</li> <li>9876: SCM HTTP</li> <li>9877: SCM HTTPS</li> <li>9894: SCM Ratis</li> <li>9895: SCM GRPC</li> <li>9961: SCM Security Service</li> <li>9882: Data Node HTTP</li> <li>9883: Data Node HTTPS</li> <li>9858: Container Ratis IPC</li> <li>9859: Container IPC</li> <li>9864: Data Node Client</li> <li>9855: Container Ratis Data Stream</li> <li>9857: Container Ratis Admin</li> <li>9856: Container Ratis Server</li> <li>9878: S3G HTTP</li> <li>9879: S3G HTTPS</li> <li>9891: Recon RPC</li> <li>9888: Recon HTTP</li> <li>9889: Recon HTTPS</li> <li>9884: Freon HTTP</li> <li>9885: Freon HTTPS</li> <li>88: KDC Server</li> <li>749: KDC Admin Server</li> <li>80: NGINX</li> </ul>"},{"location":"components/components/#spark","title":"Spark","text":"<ul> <li>7777: Master Server</li> <li>7778: Worker Server</li> <li>8880: Master Web UI</li> <li>8881: Worker Web UI</li> <li>18882: History Server</li> </ul>"},{"location":"components/components/#superset","title":"Superset","text":"<ul> <li>38088: Superset Server</li> </ul>"},{"location":"components/components/#trino","title":"Trino","text":"<ul> <li>18080: Coordinator </li> <li>18081: Worker</li> </ul>"},{"location":"components/components/#zookeeper","title":"Zookeeper","text":"<ul> <li>2181: Zookeeper Server </li> <li>2888: Zookeeper Peer Port </li> <li>3888: Zookeeper Leader Port</li> </ul>"},{"location":"components/components/#postgresql","title":"PostgreSQL","text":"<ul> <li>5432: PostgreSQL Server</li> </ul>"},{"location":"components/components/#chango-private-admin","title":"Chango Private Admin","text":"<ul> <li>28095: Admin Server </li> <li>8080: NGINX</li> </ul>"},{"location":"components/components/#chango-private-admin-ui","title":"Chango Private Admin UI","text":"<ul> <li>8123: Admin UI Server</li> </ul>"},{"location":"components/components/#chango-private-data-api","title":"Chango Private Data API","text":"<ul> <li>28097: Server </li> <li>28098: Jetty Server </li> <li>27072: Management Server </li> <li>80: NGINX</li> </ul>"},{"location":"components/components/#chango-private-trino-gateway","title":"Chango Private Trino Gateway","text":"<ul> <li>28096: Server </li> <li>27081: Management Server </li> <li>27788: Jetty Proxy Server </li> <li>6379: Redis </li> <li>443: NGINX SSL</li> </ul>"},{"location":"components/components/#chango-private-authorizer","title":"Chango Private Authorizer","text":"<ul> <li>28196: Server </li> <li>27181: Management Server </li> <li>81: NGINX</li> </ul>"},{"location":"components/components/#chango-private-rest-catalog","title":"Chango Private REST Catalog","text":"<ul> <li>28197: Server </li> <li>27182: Management Server </li> <li>28297: Jetty Proxy Server </li> <li>8008: NGINX</li> </ul>"},{"location":"features/ingestion/","title":"Chango Data Ingestion","text":"<p>Chango Private provides components like <code>Chango Data API</code> and <code>Chango Streaming</code> to insert external data to Chango with ease.</p>"},{"location":"features/ingestion/#upload-files","title":"Upload Files","text":"<p>As data analytics engineer, you don\u2019t have to struggle with long row Excel to analyze data. SQL is better to analyze data.  External data like CSV, Excel, JSON can be inserted directly to iceberg table in chango,  then you can explore and analyze iceberg tables with trino queries.</p>"},{"location":"features/ingestion/#streaming","title":"Streaming","text":"<p>External event streaming application can insert streaming events into iceberg tables in chango without building streaming platform and writing streaming jobs.</p>"},{"location":"features/ingestion/#no-streaming-platform-no-streaming-jobs","title":"No Streaming Platform, No Streaming Jobs","text":"<p>If you want to insert streaming events like user behavior events, logs, IoT events to data lakehouses, you need to build event streaming platform like kafka and write streaming jobs like spark streaming jobs in most cases. But in chango, you don't have to do so.</p> <p></p> <p>External streaming application can insert streaming events to iceberg tables in chango directly without streaming platform and streaming jobs.</p>"},{"location":"features/rbac/","title":"Chango Storage Security","text":"<p>Chango Private provides fine-grained data access control using RBAC to Chango storage.</p>"},{"location":"features/rbac/#data-access-in-secure-way","title":"Data Access in Secure Way","text":"<p>Chango Authorizer controls all the data access to Chango Data Lakehouse.  So all the chango components which want to access data in Chango Data Lakehouse need to be authenticated and authorized by Chango Authorizer.</p> <p>All data accesses are controlled in the fine-grained manner like catalog, schema and table level. </p>"},{"location":"features/rbac/#credential-role-and-privileges","title":"Credential, Role and Privileges","text":"<p>A <code>Role</code> can have many <code>Credentials</code> and many <code>Privileges</code>. There are <code>READ</code> and <code>WRITE</code> type in privilege.  Each privilege has storage access path with the convention of <code>&lt;catalog&gt;</code>.<code>&lt;schema&gt;</code>.<code>&lt;table&gt;</code>, for example.</p> <ul> <li><code>iceberg.events.behavior</code> with <code>WRITE</code> : user / credential has the <code>WRITE</code> privilege to table behavior in <code>events</code> schema of <code>iceberg</code> catalog.</li> <li><code>iceberg.events.*</code> with <code>READ</code>: user / credential has the <code>READ</code> privilege to all the tables in <code>events</code> schema of <code>iceberg</code> catalog.</li> <li><code>mysql.*</code> with <code>READ</code>: user / credential has the <code>READ</code> privilege to all the tables in all schemas of <code>mysql</code> catalog.</li> <li><code>*</code> with <code>WRITE</code>: user / credential has the <code>WRITE</code> privilege to all the tables in all schemas of all catalogs.</li> </ul>"},{"location":"features/streaming/","title":"Chango Streaming","text":"<p>External event streaming application can insert streaming events into iceberg tables in chango without building streaming platform and writing streaming jobs.</p>"},{"location":"features/streaming/#no-streaming-platform-no-streaming-jobs","title":"No Streaming Platform, No Streaming Jobs","text":"<p>If you want to insert streaming events like user behavior events, logs, IoT events to data lakehouses, you need to build event streaming platform like kafka and write streaming jobs like spark streaming jobs in most cases. But in chango, you don't have to do so. </p> <p></p> <p>External streaming application can insert streaming events to iceberg tables in chango directly without streaming platform and streaming jobs.</p>"},{"location":"features/trino-gw/","title":"Chango Trino Gateway","text":"<p><code>Chango Trino Gateway</code> is an implementation of trino gateway concept.</p>"},{"location":"features/trino-gw/#understand-trino-gateway-concept","title":"Understand Trino Gateway Concept","text":"<p>Chango has the concept of trino gateway which routes trino queries to upstream backend trino clusters dynamically.  If one of the backend trino clusters has exhausted, then trino gateway will route queries to the trino cluster  which is executing less requested queries. Trino does not support HA because trino coordinator has single point failure.  In order to support HA of trino, we need to use trino gateway.</p> <p>Let\u2019s say, there is only one large trino clusters(100 ~ 1000 workers) in the company. Many people like BI experts, data scientists,  and data engineers are running trino queries on this large cluster intensively.  These trino queries can be interactive or ETL. For example, because long running ETL queries have occupied most of the resources  which trino cluster needs to execute another queries, there can be little resources remaining trino cluster can use  to execute another interactive queries. The people who are running the interactive queries need to wait  until the long running ETL queries will finish. Such conflict problems can also happen in reverse case.</p> <p>Such monolithic approach with large trino workers can lead to be problematic. We need to separate a large trino cluster  to small trino clusters for the groups like BI team, data scientist team, and data engineer team individually.</p> <p>Let\u2019s say, BI team has 3 backeend trino clusters. If one of trino clusters needs to be scaled out, and trino catalogs needs  to be added to this trino cluster for new external data sources, or the trino cluster needs to be reinstalled with new trino version,  then  first just deactivate this trino cluster to which the queries will not be routed without down time problem of trino query execution.  After finishing scaling workers ,updating catalogs or reinstalling the trino cluster, activate this trino cluster to which the queries  will be routed again. With trino gateway, the activation and deactivation of backend trino clusters can be done with ease. </p>"},{"location":"features/trino-gw/#chango-trino-gateway-features","title":"Chango Trino Gateway Features","text":"<p><code>Chango Trino Gateway</code> provides several critical functions of trino gateway.</p>"},{"location":"features/trino-gw/#trino-user-authentication-and-authorization","title":"Trino User Authentication and Authorization","text":"<p>If trino user sends trino queries to <code>Chango Trino Gateway</code>, <code>Chango Trino Gateway</code> authenticates trino user and  authorize trino queries if trino user is allowed to access data in Chango or not. </p> <p>For authorization, <code>Chango Trino Gateway</code> works with <code>Chango Authorizer</code> which is central to authorize all credentials used by all chango components. So, <code>Chango Trino Gateway</code> controls the data access of trino user to Chango with RBAC in the fine-grained manner like catalog, schema and table level.</p> <p>Especially, RBAC to <code>Cluster Group</code> can be configured without restarting backend trino clusters.</p>"},{"location":"features/trino-gw/#route-trino-queries-to-trino-clusters","title":"Route Trino Queries to Trino Clusters","text":"<p>Routing trino queries to the backend trino clusters is fundamental function of <code>Chango Trino Gateway</code>. Let's see the following picture how <code>Chango Trino Gateway</code> will route.</p> <p></p> <p>Trino queries which trino users run who belong to the <code>Cluster Group</code> will be routed to the backend trino clusters which belong to the <code>Cluster Group</code>. <code>Chango Trino Gateway</code> detects exhausted trino clusters, and will route trino queries to less exhausted trino clusters in smart way.</p> <p>From the point of the storage security in Chango, <code>Cluster Group</code> is equivalent to <code>Role</code> in <code>Chango Security</code> model.  <code>Chango Trino Gateway</code> will check if trino queries are allowed to run according to privileges of <code>Cluster Group</code> or not.</p> <p>Even if you have just one trino cluster as backend trino cluster, you can create several <code>Cluster Groups</code>  as many as you need, and you can control data access of <code>Cluster Groups</code> as usual in <code>Chango Security</code> model. </p>"},{"location":"features/trino-gw/#activate-and-deactivate-trino-clusters","title":"Activate and Deactivate Trino Clusters","text":"<p><code>Chango Trino Gateway</code> provides the feature of activation and deactivation of trino clusters.</p> <p>If the configuration of backend trino clusters is updated, the backend trino clusters need to be restarted. In order to provide <code>Zero Downtime to Run Trino Queries</code>, <code>Chanog Trino Gateway</code> provides such feature of activation and deactivation of trino clusters.</p> <p>Do the following steps for every trino clusters in order to accomplish <code>Zero Downtime to Run Trino Queries</code>.</p> <ul> <li>deactivate the trino cluster whose configuration needs to be updated.</li> <li>update configuration.</li> <li>restart that trino cluster. </li> <li>activate that trino cluster again. </li> </ul>"},{"location":"install/install-admin/","title":"Install Chango Admin","text":"<p>Chango Private is Data Lakehouse Platform which can be installed in both connected and disconnected environment.</p> <p>Chango Private consists of <code>Chango Admin</code> and <code>Chango Components</code>. All the <code>Chango Components</code> will be installed by <code>Chango Admin</code>,  so you need to install <code>Chango Admin</code> before installing <code>Chango Components</code>.</p>"},{"location":"install/install-admin/#prerequisites","title":"Prerequisites","text":"<p>There are several things to prepare before proceeding to install Chango Private.</p>"},{"location":"install/install-admin/#supported-os-and-python","title":"Supported OS and Python","text":"<p>Supported OS and Python Version are:</p> <ul> <li><code>CentOS 7.9</code></li> <li><code>Python 3.6.8</code></li> </ul>"},{"location":"install/install-admin/#prepare-chango-nodes","title":"Prepare Chango Nodes","text":"<p>In order to install Chango Private, we need nodes. Let's call it as <code>Chango Nodes</code>. <code>Chango Nodes</code> consist of <code>Chango Admin Node</code> on which Chango Admin will be installed and <code>Chango Component Nodes</code> on which  Chango Components will be installed.</p> <p></p> <p>First, you will install <code>Chango Admin</code>, and then all the components will be installed by <code>Chango Admin</code> through ansible playbook using SSH. So, you need to prepare <code>at least 4</code> nodes to install Chango Private.</p> <p>NOTE: 1 node for <code>Chango Admin Node</code> and <code>at least 3</code> nodes for <code>Chango Comonent Nodes</code>. That is, you need to have <code>at least 4</code>nodes to install Chango Private.</p> <p>With respect to hardware requirement, 2 cores and 4GB memory is good for <code>Chango Admin Node</code>, but for <code>Chango Component Nodes</code>,  there are many variations to determine how much capacity you need for <code>Chango Component Nodes</code>.  The following components provided by Chango Private are the components which can affect the determination of hardware capacity.</p> <ul> <li>Trino</li> <li>Apache Spark</li> <li>Apache Ozone</li> <li>Apache Kafka</li> </ul>"},{"location":"install/install-admin/#open-ports","title":"Open Ports","text":"<p>You need to open ports in your subnet of <code>Chango Nodes</code>. See the details of Chango Private Ports.</p>"},{"location":"install/install-admin/#password-less-ssh-connection","title":"Password-less SSH Connection","text":"<p>From the node of <code>Chango Admin Node</code>, <code>sudo</code> user must access to <code>Chango Component Nodes</code> with password-less SSH connection.</p> <p>The following needs to be configured.</p> <ul> <li><code>sudo</code> user with the same name needs to be created on all the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>.</li> <li>configure password-less SSH connection with the created <code>sudo</code> user from <code>Chango Admin Node</code> to <code>Chango Component Nodes</code>.</li> <li>configure password-less SSH connection with the created <code>sudo</code> user from <code>Chango Admin Node</code> to the self host of <code>Chango Admin Node</code>.</li> </ul>"},{"location":"install/install-admin/#configure-host-names","title":"Configure Host Names","text":"<p>You need to configure host name on all the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>.</p> <p>For example, the following DNS entry is configured in <code>/etc/hosts</code> on <code>Chango Admin Node</code>. <pre><code>10.0.0.100  chango-admin.chango.private chango-admin\n</code></pre> And set host name with FQDN. <pre><code>sudo hostnamectl set-hostname chango-admin.chango.private\n</code></pre></p>"},{"location":"install/install-admin/#create-sudo-user","title":"Create <code>sudo</code> User","text":"<p>For example, we will create <code>sudo</code> user with the name of <code>chango</code>. <pre><code>sudo su -;\nuseradd -m chango;\npasswd chango;\n</code></pre></p> <p>In order to add to sudo user,  <pre><code>sudo visudo;\n</code></pre></p> <p>And add the following. <pre><code>chango   ALL=(ALL) NOPASSWD: ALL\n</code></pre></p> <p>NOTE: You need to create such <code>sudo</code> user on all the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>.</p>"},{"location":"install/install-admin/#create-ssh-keys","title":"Create SSH Keys","text":"<p>You need to generate SSH keys on <code>Chango Admin Node</code> logged in as the <code>sudo</code> user created before. <pre><code>sudo su - chango\n</code></pre></p> <pre><code>ssh-keygen -t rsa;\n</code></pre> <p>Configure SSH with no key checking. Open SSH configuration file. <pre><code>vi ~/.ssh/config\n</code></pre> Add the following. <pre><code>StrictHostKeyChecking no\n</code></pre></p> <p>Add permission to configuration. <pre><code>chmod 600 ~/.ssh/config\n</code></pre></p> <p>Copy the created public key on <code>Chango Admin Node</code> to all the nodes of <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code>.</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Open <code>authorized_keys</code> on the nodes of <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code>, and paste the public key to it. <pre><code>vi ~/.ssh/authorized_keys\n</code></pre></p> <p>NOTE: Take a note that you also need to add public key to the self of <code>Chango Admin Node</code>.</p> <p>And add permission. <pre><code>chmod 600 ~/.ssh/authorized_keys\n</code></pre></p> <p>And, test if password-less ssh access works from <code>Chango Admin Node</code> to the nodes of <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code></p> <pre><code># access to nodes of chango component nodes.\nssh chango-comp-1.chango.private;\nssh chango-comp-2.chango.private;\nssh chango-comp-3.chango.private;\nssh chango-comp-4.chango.private;\n\n# access to self host of chango admin node.\nssh chango-admin.chango.private\n</code></pre>"},{"location":"install/install-admin/#attache-raw-disks-to-chango-component-nodes","title":"Attache Raw Disks to <code>Chango Component Nodes</code>","text":"<p>As depicted in the above picture, several Raw Disks must be attached to <code>Chango Component Nodes</code>. But you don\u2019t have to attache any disks to <code>Chango Admin Node</code>.</p> <p>NOTE: Raw disks attached to <code>Chango Component Nodes</code> MUST NOT be mounted! Chango will mount attached disks as logical volume by installing Chango Components later.</p>"},{"location":"install/install-admin/#set-selinux-to-permissive","title":"Set Selinux to Permissive","text":"<p>You need to set selinux to <code>permissive</code> on all <code>Chango Nodes</code> with the following command.</p> <pre><code>sudo setenforce 0\nsudo sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/selinux/config\nsudo sed -i 's/SELINUX=disabled/SELINUX=permissive/g' /etc/selinux/config\n</code></pre>"},{"location":"install/install-admin/#add-yum-repositories","title":"Add Yum Repositories","text":"<p>For online environment, add the following yum repository on all the <code>Chango Nodes</code>.</p> <pre><code>sudo yum install epel-release -y\n</code></pre> <p>NOTE: For offline and disconnected environment, you don't have to do it.</p>"},{"location":"install/install-admin/#local-yum-repository","title":"Local Yum Repository","text":"<p>If you want to install Chango Private in disconnected environment, you need to install local yum repository which all the <code>Chango Nodes</code> will look up.</p> <p>NOTE: If you want to install Chango Private in connected environment, skip this instruction.</p>"},{"location":"install/install-admin/#on-the-node-of-local-yum-repository","title":"On the Node of Local Yum Repository","text":"<p>On the host of local yum repository, you need to follow the below instructions.</p> <p>Install NGINX proxy.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-libs/releases/download/chango-private-deps/nginx-1.14.0-1.el7_4.ngx.x86_64.rpm\nsudo rpm -Uvh nginx-1.14.0-1.el7_4.ngx.x86_64.rpm\n</code></pre> <p>Open port of <code>80</code>. <pre><code>sudo firewall-cmd --zone=public --add-port=80/tcp --permanent;\nsudo firewall-cmd --reload;\n</code></pre></p> <p>Install yum utils. <pre><code>sudo yum install createrepo  yum-utils -y;\n</code></pre></p> <p>Create directories of local yum repository. <pre><code># local repo directory.\nexport LOCAL_REPO_BASE=/data/var/www/html\nexport LOCAL_REPO_DIR=${LOCAL_REPO_BASE}/repos;\n\n# create repo directories.\nsudo mkdir -p ${LOCAL_REPO_DIR}/{base,epel,extras,updates};\n</code></pre></p> <p>Synchronize Yum Repos to Local Repo. <pre><code># synchronize repositories to local.\nsudo reposync -g -l -d -m --repoid=base --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\nsudo reposync -g -l -d -m --repoid=epel --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\nsudo reposync -g -l -d -m --repoid=extras --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\nsudo reposync -g -l -d -m --repoid=updates --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\n\n# create new repo for local.\nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/base/  \nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/epel/    \n\nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/extras/  \nsudo createrepo ${LOCAL_REPO_DIR}/extras/\nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/updates/  \nsudo createrepo ${LOCAL_REPO_DIR}/updates/\n</code></pre></p> <p>Create NGINX configuration. <pre><code>sudo vi /etc/nginx/conf.d/repos.conf \n</code></pre></p> <p>, and add the following configuration. <pre><code>server {\n        listen   80;\n        server_name  chango-private-yum-local-repo.chango.private;\n        root   /data/var/www/html/repos;\n        location / {\n                index  index.php index.html index.htm;\n                autoindex on;\n        }\n}\n</code></pre></p> <p>Remove NGINX default configuration. <pre><code>sudo rm -rf /etc/nginx/conf.d/default.conf;\n</code></pre></p> <p>Start NGINX. <pre><code>sudo systemctl start nginx;\n</code></pre></p> <p>Now, you can see the list of RPMs. <pre><code>http://[local-yum-repo-ip]/\n</code></pre></p> <p>If internet is available to the host of local yum repository, create crontab to update local yum repository.</p> <pre><code>sudo vi /etc/cron.daily/update-localrepos\n</code></pre> <p>Add the following job. <pre><code>#!/bin/bash\n##specify all local repositories in a single variable\nLOCAL_REPOS=\u201dbase epel extras updates\u201d\n##a loop to update repos one at a time \nfor REPO in ${LOCAL_REPOS}; do\nreposync -g -l -d -m --repoid=$REPO --newest-only --download-metadata --download_path=/data/var/www/html/repos/\ncreaterepo -g comps.xml /data/var/www/html/repos/$REPO/  \ncreaterepo /data/var/www/html/repos/$REPO/\n</code></pre></p> <p>, and add permission. <pre><code>sudo chmod 755 /etc/cron.daily/update-localrepos\n</code></pre></p>"},{"location":"install/install-admin/#on-the-nodes-of-chango-admin-node-and-chango-component-nodes","title":"On the Nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>","text":"<p>As yum client nodes to local yum repository, you need to configure the followings on the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>  to look up remote local yum repository server.</p> <p>Move current yum repo configurations.</p> <pre><code>sudo mkdir -p ~/old-repos\nsudo mv /etc/yum.repos.d/*.repo ~/old-repos/\n</code></pre> <p>Create configuration of local yum repository.</p> <pre><code>sudo vi /etc/yum.repos.d/local-repos.repo\n</code></pre> <p>Add the following configuration. <pre><code>[local-base]\nname=CentOS Base\nbaseurl=http://[local-yum-repo-ip]/base/\ngpgcheck=0\nenabled=1\n\n[local-epel]\nname=CentOS Epel\nbaseurl=http://[local-yum-repo-ip]/epel/\ngpgcheck=0\nenabled=1\n\n[local-extras]\nname=CentOS Extras\nbaseurl=http://[local-yum-repo-ip]/extras/\ngpgcheck=0\nenabled=1\n\n[local-updates]\nname=CentOS Updates\nbaseurl=http://[local-yum-repo-ip]/updates/\ngpgcheck=0\nenabled=1\n</code></pre></p> <p>Replace <code>[local-yum-repo-ip]</code> with the ip address of local yum repository.</p> <p>Refresh yum repository.</p> <pre><code>sudo yum repolist all\n</code></pre> <p>Test yum install, for example.</p> <pre><code>sudo yum install nginx\n</code></pre>"},{"location":"install/install-admin/#download-chango-private-distribution","title":"Download Chango Private Distribution","text":"<p>If you want to install Chango Private in public, then, before downloading Chango Private distribution,  you need to be logged in as <code>sudo</code> user created before on <code>Chango Admin Node</code>, for example.</p> <pre><code>sudo su - chango;\n</code></pre> <p>Download Chango Private distribution. <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-libs/releases/download/chango-private-deps/chango-private-2.1.1.tar.gz\n</code></pre></p> <p>And tar the file and move to the installation directory of Chango Private.</p> <pre><code>tar zxvf chango-private-2.1.1.tar.gz \ncd chango-private-2.1.1/ansible/\n</code></pre> <p>Download all Chango Components.</p> <pre><code>./download-component-files.sh\n</code></pre> <p>NOTE: For installing Chango Private in disconnected environment, after downloading chango component files,  you need to package the whole distribution directory with downloaded component files to the file(for example, tar.gz) which needs to be transferred to your node in which internet is not available.</p>"},{"location":"install/install-admin/#install-chango-admin_1","title":"Install Chango Admin","text":"<p>Now, you are logged in as <code>sudo</code> user on <code>Chango Admin Node</code>.</p> <p>NOTE: Make sure that current user is <code>sudo</code> user created before to access <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code> with password-less SSH connection.</p> <p>Run the following to install Chango Admin.</p> <p><pre><code>./start-chango-private.sh;\n</code></pre> , which will </p> <ul> <li>install Postgresql database.</li> <li>mount attached disks as logical volume on PostgreSQL database host.</li> <li>install Chango Admin on the current host.</li> </ul> <p>Enter values for the prompts, for example.</p> <pre><code>Installing ansible in virtual environment...\nAnsible installed...\nReady to install Chango Admin...\nEnter FQDN of the target host name on which PostgreSQL database will be installed: \nchango-comp-3.chango.private\n\nEnter comma-separated disk paths for LVM mount on the target host of PostgreSQL database (for example, '/dev/sdb,/dev/sdc'): \n/dev/sdb,/dev/sdc\n\nEnter current host name on which Chango Admin will be installed: \nchango-admin.chango.private\n</code></pre> <ul> <li><code>chango-comp-3.chango.private</code> is for PostgreSQL database host which MUST BE one of <code>Chango Component Nodes</code>, NOT <code>Chango Admin Node</code>.</li> <li><code>/dev/sdb,/dev/sdc</code> is for LVM raw disks in comma separated list on PostgreSQL database host.</li> <li><code>chango-admin.chango.private</code> is for current host name of <code>Chango Admin Node</code>.</li> </ul> <p>To enter the raw disks, run <code>lsblk</code> for example.</p> <pre><code>lsblk;\n\nNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n\n...\n\nsdb      8:16   0   512G  0 disk \nsdc      8:32   0   512G  0 disk \n</code></pre> <p>After installing Chango Admin successfully, you need to get randomly genterated password of <code>admin</code>.</p> <pre><code>cat /export/chango-private-admin-logs/admin.log | grep \"randomly generated password\";\n</code></pre> <p>Output looks like this.</p> <pre><code>2023-10-27 03:53:59,863 INFO com.cloudcheflabs.changoprivate.admin.config.AdminUserConfigurer:32 [main] randomly generated password for user 'admin': b3b3c51fde594d7abdd38fbc3eb2a581\n</code></pre> <p>For example, <code>b3b3c51fde594d7abdd38fbc3eb2a581</code> is the password of <code>admin</code>.</p> <p>The URL of Chango Admin UI is, for example. <pre><code>http://chango-admin.chango.private:8123/\n</code></pre></p> <p>NOTE: <code>8123</code> for Admin UI and <code>8080</code> for NGINX proxy to Admin on <code>Chango Admin Node</code> need to be open.</p> <p>Make sure that the host DNS entries for <code>Chango Admin Node</code> and <code>Chango Component Nodes</code> are added to the host file(for example, <code>/etc/hosts</code>) on your local PC,  for example. <pre><code>[ip-address]  chango-admin.chango.private\n\n[ip-address]  chango-comp-1.chango.private\n[ip-address]  chango-comp-2.chango.private\n[ip-address]  chango-comp-3.chango.private\n[ip-address]  chango-comp-4.chango.private\n</code></pre> , where <code>[ip-address]</code> can be public IP address for online environment or private IP address for offline environment.</p> <p>Login as <code>admin</code> with the randomly generated password. You can change the password of <code>admin</code> in Chango Admin UI <code>Account</code> -&gt; <code>Profile</code> later.</p>"},{"location":"install/install-admin/#after-installing-chango-admin","title":"After installing Chango Admin","text":"<p>After installing Chango Admin, several shell files will be created which can be used for your need later.</p> <ul> <li><code>reinstall-admin.sh</code>: Reinstall LVM mount, PostgreSQL database, and Chango Admin with the same configurations you entered for prompts.</li> <li><code>restart-admin.sh</code>: Restart Chango Admin.</li> <li><code>uninstall-admin.sh</code>: Uninstall Chango Admin.</li> <li><code>uninstall-lvm.sh</code>: Unmount LVM on the host of PostgreSQL database.</li> <li><code>uninstall-postgresql.sh</code>: Uninstall PostgreSQL database.</li> </ul> <p>NOTE: If you have installed <code>Chango Components</code> by <code>Chango Admin</code> later,  DO NOT use <code>reinstall-admin.sh</code>, <code>uninstall-lvm.sh</code>, and <code>uninstall-postgresql.sh</code>  which will destroy all the installation of Chango Private!</p> <p>When the installation of Chango Admin failed with some reasons, you need to run the above shell. If you entered wrong values for the prompts which affects installation failure and want to install Chango Admin again, then run the following sequence.</p> <ul> <li><code>uninstall-admin.sh</code></li> <li><code>uninstall-postgresql.sh</code></li> <li><code>uninstall-lvm.sh</code></li> </ul> <p>, and then, install Chango Admin again with running <code>start-chango-private.sh</code>.</p> <p>If you changed the host and port of NGINX for <code>Chango Admin</code> after adding TLS configuration to NGINX,  then, add the variables to <code>--extra-vars</code> in <code>restart-admin.sh</code>, for example.</p> <pre><code>cpadmin_nginx_scheme=https \\\ncpadmin_nginx_host=[your-nginx-host] \\\ncpadmin_nginx_port=[your-nginx-port] \\\n</code></pre> <p>The revised <code>restart-admin.sh</code> looks like this, for example. <pre><code>python3 -m venv venv;\nsource /home/chango/venv/bin/activate;\n\nansible-playbook -i admin.inv run.yml \\\n--extra-vars \"\\\nexec_user=chango \\\ntarget_hosts=changoprivate-admin-hosts \\\nrole_name=changoprivate-admin \\\ncpadmin_ansible_path=/data/chango/cp-dist/chango-private-2.1.1/ansible \\\nrun_option=restart \\\ncpadmin_nginx_scheme=https \\\ncpadmin_nginx_host=[your-nginx-host] \\\ncpadmin_nginx_port=[your-nginx-port] \\\n\"\n</code></pre></p> <p>Run the revised <code>restart-admin.sh</code>.</p>"},{"location":"install/install-component/","title":"Install Chango Components","text":"<p>There are several components supported by Chango Private.</p> <p>See What is Chango Private? for more details.</p> <p>NOTE: If Chango Private initialization failed or you want to reset Chango Private, then you need to move to  <code>http://[admin-host]:8123/cp-reset.html</code> to reset Chango Private.</p>"},{"location":"install/install-component/#initialize-chango-private","title":"Initialize Chango Private","text":"<p>If you have not initialized Chango Private, you will get the following picture to initialze Chango Private.</p> <p></p> <p>There are mandatory Chango Components like <code>MySQL</code>, <code>Object Storage</code>, <code>Chango Authorizer</code>, and  <code>Chango REST Catalog</code> which must be installed when Chango Private is initialized.  The other optional compoents can be installed after Chango Private initialization.</p>"},{"location":"install/install-component/#configure-hosts-and-ssh-private-key","title":"Configure Hosts and SSH Private Key","text":"<p>All the <code>Chango Component Nodes</code> need to be registered, and SSH private key on <code>Chango Admin Node</code> need to be added to access <code>Chango Component Nodes</code> from  the host of <code>Chango Admin Node</code>.</p> <p>NOTE: Take a note that you need to register all the nodes of <code>Chango Component Nodes</code> except <code>Chango Admin Node</code>.</p> <p>Get SSH private key on <code>Chango Admin Node</code> with the following command, and paste it to the text box <code>SSH Private Key</code>.</p> <pre><code>cat ~/.ssh/id_rsa\n</code></pre> <p></p>"},{"location":"install/install-component/#configure-lvm","title":"Configure LVM","text":"<p>Raw disks attached to the <code>Chango Component Nodes</code> in comma separated list need to be added to mount as logical volume.</p> <p>If the disks attached are same on all the nodes, use this.</p> <p></p> <p>If the disks attached are different on every node, use this.</p> <p></p>"},{"location":"install/install-component/#configure-mysql","title":"Configure MySQL","text":"<p><code>MySQL</code> is used by open source components like <code>Apache Superset</code> and <code>Apache Ozone</code> in Chango Private. Select the host on which <code>MySQL</code> will be installed.</p> <p></p>"},{"location":"install/install-component/#configure-object-storage","title":"Configure Object Storage","text":"<p>Select the options for object storage. <code>Apache Ozone</code> is default object storage provided by Chango Private,  which will be used in disconnected environment in most cases. In public, you may select the external object storage like AWS S3, MinIO, OCI Object Storage.</p> <p>If Apache Ozone is used as object storage, enter the values this below. </p> <p>If external object storage is used, enter values of S3 credentials. </p>"},{"location":"install/install-component/#configure-chango-authorizer","title":"Configure Chango Authorizer","text":"<p><code>Chango Authorizer</code> is used to authenticate and authorize all the data access to Chango.</p> <p></p>"},{"location":"install/install-component/#configure-chango-rest-catalog","title":"Configure Chango REST Catalog","text":"<p><code>Chango REST Catalog</code> is used as data catalog in Chango.</p> <p></p> <p>For now, the components you configured are mandatory.  After configuring mandatory components, you can skip configuration.</p> <p></p> <p>You can install other optional components later after finishing Chango Private initialization.</p>"},{"location":"install/install-component/#install-configured-components","title":"Install Configured Components","text":"<p>Install all the configured components.</p> <p></p> <p>When the installation is finished, press the button of <code>Installation Finished</code>.</p> <p></p> <p>Then, you will move to the main page.</p> <p></p>"},{"location":"install/install-component/#show-log","title":"Show Log","text":"<p>You can see current log produced by installed components.</p> <p></p> <p>Click the host name of components in <code>Status</code> to show log.</p> <p></p>"},{"location":"install/install-component/#apache-kafka","title":"Apache Kafka","text":"<p><code>Apache Kafka</code> is used as event streaming platform in Chango Private. Multiple Kafka clusters are supported by Chango, that is, you can install kafka clusters as many as you want.</p>"},{"location":"install/install-component/#install-kafka","title":"Install Kafka","text":"<p>If you want to install <code>Apache Kafka</code>, press <code>Go to Install</code> button.</p> <p>NOTE: If you have not installed any kafka cluster, then, enter <code>default</code> for the cluster name.</p> <p></p> <p>After installing kafka, you will see kafka page like this.</p> <p></p> <p>Because Chango Private supports multiple kafka clusters, you can install another kafka cluster.</p> <p>NOTE: Because you have already installed default kafka cluster, you can enter anything for the cluster name.</p> <p></p> <p>After installing another kafka cluster, new created kafka cluster will be shown in tab list.</p> <p></p>"},{"location":"install/install-component/#scale-broker","title":"Scale Broker","text":"<p>Kafka Broker can be scaled out or unscaled.</p> <p></p> <p>First, select hosts for scaling out or unscaling kafka brokers, and then, press the button of <code>Scale Out Broker</code> to scale out brokers  or press the button of <code>Unscale Broker</code> to unscale brokers.</p>"},{"location":"install/install-component/#configure-kafka","title":"Configure Kafka","text":"<p>You can update kafka configurations like heap memory and <code>server.properties</code>.</p> <p>First, select kafka cluster which you want to configure.</p> <p></p> <p>After modifying configuration, press <code>Update</code> to update the selected kafka cluster.</p> <p></p>"},{"location":"install/install-component/#apache-spark","title":"Apache Spark","text":"<p><code>Apache Spark</code> is used as computing engine to run batch and streaming jobs in Chango.</p>"},{"location":"install/install-component/#install-spark","title":"Install Spark","text":"<p>Select hosts to install master, worker and history server of Spark. </p> <p>After installing Spark, you will see the spark page like this. </p>"},{"location":"install/install-component/#scale-worker","title":"Scale Worker","text":"<p>You can scale out and unscale spark workers.</p> <p></p>"},{"location":"install/install-component/#ui","title":"UI","text":"<p>There are URL links to get Spark Master UI and Spark History Server UI in <code>UI</code> of Spark Page.</p> <p>Spark Master UI looks as below.</p> <p></p> <p>Spark History Server UI looks like this.</p> <p></p>"},{"location":"install/install-component/#trino","title":"Trino","text":"<p><code>Trino</code> is used as query engine to run interactive and long running ETL query in Chango.  Chango Private provides multiple trino clusters, so, you can install trino clusers as many as you want.</p>"},{"location":"install/install-component/#install-trino","title":"Install Trino","text":"<p>Enter <code>default</code> for the cluser name if default trino cluster is not installed. </p> <p>After installing default trino cluster, trino page looks like this. </p> <p>If you want to install another trino cluster, enter any name for cluster name. </p> <p>After installing another trino cluster, new created trino cluster will be shown in the cluster tab list. </p>"},{"location":"install/install-component/#scale-worker_1","title":"Scale Worker","text":"<p>You can scale out and unscale trino workers.</p> <p></p>"},{"location":"install/install-component/#configure-trino","title":"Configure Trino","text":"<p>You can update trino memory configurations and catalogs.</p> <p>To update memory properties in trino, select trino cluster. </p> <p>Update catalogs in trino. </p> <p>You can also add catalogs. </p>"},{"location":"install/install-component/#ui_1","title":"UI","text":"<p>You can get Trino UI clicking link in <code>UI</code> of the selected trino cluster.</p> <p></p>"},{"location":"install/install-component/#chango-trino-gateway","title":"Chango Trino Gateway","text":"<p><code>Chango Trino Gateway</code> is used to route trino queries to the backend trino clusters in Chango.</p> <p>In addition, <code>Chango Trino Gateway</code> also provides the following functions. - authenticate trino users and authorize the queries run by trino users. - activate and deactivate the backend trino clusters.</p>"},{"location":"install/install-component/#install-chango-trino-gateway","title":"Install Chango Trino Gateway","text":"<p>Select hosts for <code>Chango Trino Gateway</code> servers, host for NGINX proxy, and host for Redis cache. </p> <p>It looks as below after installing it. </p>"},{"location":"install/install-component/#apache-superset","title":"Apache Superset","text":"<p><code>Apache Superset</code> is used as BI tool in Chango.</p>"},{"location":"install/install-component/#install-superset","title":"Install Superset","text":"<p>Select host for superset server. </p>"},{"location":"install/install-component/#ui_2","title":"UI","text":"<p>You can get Superset UI clicking link in <code>UI</code> of superset page. </p>"},{"location":"install/install-component/#azkaban","title":"Azkaban","text":"<p><code>Azkaban</code> is used as workflow to integrate all the batch jobs like spark ETL jobs and trino ETL jobs in Chango.</p>"},{"location":"install/install-component/#install-azkaban","title":"Install Azkaban","text":"<p>Select host for web and hosts for executors. </p>"},{"location":"install/install-component/#ui_3","title":"UI","text":"<p>You can get Azkaban UI clicking link in <code>UI</code> of azkaban page. </p>"},{"location":"install/install-component/#azkaban-cli","title":"Azkaban CLI","text":"<p><code>Azkaban CLI</code> is CLI to create and update azkaban project on Azkaban.</p>"},{"location":"install/install-component/#install-azkaban-cli","title":"Install Azkaban CLI","text":"<p>Select hosts for azkaban CLI. </p>"},{"location":"install/install-component/#dbt","title":"dbt","text":"<p><code>dbt</code> is CLI tool to transform data in Chango. In most cases, it will be used to run trino queries.</p>"},{"location":"install/install-component/#install-dbt","title":"Install dbt","text":"<p>Select hosts for dbt. </p>"},{"location":"install/install-component/#chango-data-api","title":"Chango Data API","text":"<p>Chango provides data ingestion especially for streaming events. <code>Chango Data API</code> is used to collect streaming events and produce them to kafka.</p>"},{"location":"install/install-component/#install-chango-data-api","title":"Install Chango Data API","text":"<p>Select hosts for Chango Data API servers and host for NGINX proxy. </p>"},{"location":"install/install-component/#scale-server","title":"Scale Server","text":"<p>You can scale out and unscale <code>Chango Data API</code> servers. </p>"},{"location":"install/install-component/#chango-streaming","title":"Chango Streaming","text":"<p><code>Chango Streaming</code> is a spark streaming job and used to consume streaming events from kafka and save them to Iceberg table in Chango.</p>"},{"location":"install/install-component/#install-chango-streaming","title":"Install Chango Streaming","text":"<p>Enter spark configurations for <code>Chango Streaming</code> job. </p> <p>After installation, you will see the driver host of <code>Chango Streaming</code> spark job. </p> <p>In spark master UI, <code>Chango Streaming</code> job will be found. </p>"},{"location":"install/run-first-ctas/","title":"Run First Trino Query","text":"<p>This is an example to run queries to Trino through <code>Chango Trino Gateway</code>.</p>"},{"location":"install/run-first-ctas/#create-cluster-group","title":"Create Cluster Group","text":"<p>First, you need to create <code>Cluster Group</code> in <code>Trino Gateway</code>. Go to <code>Settings</code> -&gt; <code>Trino Gateway</code>. Click <code>Create Cluster Group</code> in <code>Cluster Groups</code> section.</p> <p></p> <p>And, enter cluster group name <code>bi</code>. </p>"},{"location":"install/run-first-ctas/#create-trino-user-and-register-trino-cluster","title":"Create Trino User and Register Trino Cluster","text":"<p>In order to create trino user and register trino cluster, first select cluster group created before. </p> <p>Click <code>Create Trino User</code> in <code>Trino Users</code> section, and enter user name with password.</p> <p></p> <p>To register trino cluster, click <code>Register Trino Cluster</code> in <code>Trino Clusters</code> section.  Select trino cluster which trino queries will be routed to.</p> <p></p>"},{"location":"install/run-first-ctas/#create-privileges","title":"Create Privileges","text":"<p>To add privileges to the cluster group, go to <code>Settings</code> -&gt; <code>Security</code>. And select role in <code>Roles</code> section.</p> <p></p> <p>Add privilege of <code>*</code> for READ type. </p> <p>Add privilege of <code>*</code> for WRITE type. </p> <p><code>*</code> means all data access to Chango is allowed.</p>"},{"location":"install/run-first-ctas/#get-endpoint-of-chango-trino-gateway","title":"Get Endpoint of Chango Trino Gateway","text":"<p>You need endpoint of <code>Chango Trino Gateway</code> to which clients will connect to run queries.</p> <p>Go to <code>Components</code> -&gt; <code>Chango Trino Gateway</code>. Get the endpoint in <code>Endpoint</code> section. </p>"},{"location":"install/run-first-ctas/#run-first-ctas-query-with-superset","title":"Run First CTAS Query with Superset","text":"<p>To run queries to trino, Superset will be used.</p>"},{"location":"install/run-first-ctas/#login-to-superset","title":"Login to Superset","text":"<p>To move to <code>Superset</code> UI, go to <code>Components</code> -&gt; <code>Apache Superset</code>. Click UI URL to move to Superset UI.</p> <p>First, login as <code>admin</code> with default password <code>SupersetPass1#</code>.</p>"},{"location":"install/run-first-ctas/#add-trino-database","title":"Add Trino Database","text":"<p>You need to add trino database in superset.</p> <p>You need to enter trino url in <code>SQLAlchemy URI *</code> with the following convention. <pre><code>trino://[user]:[password]@[trino-gateway-endpoint-without-scheme]\n</code></pre> For example, you can enter like this. <pre><code>trino://trino:trino123@chango-private-3.chango.private:443\n</code></pre> Replace <code>chango-private-3.chango.private:443</code> with your trino gateway endpoint without <code>https://</code>.</p> <p></p> <p>And you need to add the following in <code>Extra</code> to disable TLS validation. <pre><code>{\n    \"metadata_params\": {},\n    \"engine_params\": {\n          \"connect_args\":{\n              \"http_scheme\": \"https\",\n              \"verify\": false\n        }\n     },\n    \"metadata_cache_timeout\": {},\n    \"schemas_allowed_for_csv_upload\": []\n}\n</code></pre> </p> <p>Check the options of <code>Allow CREATE TABLE AS</code>, <code>Allow CREATE VIEW AS</code>, <code>Allow DML</code>.</p> <p>Finally, press <code>Save</code>.</p>"},{"location":"install/run-first-ctas/#run-ctas-query","title":"Run CTAS Query","text":"<p>Run CTAS query which selects rows from <code>tpch.sf1000.lineitem</code> table and insert them to new created iceberg table <code>iceberg.iceberg_db.test_ctas</code>. <pre><code>-- create schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- ctas.\nCREATE TABLE IF NOT EXISTS iceberg.iceberg_db.test_ctas \nAS\nSELECT\n    *\nFROM tpch.sf1000.lineitem limit 1000\n;\n</code></pre> </p> <p>And select rows from created iceberg table. <pre><code>-- select.\nselect * from iceberg.iceberg_db.test_ctas limit 1000;\n</code></pre> </p> <p>Congratulations!</p>"},{"location":"intro/intro/","title":"What is Chango Private?","text":"<p>Chango Private is a Data Lakehouse Platform for both online and disconnected environment.</p>"},{"location":"intro/intro/#chango-private-data-lakehouse-platform","title":"Chango Private Data Lakehouse Platform","text":"<p>In <code>Ingestion</code> layer:</p> <ul> <li><code>Spark</code> and <code>Trino</code> with <code>dbt</code> will be used as data integration tool.</li> <li><code>Kafka</code> is used as event streaming platform to handle streaming events.</li> <li><code>Chango Data API</code> and <code>Chango Streaming</code> will be used to insert incoming streaming events to Chango directly.</li> </ul> <p>In <code>Storage</code> layer:</p> <ul> <li>Chango Private supports Apache Ozone as object storage by default and external S3 compatible object storage like AWS S3, MinIO, OCI Object Storage.</li> <li>Default storage format is <code>Iceberg</code> table format used for data warehousing in Chango.</li> </ul> <p>In <code>Transformation</code> layer:</p> <ul> <li><code>Spark</code> and <code>Trino</code> with <code>dbt</code> will be used to run ETL jobs.</li> </ul> <p>In <code>Analytics</code> layer:</p> <ul> <li><code>Trino</code> will be used as query engine to explore all the data in Chango.</li> <li><code>BI</code> tools like <code>Apache Superset</code> will connect to <code>Trino</code> to run queries through <code>Chango Trino Gateway</code>.</li> </ul> <p>In <code>Management</code> layer:</p> <ul> <li><code>Azkaban</code> will be used as workflow. All the batch jobs like ETL will be integrated with <code>Azkaban</code>.</li> <li><code>Chango REST Catalog</code> will be used as data catalog in Chango.</li> <li>Chango Private supports storage security to control data access based on RBAC in Chango. <code>Chango Authorizer</code> will be used for it.</li> <li><code>Chango Trino Gateway</code> is an implementation of Trino Gateway concept. <code>Chango Trino Gateway</code> provides several features like authentication, authorization, smart query routing(routing to less exhausted trino clusters), trino cluster activation/deactivation. For more details, see Chango Trino Gateway.</li> </ul>"},{"location":"user-guide/azkaban-cli/","title":"Create Azkaban Project using CLI","text":"<p>Azkaban Project file is a zip file in which workflow files are contained.  Azkaban project file can be uploaded either via Azkaban Web UI or via Azkaban CLI to create workflow in Azkaban.</p>"},{"location":"user-guide/azkaban-cli/#create-azkaban-project-file","title":"Create Azkaban Project File","text":"<p>First, create flow file, for example, <code>spark-pi-client.flow</code>.</p> <pre><code>---\nconfig:\n  failure.emails: admin@your-domain.com\n\nnodes:\n  - name: Start\n    type: noop\n\n  - name: SparkPi\n    type: command\n    config:\n      command: sshpass -p \"xxx\" ssh spark@chango-private-3 \"/home/spark/run-spark-pi-client.sh\"\n    dependsOn:\n      - Start\n\n  - name: End\n    type: noop\n    dependsOn:\n      - SparkPi\n</code></pre> <p>And create project file <code>flow20.project</code>.</p> <pre><code>azkaban-flow-version: 2.0\n</code></pre> <p>Finally, package as zip file. <pre><code>zip spark-pi-client.zip spark-pi-client.flow flow20.project \n</code></pre></p>"},{"location":"user-guide/azkaban-cli/#upload-azkaban-project-file-with-azkaban-cli","title":"Upload Azkaban Project File with Azkaban CLI","text":"<p>First, access to the host where <code>Azkaban CLI</code> is installed.</p> <p>Login as azkaban cli user with activating python virtual environment.</p> <pre><code>sudo su - azkabancli;\nsource venv/bin/activate;\nazkaban --help;\n</code></pre> <p>Create azkaban project with the name of <code>spark-pi-client</code> with CLI.</p> <p><pre><code>azkaban upload -c -p spark-pi-client -u azkaban@http://[azkaban-web-host]:28081 ./spark-pi-client.zip\n</code></pre> Default password for the user <code>azkaban</code> is <code>azkaban</code>.</p> <p>To update azkaban project, parameter <code>-c</code> needs to be removed from the above command.</p>"},{"location":"user-guide/connect-trino-gw/","title":"Connect to Chango Trino Gateway with Trino Clients","text":"<p>Using Trino Clients like Trino CLI and Apache Superset, you can connect to <code>Chango Trino Gateway</code>.</p> <p>Before proceeding to this example, you need to see Run First Trino Query  how to create <code>Cluster Group</code> in <code>Chango Trino Gateway</code> with storage security.</p>"},{"location":"user-guide/connect-trino-gw/#conect-to-chango-trino-gateway-with-apache-superset","title":"Conect to Chango Trino Gateway with Apache Superset","text":"<p>You need to follow the instruction shown in Run First Trino Query to  connect to <code>Chango Trino Gateway</code> using Apache Superset.</p>"},{"location":"user-guide/connect-trino-gw/#connect-to-chango-trino-gateway-with-trino-cli","title":"Connect to Chango Trino Gateway with Trino CLI","text":""},{"location":"user-guide/connect-trino-gw/#install-trino-cli","title":"Install Trino CLI","text":"<p>Run the following to install Trino CLI.</p> <pre><code>mkdir -p ~/trino-cli;\ncd ~/trino-cli;\n\nexport TRINO_VERSION=422\ncurl -L -O https://repo1.maven.org/maven2/io/trino/trino-cli/${TRINO_VERSION}/trino-cli-${TRINO_VERSION}-executable.jar;\nmv trino-cli-${TRINO_VERSION}-executable.jar trino\nchmod +x trino\n\n./trino --version;\n</code></pre>"},{"location":"user-guide/connect-trino-gw/#connect-to-chango-trino-gateway","title":"Connect to Chango Trino Gateway","text":"<p>Let's say you have created trino user <code>trino</code> with proper password in the previous section, and the endpoint of  <code>Chango Trino Gateway</code> is <code>https://chango-private-1.chango.private:443</code>.  Run the following to connect to <code>Chango Trino Gateway</code>, and enter the password for the user <code>trino</code>.</p> <pre><code>./trino --server https://chango-private-1.chango.private \\\n--user trino \\\n--insecure \\\n--password;\n</code></pre> <p>If there is no problem occurred to connect, then you will get the following with running <code>show catalogs</code>.</p> <pre><code>trino&gt; show catalogs;\n Catalog \n---------\n iceberg \n jmx     \n system  \n tpcds   \n tpch    \n(5 rows)\n\nQuery 20231108_052020_00744_pdmmv, FINISHED, 1 node\nSplits: 1 total, 1 done (100.00%)\n0.04 [0 rows, 0B] [0 rows/s, 0B/s]\n</code></pre>"},{"location":"user-guide/cred/","title":"Get Chango Credential","text":"<p>To get Chango Credential, you need to create new role with proper privileges for credentials in most cases.</p> <p>Go to <code>Settings</code> -&gt; <code>Security</code>.</p>"},{"location":"user-guide/cred/#create-role","title":"Create Role","text":"<p>Click <code>Create Role</code> in <code>Roles</code> section. Enter role name.</p> <p></p>"},{"location":"user-guide/cred/#create-credential","title":"Create Credential","text":"<p>First, select role you created, and click <code>Create Credential</code> in <code>Credentials</code> section.</p> <p></p>"},{"location":"user-guide/cred/#create-privileges","title":"Create Privileges","text":"<p>To add data access control, you need to create privileges with storage path.</p> <p>Create privileges for READ and WRITE type clicking <code>Create Privileges</code> in <code>Privileges</code> section.</p> <p>For READ privileges. </p> <p>For WRITE privileges. </p>"},{"location":"user-guide/cred/#list-credential","title":"List Credential","text":"<p>You will get credential created before from the list of credentials of role in <code>Credentials</code> section.</p> <p></p>"},{"location":"user-guide/dbt/","title":"Run dbt Model","text":"<p>This is an example to run <code>dbt</code> model in Chango Private.</p>"},{"location":"user-guide/dbt/#configure-trino-connection","title":"Configure Trino Connection","text":"<p>You need to configure trino connection in <code>profiles.yml</code> in dbt. The following is a template to connect to <code>Chango Trino Gateway</code> from dbt. <pre><code>trino:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      method: ldap\n      user: trino\n      password: xxx\n      host: chango-private-5.chango.private\n      port: 443\n      database: iceberg\n      schema: silver\n      threads: 8\n      http_scheme: https\n      session_properties:\n        query_max_run_time: 5d\n        exchange_compression: True\n</code></pre> The target catalog is <code>iceberg</code> and schema is <code>silver</code>. <code>host</code> is the host name of <code>Chango Trino Gateway</code> endpoint which  can be found in <code>Endpoint</code> section in <code>Components</code> -&gt; <code>Chango Trino Gateway</code>.</p> <p><code>user</code> and <code>password</code> can be created in <code>Settings</code> -&gt; <code>Trino Gateway</code>. For more details, see here.</p>"},{"location":"user-guide/dbt/#run-dbt-model-using-git-sync","title":"Run dbt Model using <code>git-sync</code>","text":"<p><code>dbt</code> is installed as docker container in Chango Private. To see on which hosts <code>dbt</code> is installed, go to <code>Components</code> -&gt; <code>dbt</code>. First, you need to access <code>dbt</code> host to run dbt model. </p> <p>Assumed that your <code>dbt</code> model files are source controlled by git repo, you can run <code>dbt</code> model with this command.</p> <pre><code>docker exec -it dbt \\\n/bin/sh -c '\n/git-sync \\\n-repo [git-repo-url] \\\n-branch [branch] \\\n-username [user] \\\n-password [password] \\\n-root [root-dir] \\\n-one-time &amp;&amp; \\\ncd [root-dir]/[git-repo-name].git/[dbt-model-dir] &amp;&amp; \\\ndbt run \\\n--profiles-dir ../ \\\n--project-dir ./ \\\n-m models/[model-sql-file]\n'\n</code></pre> <p><code>git-sync</code> already installed in <code>dbt</code> docker image will be used to pull <code>dbt</code> model files from git repo with the following command. <pre><code>/git-sync \\\n-repo [git-repo-url] \\\n-branch [branch] \\\n-username [user] \\\n-password [password] \\\n-root [root-dir] \\\n-one-time &amp;&amp; \\\n</code></pre></p> <p>And run dbt model like this. <pre><code>dbt run \\\n--profiles-dir ../ \\\n--project-dir ./ \\\n-m models/[model-sql-file]\n</code></pre></p>"},{"location":"user-guide/kafka-topic/","title":"Create Kafka Topic","text":"<p>To create kafka topic in Chango Private, first access to the host of kafka nodes.</p> <p>Login as kafka user and move to kafka installation directory.</p> <pre><code># login as kafka.\nsudo su - kafka;\n\n# move to kafka installation directory.\ncd /usr/lib/kafka;\n</code></pre> <p>Run the command to create topic.</p> <pre><code>export JAVA_HOME=/usr/lib/jdk\n\nbin/kafka-topics \\\n--create \\\n--bootstrap-server kafka-host-1:9092,kafka-host-2:9092,kafka-host-3:9092 \\\n--topic [topic] \\\n--replication-factor 3 \\\n--partitions 10 \\\n--config cleanup.policy=compact\n</code></pre> <p><code>[topic]</code> is topic name to create.</p>"},{"location":"user-guide/rest-in-spark/","title":"Create Iceberg REST Catalog in Spark","text":"<p>This shows how to create Iceberg REST catalog in spark applications in Chango Private.</p> <p>If spark applications need to access Iceberg tables using <code>Chango REST Catalog</code>, Chango credential and S3 credential are necessary.</p> <p>Let's see the following codes to create Iceberg REST Catalog in Spark. <pre><code>        String s3AccessKey = \"...\";\nString s3SecretKey = \"...\";\nString s3Endpoint = \"...\";\nString s3Region = \"...\";\nString restUrl = \"...\";\nString warehouse = \"...\";\nString token = \"...\";\nSparkConf sparkConf = new SparkConf().setAppName(\"Run Spark with Iceberg REST Catalog\");\n// set aws system properties.\nSystem.setProperty(\"aws.region\", (s3Region != null) ? s3Region : \"us-east-1\");\nSystem.setProperty(\"aws.accessKeyId\", s3AccessKey);\nSystem.setProperty(\"aws.secretAccessKey\", s3SecretKey);\n// iceberg rest catalog.\nsparkConf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\");\nsparkConf.set(\"spark.sql.catalog.rest\", \"org.apache.iceberg.spark.SparkCatalog\");\nsparkConf.set(\"spark.sql.catalog.rest.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\");\nsparkConf.set(\"spark.sql.catalog.rest.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\");\nsparkConf.set(\"spark.sql.catalog.rest.uri\", restUrl);\nsparkConf.set(\"spark.sql.catalog.rest.warehouse\", warehouse);\nsparkConf.set(\"spark.sql.catalog.rest.token\", token);\nsparkConf.set(\"spark.sql.catalog.rest.s3.endpoint\", s3Endpoint);\nsparkConf.set(\"spark.sql.catalog.rest.s3.path-style-access\", \"true\");\nsparkConf.set(\"spark.sql.defaultCatalog\", \"rest\");\nSparkSession spark = SparkSession\n.builder()\n.config(sparkConf)\n.enableHiveSupport()\n.getOrCreate();\nConfiguration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\nhadoopConfiguration.set(\"fs.s3a.endpoint\", s3Endpoint);\nif(s3Region != null) {\nhadoopConfiguration.set(\"fs.s3a.endpoint.region\", s3Region);\n}\nhadoopConfiguration.set(\"fs.s3a.access.key\", s3AccessKey);\nhadoopConfiguration.set(\"fs.s3a.secret.key\", s3SecretKey);\nhadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\");\nhadoopConfiguration.set(\"fs.s3a.change.detection.mode\", \"warn\");\nhadoopConfiguration.set(\"fs.s3a.change.detection.version.required\", \"false\");\nhadoopConfiguration.set(\"fs.s3a.multiobjectdelete.enable\", \"true\");\nhadoopConfiguration.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\");\nhadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\");\n</code></pre></p> <ul> <li><code>s3AccessKey</code>: S3 Access Key.</li> <li><code>s3SecretKey</code>: S3 Secret Key.</li> <li><code>s3Endpoint</code>: S3 Endpoint.</li> <li><code>s3Region</code>: S3 Region.</li> <li><code>restUrl</code>: Endpoint of <code>Chango REST Catalog</code>.</li> <li><code>warehouse</code>: Warehouse Path.</li> <li><code>token</code>: Chango Credential. </li> </ul> <p>To get S3 Credential, see Get S3 Credential.  For the value of <code>warehouse</code>, it can be <code>s3a://[bucket]/warehouse-rest</code> where <code>[bucket]</code> is found in your S3 credential list.</p> <p>To get Endpoint URL of <code>Chango REST Catalog</code>, go to <code>Components</code> -&gt; <code>Chango REST Catalog</code>.  The URL will be shown in <code>Endpoint</code> section.</p> <p>To get Chango Credential, see Get Chango Credential.</p>"},{"location":"user-guide/s3-cred/","title":"Get S3 Credential","text":"<p>Chango provides object storage as storage. You can select either <code>Apache Ozone</code> or <code>External S3 compatible Object Storage</code> like AWS S3, MinIO and OCI Object Storage  in Chango.</p> <p>Applications using Spark and Trino need to have S3 Credential to access to Chango Storage.</p> <p>Go to <code>Settings</code> -&gt; <code>S3 Credential</code>.</p>"},{"location":"user-guide/s3-cred/#create-s3-credential","title":"Create S3 Credential","text":"<p>If <code>Apache Ozone</code> is being used as object storage in Chango Private, you can create new S3 Credential as you want.</p> <p>Press <code>Create</code> button, and enter description. </p> <p>Then, the created S3 credential will be shown in the list. </p>"},{"location":"user-guide/schema-reg/","title":"Configure Schema Registry URL","text":"<p>Schema Registry is schema registry server for kafka to handle schemas of kafka messages.  For example, if you want to send Avro messages to kafka, then first, you need to register Avro schema to Schema Registry.</p> <p>To send Avro messages to Kafka using Schema Registry, see here for more details.</p> <p>The following property needs to be configured to Kafka producer and consumer.</p> <pre><code>schema.registry.url=http://[schema-registry-host]:8081\n</code></pre> <p>To find the host of Schema Registry, go to <code>Components</code> -&gt; <code>Apache Kafka</code>.  The host name will be found in <code>Schema Registry Host</code> of <code>Status</code> section.</p>"},{"location":"user-guide/spark-azkaban/","title":"Run Spark Jobs with Azkaban","text":"<p><code>Azkaban</code> is used as Workflow in Chango Private.  All the batch jobs like spark jobs and trino ETL jobs using dbt can be integrated with <code>Azkaban</code> in Chango.</p> <p>This example shows how spark jobs can be integrated with Azkaban.</p>"},{"location":"user-guide/spark-azkaban/#run-spark-jobs-on-spark-nodes","title":"Run Spark Jobs on Spark Nodes","text":"<p>Let's run spark pi example job on spark nodes which can be either spark master or spark workers.</p> <p>First, login as <code>spark</code> user.</p> <pre><code>sudo su - spark;\n</code></pre> <p>Run Spark PI example job.</p> <pre><code>export JAVA_HOME=/usr/lib/another-jdk/java-11\nexport SPARK_VERSION=3.4.0\nexport SPARK_HOME=/usr/lib/spark\nexport SPARK_MASTER=spark://chango-private-1.chango.private:7777\n\nspark-submit \\\n--master ${SPARK_MASTER} \\\n--deploy-mode client \\\n--name spark-pi \\\n--class org.apache.spark.examples.SparkPi \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n--num-executors 1 \\\nfile://${SPARK_HOME}/examples/jars/spark-examples_2.12-${SPARK_VERSION}.jar 100;\n</code></pre> <p>Make sure the env value of <code>SPARK_MASTER</code> is set correctly.</p>"},{"location":"user-guide/spark-azkaban/#run-sparks-jobs-from-azkaban-executor-nodes","title":"Run Sparks Jobs from Azkaban Executor Nodes","text":"<p>In order to integrate spark jobs with Azkaban, spark jobs must be run from Azkaban executor nodes via SSH.</p> <p>Let's say, Azkaban executor nodes are <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code>, and Spark Node is <code>chango-private-3.chango.private</code>.</p>"},{"location":"user-guide/spark-azkaban/#password-less-ssh-access-to-spark-node-from-azkaban-executor-nodes","title":"Password-less SSH Access to Spark Node from Azkaban Executor Nodes","text":"<p>To access Spark Node from Azkaban executor nodes via SSH, we need to configure as follows.</p> <p>Copy ssh public key on all the azkaban executor nodes <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code></p> <pre><code>sudo su - azkaban;\ncat ~/.ssh/id_rsa.pub;\n</code></pre> <p>Paste ssh public key of Azkaban executor nodes to <code>authorized_keys</code> on spark node <code>chango-private-3.chango.private</code>.</p> <pre><code>sudo su - spark;\nmkdir -p ~/.ssh\nvi ~/.ssh/authorized_keys\n</code></pre> <p>Add permission on Spark Node <code>chango-private-3.chango.private</code></p> <pre><code>chmod 600 ~/.ssh/authorized_keys\nchmod 700 ~/.ssh\n</code></pre> <p>Test if password-less ssh access to Spark node <code>chango-private-3.chango.private</code> from Azkaban executor nodes <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code>. Move to Azkaban executor nodes, and run the following. <pre><code>sudo su - azkaban;\nssh spark@chango-private-3.chango.private;\n</code></pre></p>"},{"location":"user-guide/spark-azkaban/#run-spark-pi-example-remotely","title":"Run Spark PI Example Remotely","text":"<p>Create spark job run shell file <code>run-spark-pi-client.sh</code> on Spark node <code>chango-private-3.chango.private</code>.</p> <p>Login as <code>spark</code> user on Spark node. <pre><code>sudo su - spark;\n</code></pre></p> <p>Create file <code>run-spark-pi-client.sh</code>.</p> <pre><code>export JAVA_HOME=/usr/lib/another-jdk/java-11\nexport SPARK_VERSION=3.4.0\nexport SPARK_HOME=/usr/lib/spark\nexport SPARK_MASTER=spark://chango-private-1.chango.private:7777\n\nspark-submit \\\n--master ${SPARK_MASTER} \\\n--deploy-mode client \\\n--name spark-pi \\\n--class org.apache.spark.examples.SparkPi \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n--num-executors 1 \\\nfile://${SPARK_HOME}/examples/jars/spark-examples_2.12-${SPARK_VERSION}.jar 100;\n</code></pre> <p>Add permission. <pre><code>chmod +x run-spark-pi-client.sh;\n</code></pre></p> <p>On Azkaban executor nodes <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code>, run spark job remotely.</p> <pre><code>sudo su - azkaban;\nssh spark@chango-private-3.chango.private \"/home/spark/run-spark-pi-client.sh\"\n</code></pre> <p>The output looks like this.</p> <pre><code>client mode..\n23/11/08 00:49:17 INFO SparkContext: Running Spark version 3.4.0\n23/11/08 00:49:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/11/08 00:49:17 INFO ResourceUtils: ==============================================================\n23/11/08 00:49:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n23/11/08 00:49:17 INFO ResourceUtils: ==============================================================\n23/11/08 00:49:17 INFO SparkContext: Submitted application: Spark Pi\n23/11/08 00:49:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -&gt; name: cores, amount: 1, script: , vendor: , memory -&gt; name: memory, amount: 1024, script: , vendor: , offHeap -&gt; name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -&gt; name: cpus, amount: 1.0)\n23/11/08 00:49:17 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n23/11/08 00:49:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n23/11/08 00:49:17 INFO SecurityManager: Changing view acls to: spark\n23/11/08 00:49:17 INFO SecurityManager: Changing modify acls to: spark\n23/11/08 00:49:17 INFO SecurityManager: Changing view acls groups to: \n23/11/08 00:49:17 INFO SecurityManager: Changing modify acls groups to: \n23/11/08 00:49:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY\n23/11/08 00:49:18 INFO Utils: Successfully started service 'sparkDriver' on port 44612.\n23/11/08 00:49:18 INFO SparkEnv: Registering MapOutputTracker\n23/11/08 00:49:18 INFO SparkEnv: Registering BlockManagerMaster\n23/11/08 00:49:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n23/11/08 00:49:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n23/11/08 00:49:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n23/11/08 00:49:18 INFO DiskBlockManager: Created local directory at /export/spark/local/blockmgr-f486810b-b6fc-4973-8c23-491e7c8f43c6\n23/11/08 00:49:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n23/11/08 00:49:18 INFO SparkEnv: Registering OutputCommitCoordinator\n23/11/08 00:49:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n23/11/08 00:49:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n23/11/08 00:49:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n23/11/08 00:49:18 INFO SparkContext: Added JAR file:///usr/lib/spark/examples/jars/spark-examples_2.12-3.4.0.jar at spark://chango-private-3.chango.private:44612/jars/spark-examples_2.12-3.4.0.jar with timestamp 1699404557683\n23/11/08 00:49:18 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://chango-private-1.chango.private:7777...\n23/11/08 00:49:18 INFO TransportClientFactory: Successfully created connection to chango-private-1.chango.private/10.0.0.188:7777 after 26 ms (0 ms spent in bootstraps)\n\n...\n\n23/11/08 00:49:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n23/11/08 00:49:23 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.565842 s\nPi is roughly 3.1414131141413115\n23/11/08 00:49:23 INFO SparkContext: SparkContext is stopping with exitCode 0.\n23/11/08 00:49:23 INFO SparkUI: Stopped Spark web UI at http://chango-private-3.chango.private:4041\n23/11/08 00:49:23 INFO StandaloneSchedulerBackend: Shutting down all executors\n23/11/08 00:49:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n23/11/08 00:49:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n23/11/08 00:49:23 INFO MemoryStore: MemoryStore cleared\n23/11/08 00:49:23 INFO BlockManager: BlockManager stopped\n23/11/08 00:49:23 INFO BlockManagerMaster: BlockManagerMaster stopped\n23/11/08 00:49:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n23/11/08 00:49:23 INFO SparkContext: Successfully stopped SparkContext\n23/11/08 00:49:23 INFO ShutdownHookManager: Shutdown hook called\n23/11/08 00:49:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2184120-a77d-40f1-9dba-6b10c726697a\n23/11/08 00:49:23 INFO ShutdownHookManager: Deleting directory /export/spark/local/spark-ae6bef98-6f06-48ca-8b9d-10c1a2cd2c87\n23/11/08 00:49:23 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...\n23/11/08 00:49:23 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.\n23/11/08 00:49:23 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n</code></pre>"},{"location":"user-guide/spark-azkaban/#integrate-spark-jobs-with-azkaban","title":"Integrate Spark Jobs with Azkaban","text":"<p>There is a CLI tool for Azkaban to create and update azkaban project.  We will use Azkaban CLI to create and update azkaban projects here.</p>"},{"location":"user-guide/spark-azkaban/#experience-sample-projects","title":"Experience Sample Projects","text":"<p>Let's create azkaban sample projects.</p> <p>Login as <code>azkabancli</code> user on Azkaban CLI node.</p> <pre><code>sudo su - azkabancli;\nsource venv/bin/activate;\nazkaban -v;\n</code></pre> <p>Download sample projects.</p> <pre><code>wget https://github.com/azkaban/azkaban/raw/master/az-examples/flow20-projects/basicFlow20Project.zip;\nwget https://github.com/azkaban/azkaban/raw/master/az-examples/flow20-projects/embeddedFlow20Project.zip;\n</code></pre> <p>Create a project on Azkaban Web Server using Azkaban CLI. Enter password <code>azkaban</code>.</p> <pre><code>azkaban upload -c -p basicFlow20Project -u azkaban@http://chango-private-1.chango.private:28081 ./basicFlow20Project.zip;\n</code></pre> <p>After login with the default password <code>azkaban</code> for the user <code>azkaban</code> in Azkaban Web UI, you will see the following picture.</p> <p></p> <p>Execute the flow.</p> <p></p> <p>Let's create another sample project.</p> <pre><code>azkaban upload -c -p embeddedFlow20Project -u azkaban@http://chango-private-1.chango.private:28081 ./embeddedFlow20Project.zip;\n</code></pre> <p></p> <p>Execute the flow.</p> <p></p>"},{"location":"user-guide/spark-azkaban/#create-azkaban-project-for-spark-job","title":"Create Azkaban Project for Spark Job","text":"<p>Create flow file <code>spark-pi-client.flow</code>.</p> <pre><code>---\nconfig:\n  failure.emails: admin@cloudchef-labs.com\n\nnodes:\n  - name: Start\n    type: noop\n\n  - name: SparkPi\n    type: command\n    config:\n      command: ssh spark@chango-private-3.chango.private \"/home/spark/run-spark-pi-client.sh\"\n    dependsOn:\n      - Start\n\n  - name: End\n    type: noop\n    dependsOn:\n      - SparkPi\n</code></pre> <p>Create project file <code>flow20.project</code>.</p> <pre><code>azkaban-flow-version: 2.0\n</code></pre> <p>Package as zip file. <pre><code>zip spark-pi-client.zip spark-pi-client.flow flow20.project \n</code></pre></p> <p>Upload project file.</p> <pre><code>azkaban upload -c -p spark-pi-client -u azkaban@http://chango-private-1.chango.private:28081 ./spark-pi-client.zip\n</code></pre> <p>Now, you will see that the project for spark job has been created.</p> <p></p> <p>Execute spark project flow.</p> <p></p> <p></p> <p>If spark job has been run in <code>client</code> mode, you will see the spark job logs like this.</p> <p></p> <p>You will see the execution history like this.</p> <p></p>"},{"location":"user-guide/streaming/","title":"Send Streaming Events","text":"<p><code>Chango Data API</code> server will collect all the incoming streaming events and produce them to kafka.  <code>Chango Streaming</code> will consume events from kafka and save events to Iceberg table in Chango directly. </p> <p>That is, if you send streaming events to <code>Chango Data API</code> server, then all the streaming events will be inserted to Chango automatically. <code>Chango Client</code> is used to send streaming events to <code>Chango Data API</code> server with ease.</p>"},{"location":"user-guide/streaming/#add-chango-client-library-to-classpath","title":"Add Chango Client Library to Classpath","text":"<p>You can add the following maven dependency to your project.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;co.cloudcheflabs.chango&lt;/groupId&gt;\n  &lt;artifactId&gt;chango-client&lt;/artifactId&gt;\n  &lt;version&gt;2.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>You can also download Chango Client jar file to add it to your application classpath.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-client/releases/download/2.0.0/chango-client-2.0.0-executable.jar;\n</code></pre>"},{"location":"user-guide/streaming/#create-iceberg-table-before-sending-json-events","title":"Create Iceberg Table before Sending JSON Events","text":"<p>Before sending json as streaming events to <code>Chango Data API</code> server, Iceberg table needs to be created beforehand with  trino clients like Trino CLI and Apache Superset.</p> <p>It is good practice that Iceberg table for streaming events needs to be partitioned with date, for example, <code>year</code>, <code>month</code> and <code>day</code>. In addition, timestamp column like <code>ts</code> also needs to be added to Iceberg table for compacting small files.</p> <p>Especially, in order to compact small files and add partitioning in Iceberg table in Chango, you need to follow the rules.</p> <p>The name of timestamp column must be <code>ts</code> whose type is <code>bigint</code> which is equivalent to <code>long</code> in java.  Column names <code>year</code>, <code>month</code>, <code>day</code> will be used as partitioning columns.</p> <ul> <li><code>ts</code>:  the number of milliseconds since 1970-01-01 00:00:00.</li> <li><code>year</code>: year with the format of <code>yyyy</code> which is necessary for partitioning.</li> <li><code>month</code>: month of the year with the format of <code>MM</code> which is necessary for partitioning.</li> <li><code>day</code>: day of the month with the format of <code>dd</code> which is necessary for partitioning.</li> </ul> <p>For example, create <code>logs</code> table with partitioning and timestamp.</p> <pre><code>-- create iceberg schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- create iceberg table.\nCREATE TABLE iceberg.iceberg_db.logs (\n    day varchar,\n    level varchar,\n    message varchar,\n    month varchar,\n    ts bigint,\n    year varchar \n)\nWITH (\n    partitioning=ARRAY['year', 'month', 'day'],\n    format = 'PARQUET'\n);\n</code></pre> <p>NOTE: The sequence  of table column names in lower case must be alphanumeric in ascending order.</p> <p>You can create Iceberg table with Superset provided by Chango Private like this.</p> <p></p>"},{"location":"user-guide/streaming/#send-json-events-with-chango-client","title":"Send JSON Events with Chango Client","text":"<p>It is very simple to use Chango Client API in your code. </p> <p>You can construct <code>ChangoClient</code> instance like this. <pre><code>        ChangoClient changoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval\n);\n</code></pre></p> <ul> <li><code>token</code> : Data access credential issued by <code>Chango Authorizer</code>.</li> <li><code>dataApiServer</code> : Endpoint URL of <code>Chango Data API</code>.</li> <li><code>schema</code>: Target Iceberg schema which needs to be created before sending json data to chango.</li> <li><code>table</code>: Target Iceberg table which also needs to be created beforehand.</li> <li><code>batchSize</code> : The size of json list which will be sent to chango in batch mode and in gzip format.</li> <li><code>interval</code> : Json data will be queued internally in chango client. The queued json list will be sent in this period whose unit is milliseconds.</li> </ul> <p>In order to get <code>token</code>, see Get Chango Credential.</p> <p>To get the endpoint of <code>Chango Data API</code>, go to <code>Components</code> -&gt; <code>Chango Data API</code>.</p> <ul> <li>Get URL in <code>Endpoint</code> section.</li> </ul> <p>And send JSON events.</p> <pre><code>        // send json.\nchangoClient.add(json);\n</code></pre> <p>Let's see the full codes to send JSON events.</p> <pre><code>import co.cloudcheflabs.chango.client.component.ChangoClient;\nimport com.cloudcheflabs.changoprivate.example.util.JsonUtils;\nimport org.joda.time.DateTime;\nimport org.junit.Test;\nimport java.util.HashMap;\nimport java.util.Map;\npublic class SendLogsToDataAPI {\n@Test\npublic void sendLogs() throws Exception {\nString token = System.getProperty(\"token\");\nString dataApiServer = System.getProperty(\"dataApiServer\");\nString table = System.getProperty(\"table\");\nint batchSize = 10000;\nlong interval = 1000;\nString schema = \"iceberg_db\";\nChangoClient changoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval\n);\nlong count = 0;\nwhile (true) {\nint MAX = 50 * 1000;\nfor(int i = 0; i &lt; MAX; i++) {\nMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();\nDateTime dt = DateTime.now();\nString year = String.valueOf(dt.getYear());\nString month = padZero(dt.getMonthOfYear());\nString day = padZero(dt.getDayOfMonth());\nlong ts = dt.getMillis(); // in milliseconds.\nmap.put(\"level\", \"INFO\");\nmap.put(\"message\", \"any log message ... [\" + count + \"]\");\nmap.put(\"ts\", ts);\nmap.put(\"year\", year);\nmap.put(\"month\", month);\nmap.put(\"day\", day);\nString json = JsonUtils.toJson(map);\n// send json.\nchangoClient.add(json);\ncount++;\n}\nThread.sleep(10 * 1000);\nSystem.out.println(\"log [\" + count +\"] sent...\");\n}\n}\nprivate String padZero(int value) {\nString strValue = String.valueOf(value);\nif(strValue.length() == 1) {\nstrValue = \"0\" + strValue;\n}\nreturn strValue;\n}\n}\n</code></pre> <p>Take a look at the value of <code>ts</code> must be the number of milliseconds since 1970-01-01 00:00:00.</p>"},{"location":"user-guide/streaming/#run-query-in-iceberg-table-for-streaming-events","title":"Run Query in Iceberg Table for Streaming Events","text":"<p>You can run queries in iceberg table <code>logs</code> to which streaming events are inserted.</p> <pre><code>-- select with partitioning columns.\nselect *, from_unixtime(ts/1000) from iceberg.iceberg_db.logs where year = '2023' and month = '11' and day = '07' limit 1000;\n</code></pre> <p></p>"},{"location":"user-guide/upload/","title":"Upload Files","text":"<p>Using <code>Chango CLI</code>, CSV, JSON, and Excel Files can be uploaded whose data will be inserted to Iceberg table in Chango automatically.</p>"},{"location":"user-guide/upload/#install-chango-cli","title":"Install Chango CLI","text":"<p>Download chango client jar and install it as Chango CLI. <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-client/releases/download/2.0.0/chango-client-2.0.0-executable.jar;\ncp chango-client-2.0.0-executable.jar ~/bin/chango;\nchmod +x ~/bin/chango;\n</code></pre></p>"},{"location":"user-guide/upload/#initialize-chango-cli","title":"Initialize Chango CLI","text":"<p>Credential needs to be entered when Chango CLI is initialized. To Get Chango Credential, see Get Chango Credential. <pre><code>chango init\n</code></pre></p> <p>After credential entered, output looks like this. <pre><code>chango init\n\nEnter Token: xxxxxxxx\nInitialization success!\n</code></pre></p>"},{"location":"user-guide/upload/#upload-json","title":"Upload JSON","text":"<p>You can upload individual JSON file or all the JSON files in directory.</p> <p>NOTE: Before uploading JSON files, target Iceberg table needs to be created beforehand.  See here for more details.</p> <p>Upload JSON file.</p> <pre><code>chango upload json local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--file /home/chango/multi-line-json.json \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>schema</code> : iceberg schema created before.</li> <li><code>table</code>: iceberg table where json data will be ingested in chango.</li> <li><code>file</code> : local json file path.</li> <li><code>batch-size</code> : list of json in gzip will be sent in batch mode. The size of json list.</li> </ul> <p>Upload all JSON files in directory.</p> <pre><code>chango upload json local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--directory /home/chango/json-files \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>directory</code>: the directory where json files are located which will be sent to chango.</li> </ul> <p><code>[endpoint-of-data-api]</code> is the endpoint of <code>Chango Data API</code>, go to <code>Components</code> -&gt; <code>Chango Data API</code>, and get URL in <code>Endpoint</code> section.</p>"},{"location":"user-guide/upload/#upload-excel","title":"Upload Excel","text":"<p>You don't have to create Iceberg table before uploading Excel to chango. Chango will create Iceberg table automatically.</p> <p>NOTE: Take a note that the header of Excel must exist.</p> <p>You can also create Iceberg table(for example, Iceberg table with the definition of partition columns) before uploading Excel to chango.</p> <pre><code>chango upload excel local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table excel_to_json \\\n--file /home/chango/data/excel-to-json.xlsx \\\n;\n</code></pre>"},{"location":"user-guide/upload/#upload-csv","title":"Upload CSV","text":"<p>As like Excel, you don\u2019t have to create Iceberg table beforehand. Chango will create Iceberg table automatically.</p> <p>NOTE: Take a note that the header of CSV must exist.</p> <p>You can also define a Iceberg table(for example, Iceberg table with the definition of partition columns) before uploading CSV to chango.</p> <p>The following example is to upload CSV with the separator of comma.</p> <pre><code>chango upload csv local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--file /home/chango/data/csv-to-json-comma.csv \\\n;\n</code></pre> <ul> <li><code>separator</code> is the separator of csv data. If csv data is tab separated, the value is <code>TAB</code> . But for another separators, you need to type a separator value, for instance,  <code>,</code> , <code>|</code> , or something else.</li> <li><code>is-single-quote</code> is if the escaped value of csv data is single quoted or not. Default is <code>false</code>.</li> </ul> <p>For tab separated CSV, use the following.</p> <pre><code>chango upload csv local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table csv_to_json_tab \\\n--separator TAB \\\n--is-single-quote false \\\n--file /home/chango/data/csv-to-json-tab.csv \\\n;\n</code></pre> <ul> <li>parameter <code>separator</code> value must be <code>TAB</code>.</li> </ul> <p>You can upload multiple CSV files located in the directory.</p> <pre><code>chango upload csv local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--directory /home/chango/local-csvs \\\n;\n</code></pre>"}]}