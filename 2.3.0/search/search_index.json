{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Chango Makes Your Life Simpler <p>       Let's get started with Chango Private.     </p> <p>       Join Chango Discussion to discuss Chango Private.     </p>"},{"location":"components/components/","title":"Chango Private Components","text":""},{"location":"components/components/#component-version","title":"Component Version","text":"Component Version Apache Ozone 1.3.0 Kafka 3.4.x Zookeeper 3.6.4 Schema Registry 7.4.0 Spark 3.4.3 / Scala 2.12 Livy 0.8.0-incubating Trino 435 Iceberg 1.4.2 Open JDK 8 / 11 / 17 Azkaban 3.69.0-8 Azkaban CLI 0.9.14 Superset 0.38.1 dbt dbt-core: 1.2.1, dbt-trino: 1.2.2 Chango Private Admin 2.3.0 Chango Private Streaming 2.3.0 Chango Private Streaming Tx 2.3.0 Chango Spark Thrift Server 2.3.0 Chango Spark SQL Runner 2.3.0 Chango Private Data API 2.3.0 Chango Private Trino Gateway 2.3.0 Chango Private Authorizer 2.3.0 Chango Private REST Catalog 2.3.0 Chango Private Query Exec 2.3.0"},{"location":"components/components/#component-ports","title":"Component Ports","text":""},{"location":"components/components/#azkaban","title":"Azkaban","text":"<ul> <li>28081: Azkaban Web Server</li> <li>28082: Azkaban Executor Server</li> </ul>"},{"location":"components/components/#kafka","title":"Kafka","text":"<ul> <li>9092: Kafka Server</li> </ul>"},{"location":"components/components/#schema-registry","title":"Schema Registry","text":"<ul> <li>8081: Registry Server</li> </ul>"},{"location":"components/components/#mysql","title":"MySQL","text":"<ul> <li>3306: MySQL Server</li> </ul>"},{"location":"components/components/#ozone","title":"Ozone","text":"<ul> <li>9862: OM RPC</li> <li>9872: OM Ratis</li> <li>9874: OM HTTP</li> <li>9875: OM HTTPS</li> <li>9861: SCM Data Node</li> <li>9863: SCM Block Client</li> <li>9860: SCM Client</li> <li>9876: SCM HTTP</li> <li>9877: SCM HTTPS</li> <li>9894: SCM Ratis</li> <li>9895: SCM GRPC</li> <li>9961: SCM Security Service</li> <li>9882: Data Node HTTP</li> <li>9883: Data Node HTTPS</li> <li>9858: Container Ratis IPC</li> <li>9859: Container IPC</li> <li>9864: Data Node Client</li> <li>9855: Container Ratis Data Stream</li> <li>9857: Container Ratis Admin</li> <li>9856: Container Ratis Server</li> <li>9878: S3G HTTP</li> <li>9879: S3G HTTPS</li> <li>9891: Recon RPC</li> <li>9888: Recon HTTP</li> <li>9889: Recon HTTPS</li> <li>9884: Freon HTTP</li> <li>9885: Freon HTTPS</li> <li>88: KDC Server</li> <li>749: KDC Admin Server</li> <li>80: NGINX</li> </ul>"},{"location":"components/components/#spark","title":"Spark","text":"<ul> <li>7777: Master Server</li> <li>7778: Worker Server</li> <li>8880: Master Web UI</li> <li>8881: Worker Web UI</li> <li>18882: History Server</li> </ul>"},{"location":"components/components/#livy","title":"Livy","text":"<ul> <li>8998: Server</li> </ul>"},{"location":"components/components/#superset","title":"Superset","text":"<ul> <li>38088: Superset Server</li> </ul>"},{"location":"components/components/#trino","title":"Trino","text":"<ul> <li>18080: Coordinator </li> <li>18081: Worker</li> </ul>"},{"location":"components/components/#zookeeper","title":"Zookeeper","text":"<ul> <li>2181: Zookeeper Server </li> <li>2888: Zookeeper Peer Port </li> <li>3888: Zookeeper Leader Port</li> </ul>"},{"location":"components/components/#postgresql","title":"PostgreSQL","text":"<ul> <li>5432: PostgreSQL Server</li> </ul>"},{"location":"components/components/#chango-private-admin","title":"Chango Private Admin","text":"<ul> <li>28095: Admin Server </li> <li>8080: NGINX</li> </ul>"},{"location":"components/components/#chango-private-admin-ui","title":"Chango Private Admin UI","text":"<ul> <li>8123: Admin UI Server</li> </ul>"},{"location":"components/components/#chango-spark-thrift-server","title":"Chango Spark Thrift Server","text":"<ul> <li>10000: Server</li> </ul>"},{"location":"components/components/#chango-spark-sql-runner","title":"Chango Spark SQL Runner","text":"<ul> <li>29080: Server</li> </ul>"},{"location":"components/components/#chango-private-data-api","title":"Chango Private Data API","text":"<ul> <li>28097: Server </li> <li>28098: Jetty Server </li> <li>27072: Management Server </li> <li>80: NGINX</li> </ul>"},{"location":"components/components/#chango-private-trino-gateway","title":"Chango Private Trino Gateway","text":"<ul> <li>28096: Server </li> <li>27081: Management Server </li> <li>27788: Jetty Proxy Server </li> <li>6379: Redis Master</li> <li>6380: Redis Slave</li> <li>16379: Redis Master Cluster</li> <li>16380: Redis Slave Cluster</li> <li>443: NGINX SSL</li> </ul>"},{"location":"components/components/#chango-private-authorizer","title":"Chango Private Authorizer","text":"<ul> <li>28196: Server </li> <li>27181: Management Server </li> <li>81: NGINX</li> </ul>"},{"location":"components/components/#chango-private-rest-catalog","title":"Chango Private REST Catalog","text":"<ul> <li>28197: Server </li> <li>27182: Management Server </li> <li>28297: Jetty Proxy Server </li> <li>8008: NGINX</li> </ul>"},{"location":"components/components/#chango-private-query-exec","title":"Chango Private Query Exec","text":"<ul> <li>28291: Server</li> <li>27291: Management Server</li> <li>8009: NGINX</li> </ul>"},{"location":"features/ingestion/","title":"Data Ingestion","text":"<p>Chango Private provides components like <code>Chango Data API</code> and <code>Chango Streaming</code> to insert external data to Chango with ease.</p>"},{"location":"features/ingestion/#upload-files","title":"Upload Files","text":"<p>As data analytics engineer, you don\u2019t have to struggle with long row Excel to analyze data. SQL is better to analyze data.  External data like CSV, Excel, JSON can be inserted directly to iceberg table in chango,  then you can explore and analyze iceberg tables with trino queries.</p>"},{"location":"features/ingestion/#streaming","title":"Streaming","text":"<p>External event streaming application can insert streaming events into iceberg tables in chango without building streaming platform and writing streaming jobs.</p>"},{"location":"features/ingestion/#no-streaming-platform-no-streaming-jobs","title":"No Streaming Platform, No Streaming Jobs","text":"<p>If you want to insert streaming events like user behavior events, logs, IoT events to data lakehouses, you need to build event streaming platform like kafka and write streaming jobs like spark streaming jobs in most cases. But in chango, you don't have to do so.</p> <p></p> <p>External streaming application can insert streaming events to iceberg tables in chango directly without streaming platform and streaming jobs.</p>"},{"location":"features/ingestion/#aggregate-logs","title":"Aggregate Logs","text":"<p>Chango Log is a log agent to read local log files and send logs to Chango to analyze logs.  Using <code>Chango Log</code>, you can analyze logs from all your distributed logs joining different databases in richer manner realtimely in Chango. See Reading Log Files Using Chango Log for more details.</p>"},{"location":"features/ingestion/#change-data-capture","title":"Change Data Capture","text":"<p>Chango CDC is Change Data Capture application to catch CDC data of database and send CDC data to Chango. You don't need such as Kafka and Kafka Connect cluster to accomplish CDC. See Change Data Capture Using Chango CDC for more details.</p>"},{"location":"features/rbac/","title":"Storage Security","text":"<p>Chango Private provides fine-grained data access control using RBAC to Chango storage.</p>"},{"location":"features/rbac/#data-access-in-secure-way","title":"Data Access in Secure Way","text":"<p>Chango Authorizer controls all the data access to Chango Data Lakehouse.  So all the chango components which want to access data in Chango Data Lakehouse need to be authenticated and authorized by Chango Authorizer.</p> <p>All data accesses are controlled in the fine-grained manner like catalog, schema and table level. </p>"},{"location":"features/rbac/#credential-role-and-privileges","title":"Credential, Role and Privileges","text":"<p>A <code>Role</code> can have many <code>Credentials</code> and many <code>Privileges</code>. There are <code>READ</code> and <code>WRITE</code> type in privilege.  Each privilege has storage access path with the convention of <code>&lt;catalog&gt;</code>.<code>&lt;schema&gt;</code>.<code>&lt;table&gt;</code>, for example.</p> <ul> <li><code>iceberg.events.behavior</code> with <code>WRITE</code> : user / credential has the <code>WRITE</code> privilege to table behavior in <code>events</code> schema of <code>iceberg</code> catalog.</li> <li><code>iceberg.events.*</code> with <code>READ</code>: user / credential has the <code>READ</code> privilege to all the tables in <code>events</code> schema of <code>iceberg</code> catalog.</li> <li><code>mysql.*</code> with <code>READ</code>: user / credential has the <code>READ</code> privilege to all the tables in all schemas of <code>mysql</code> catalog.</li> <li><code>*</code> with <code>WRITE</code>: user / credential has the <code>WRITE</code> privilege to all the tables in all schemas of all catalogs.</li> </ul>"},{"location":"features/realtime/","title":"Realtime Analytics","text":"<p>Chango Private provides several solutions for realtime analytics.  Streaming events from external applications, distributed logs and CDC(Change Data Capture) data will be inserted to Iceberg tables  in Chango and will be analyzed in realtime for example using <code>Trino</code> provided by Chango.</p>"},{"location":"features/realtime/#streaming","title":"Streaming","text":"<p>External streaming application can insert streaming events to Chango using Chango Client. See Send Streaming Events for more details.</p>"},{"location":"features/realtime/#aggregate-logs","title":"Aggregate Logs","text":"<p>Chango Log is a log agent to read local log files and send logs to Chango to analyze logs.  See Reading Log Files Using Chango Log for more details.</p>"},{"location":"features/realtime/#change-data-capture","title":"Change Data Capture","text":"<p>Chango CDC is Change Data Capture application to catch CDC data of database and send CDC data to Chango. See Change Data Capture Using Chango CDC for more details.</p>"},{"location":"features/transform/","title":"Data Transformation","text":"<p>In most of cases, trino ETL queries will be used to transform data in Chango,  and all the ETL jobs of trino queries need to be integrated with workflow like <code>Azkaban</code>.</p>"},{"location":"features/transform/#chango-query-exec","title":"Chango Query Exec","text":"<p><code>Chango Query Exec</code> is a REST application to execute trino ETL queries to transform data in Chango.  You just send trino ETL queries to <code>Chango Query Exec</code> through REST using such as <code>curl</code>. </p> <p>All the ETL jobs need to be integrated with workflow engine like <code>Azkaban</code>. Let's see the following pictures to run trino ETL queries integrated with workflow <code>Azkaban</code> using <code>dbt</code> and <code>Chango Query Exec</code>.</p> <p></p> <p>A workflow DAG task runs dbt model to run trino ETL queries.</p> <p></p> <p>In similar way, a workflow DAG task sends trino ETL queries to <code>Chango Query Exec</code> through REST to run trino ETL queries.</p> <p>As seen, both of them have similar approach to run trino ETL queries, but <code>Chango Query Exec</code> has several advantages, for example.</p> <ul> <li>send trino ETL queries via REST simply without additional tool and library installation.</li> <li>just use the same trino ETL queries which you already used to explore data with your BI tools.</li> <li>easy way to integrate with workflow engine.</li> </ul> <p>See Data Transformation Using Chango Query Exec.</p>"},{"location":"features/trino-gw/","title":"Trino Gateway","text":"<p><code>Chango Trino Gateway</code> is an implementation of trino gateway concept.</p>"},{"location":"features/trino-gw/#understand-trino-gateway-concept","title":"Understand Trino Gateway Concept","text":"<p>Chango has the concept of trino gateway which routes trino queries to upstream backend trino clusters dynamically.  If one of the backend trino clusters has exhausted, then trino gateway will route queries to the trino cluster  which is executing less requested queries. Trino does not support HA because trino coordinator has single point failure.  In order to support HA of trino, we need to use trino gateway.</p> <p>Let\u2019s say, there is only one large trino clusters(100 ~ 1000 workers) in the company. Many people like BI experts, data scientists,  and data engineers are running trino queries on this large cluster intensively.  These trino queries can be interactive or ETL. For example, because long running ETL queries have occupied most of the resources  which trino cluster needs to execute another queries, there can be little resources remaining trino cluster can use  to execute another interactive queries. The people who are running the interactive queries need to wait  until the long running ETL queries will finish. Such conflict problems can also happen in reverse case.</p> <p>Such monolithic approach with large trino workers can lead to be problematic. We need to separate a large trino cluster  to small trino clusters for the groups like BI team, data scientist team, and data engineer team individually.</p> <p>Let\u2019s say, BI team has 3 backeend trino clusters. If one of trino clusters needs to be scaled out, and trino catalogs needs  to be added to this trino cluster for new external data sources, or the trino cluster needs to be reinstalled with new trino version,  then  first just deactivate this trino cluster to which the queries will not be routed without down time problem of trino query execution.  After finishing scaling workers ,updating catalogs or reinstalling the trino cluster, activate this trino cluster to which the queries  will be routed again. With trino gateway, the activation and deactivation of backend trino clusters can be done with ease. </p>"},{"location":"features/trino-gw/#chango-trino-gateway-features","title":"Chango Trino Gateway Features","text":"<p><code>Chango Trino Gateway</code> provides several critical functions of trino gateway.</p>"},{"location":"features/trino-gw/#trino-user-authentication-and-authorization","title":"Trino User Authentication and Authorization","text":"<p>If trino user sends trino queries to <code>Chango Trino Gateway</code>, <code>Chango Trino Gateway</code> authenticates trino user and  authorize trino queries if trino user is allowed to access data in Chango or not. </p> <p>For authorization, <code>Chango Trino Gateway</code> works with <code>Chango Authorizer</code> which is central to authorize all credentials used by all chango components. So, <code>Chango Trino Gateway</code> controls the data access of trino user to Chango with RBAC in the fine-grained manner like catalog, schema and table level.</p> <p>Especially, RBAC to <code>Cluster Group</code> can be configured without restarting backend trino clusters.</p>"},{"location":"features/trino-gw/#route-trino-queries-to-trino-clusters","title":"Route Trino Queries to Trino Clusters","text":"<p>Routing trino queries to the backend trino clusters is fundamental function of <code>Chango Trino Gateway</code>. Let's see the following picture how <code>Chango Trino Gateway</code> will route.</p> <p></p> <p>Trino queries which trino users run who belong to the <code>Cluster Group</code> will be routed to the backend trino clusters which belong to the <code>Cluster Group</code>. <code>Chango Trino Gateway</code> detects exhausted trino clusters, and will route trino queries to less exhausted trino clusters in smart way.</p> <p>From the point of the storage security in Chango, <code>Cluster Group</code> is equivalent to <code>Role</code> in <code>Chango Security</code> model.  <code>Chango Trino Gateway</code> will check if trino queries are allowed to run according to privileges of <code>Cluster Group</code> or not.</p> <p>Even if you have just one trino cluster as backend trino cluster, you can create several <code>Cluster Groups</code>  as many as you need, and you can control data access of <code>Cluster Groups</code> as usual in <code>Chango Security</code> model. </p>"},{"location":"features/trino-gw/#activate-and-deactivate-trino-clusters","title":"Activate and Deactivate Trino Clusters","text":"<p><code>Chango Trino Gateway</code> provides the feature of activation and deactivation of trino clusters. If one of the backend trino clusters  is deactivated, trino queries will not be routed to this trino cluster. If the trino cluster is activated, then trino queries will be routed to this  trino cluster.</p> <p>For example, if the configuration of backend trino clusters is updated, the backend trino clusters need to be restarted. In order to provide <code>Zero Downtime to Run Trino Queries</code>, <code>Chanog Trino Gateway</code> provides such feature of activation and deactivation of trino clusters.</p> <p>Do the following steps for every trino clusters in order to accomplish <code>Zero Downtime to Run Trino Queries</code>.</p> <ul> <li>deactivate the trino cluster whose configuration needs to be updated.</li> <li>update configuration.</li> <li>restart that trino cluster. </li> <li>activate that trino cluster again. </li> </ul>"},{"location":"install/install-admin/","title":"Install Chango Admin","text":"<p>Chango Private is Data Lakehouse Platform which can be installed in both connected and disconnected environment.</p> <p>Chango Private consists of <code>Chango Admin</code> and <code>Chango Components</code>. All the <code>Chango Components</code> will be installed by <code>Chango Admin</code>,  so you need to install <code>Chango Admin</code> before installing <code>Chango Components</code>.</p>"},{"location":"install/install-admin/#prerequisites","title":"Prerequisites","text":"<p>There are several things to prepare before proceeding to install Chango Private.</p>"},{"location":"install/install-admin/#supported-os-and-python","title":"Supported OS and Python","text":"<p>Supported OS and Python Version are:</p> <ul> <li><code>CentOS 7.9</code></li> <li><code>Python 3.6.8</code></li> </ul>"},{"location":"install/install-admin/#prepare-chango-nodes","title":"Prepare Chango Nodes","text":"<p>In order to install Chango Private, we need nodes. Let's call it as <code>Chango Nodes</code>. <code>Chango Nodes</code> consist of <code>Chango Admin Node</code> on which Chango Admin will be installed and <code>Chango Component Nodes</code> on which  Chango Components will be installed.</p> <p></p> <p>First, you will install <code>Chango Admin</code>, and then all the components will be installed by <code>Chango Admin</code> through ansible playbook using SSH. So, you need to prepare <code>at least 4</code> nodes to install Chango Private.</p> <p>NOTE: 1 node for <code>Chango Admin Node</code> and <code>at least 3</code> nodes for <code>Chango Comonent Nodes</code>. That is, you need to have <code>at least 4</code>nodes to install Chango Private.</p> <p>With respect to hardware requirement, 2 cores and 4GB memory is good for <code>Chango Admin Node</code>, but for <code>Chango Component Nodes</code>,  there are many variations to determine how much capacity you need for <code>Chango Component Nodes</code>.  The following components provided by Chango Private are the components which can affect the determination of hardware capacity.</p> <ul> <li>Trino</li> <li>Apache Spark</li> <li>Apache Ozone</li> <li>Apache Kafka</li> </ul>"},{"location":"install/install-admin/#open-ports","title":"Open Ports","text":"<p>You need to open ports in your subnet of <code>Chango Nodes</code>. See the details of Chango Private Ports.</p>"},{"location":"install/install-admin/#password-less-ssh-connection","title":"Password-less SSH Connection","text":"<p>From the node of <code>Chango Admin Node</code>, <code>sudo</code> user must access to <code>Chango Component Nodes</code> with password-less SSH connection.</p> <p>The following needs to be configured.</p> <ul> <li><code>sudo</code> user with the same name needs to be created on all the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>.</li> <li>configure password-less SSH connection with the created <code>sudo</code> user from <code>Chango Admin Node</code> to <code>Chango Component Nodes</code>.</li> <li>configure password-less SSH connection with the created <code>sudo</code> user from <code>Chango Admin Node</code> to the self host of <code>Chango Admin Node</code>.</li> </ul>"},{"location":"install/install-admin/#configure-host-names","title":"Configure Host Names","text":"<p>You need to configure host name on all the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>.</p> <p>For example, the following DNS entry is configured in <code>/etc/hosts</code> on <code>Chango Admin Node</code>. <pre><code>10.0.0.100  chango-admin.chango.private chango-admin\n</code></pre> And set host name with FQDN. <pre><code>sudo hostnamectl set-hostname chango-admin.chango.private\n</code></pre></p>"},{"location":"install/install-admin/#create-sudo-user","title":"Create <code>sudo</code> User","text":"<p>For example, we will create <code>sudo</code> user with the name of <code>chango</code>. <pre><code>sudo su -;\nuseradd -m chango;\npasswd chango;\n</code></pre></p> <p>In order to add to sudo user,  <pre><code>sudo visudo;\n</code></pre></p> <p>And add the following. <pre><code>chango   ALL=(ALL) NOPASSWD: ALL\n</code></pre></p> <p>NOTE: You need to create such <code>sudo</code> user on all the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>.</p>"},{"location":"install/install-admin/#create-ssh-keys","title":"Create SSH Keys","text":"<p>You need to generate SSH keys on <code>Chango Admin Node</code> logged in as the <code>sudo</code> user created before. <pre><code>sudo su - chango\n</code></pre></p> <pre><code>ssh-keygen -t rsa;\n</code></pre> <p>Configure SSH with no key checking. Open SSH configuration file. <pre><code>vi ~/.ssh/config\n</code></pre> Add the following. <pre><code>StrictHostKeyChecking no\n</code></pre></p> <p>Add permission to configuration. <pre><code>chmod 600 ~/.ssh/config\n</code></pre></p> <p>Copy the created public key on <code>Chango Admin Node</code> to all the nodes of <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code>.</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Open <code>authorized_keys</code> on the nodes of <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code>, and paste the public key to it. <pre><code>vi ~/.ssh/authorized_keys\n</code></pre></p> <p>NOTE: Take a note that you also need to add public key to the self of <code>Chango Admin Node</code>.</p> <p>And add permission. <pre><code>chmod 600 ~/.ssh/authorized_keys\n</code></pre></p> <p>And, test if password-less ssh access works from <code>Chango Admin Node</code> to the nodes of <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code></p> <pre><code># access to nodes of chango component nodes.\nssh chango-comp-1.chango.private;\nssh chango-comp-2.chango.private;\nssh chango-comp-3.chango.private;\nssh chango-comp-4.chango.private;\n\n# access to self host of chango admin node.\nssh chango-admin.chango.private\n</code></pre>"},{"location":"install/install-admin/#attache-raw-disks-to-chango-component-nodes","title":"Attache Raw Disks to <code>Chango Component Nodes</code>","text":"<p>As depicted in the above picture, several Raw Disks must be attached to <code>Chango Component Nodes</code>. But you don\u2019t have to mount any disks to <code>Chango Admin Node</code>.</p> <p>NOTE: Raw disks attached to <code>Chango Component Nodes</code> MUST NOT be mounted! Chango will mount attached disks as logical volume by installing Chango Components later.</p>"},{"location":"install/install-admin/#set-selinux-to-permissive","title":"Set Selinux to Permissive","text":"<p>You need to set selinux to <code>permissive</code> on all <code>Chango Nodes</code> with the following command.</p> <pre><code>sudo setenforce 0\nsudo sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/selinux/config\nsudo sed -i 's/SELINUX=disabled/SELINUX=permissive/g' /etc/selinux/config\n</code></pre>"},{"location":"install/install-admin/#install-python-36","title":"Install Python 3.6","text":"<p>For online environment, install python 3.6 on all the <code>Chango Nodes</code>.</p> <pre><code>sudo yum -y install wget make gcc openssl-devel bzip2-devel\n\ncd /tmp/\nwget https://www.python.org/ftp/python/3.6.12/Python-3.6.12.tgz\n\ntar xzf Python-3.6.12.tgz\ncd Python-3.6.12\n./configure --enable-optimizations\nsudo make altinstall\n\nsudo ln -sfn /usr/local/bin/python3.6 /usr/bin/python3.6\nsudo ln -sfn /usr/local/bin/python3.6 /usr/bin/python3\nsudo ln -sfn /usr/local/bin/pip3.6 /usr/bin/pip3.6\nsudo ln -sfn /usr/local/bin/pip3.6 /usr/bin/pip3\n</code></pre>"},{"location":"install/install-admin/#install-lvm","title":"Install LVM","text":"<p>For online environment, install LVM on all the <code>Chango Nodes</code>.</p> <pre><code>sudo yum install lvm2 -y\n</code></pre>"},{"location":"install/install-admin/#add-yum-repositories","title":"Add Yum Repositories","text":"<p>For online environment, add the following yum repository on all the <code>Chango Nodes</code>.</p> <pre><code>sudo yum install epel-release -y\n</code></pre> <p>NOTE: For offline and disconnected environment, you don't have to do it.</p>"},{"location":"install/install-admin/#local-yum-repository","title":"Local Yum Repository","text":"<p>If you want to install Chango Private in disconnected environment, you need to install local yum repository which all the <code>Chango Nodes</code> will look up.</p> <p>NOTE: If you want to install Chango Private in connected environment, skip this instruction.</p>"},{"location":"install/install-admin/#on-the-node-of-local-yum-repository","title":"On the Node of Local Yum Repository","text":"<p>On the host of local yum repository, you need to follow the below instructions.</p> <p>Install NGINX proxy.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-libs/releases/download/chango-private-deps/nginx-1.14.0-1.el7_4.ngx.x86_64.rpm\nsudo rpm -Uvh nginx-1.14.0-1.el7_4.ngx.x86_64.rpm\n</code></pre> <p>Open port of <code>80</code>. <pre><code>sudo firewall-cmd --zone=public --add-port=80/tcp --permanent;\nsudo firewall-cmd --reload;\n</code></pre></p> <p>Install yum utils. <pre><code>sudo yum install createrepo  yum-utils -y;\n</code></pre></p> <p>Create directories of local yum repository. <pre><code># local repo directory.\nexport LOCAL_REPO_BASE=/data/var/www/html\nexport LOCAL_REPO_DIR=${LOCAL_REPO_BASE}/repos;\n\n# create repo directories.\nsudo mkdir -p ${LOCAL_REPO_DIR}/{base,epel,extras,updates};\n</code></pre></p> <p>Synchronize Yum Repos to Local Repo. <pre><code># synchronize repositories to local.\nsudo reposync -g -l -d -m --repoid=base --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\nsudo reposync -g -l -d -m --repoid=epel --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\nsudo reposync -g -l -d -m --repoid=extras --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\nsudo reposync -g -l -d -m --repoid=updates --newest-only --download-metadata --download_path=${LOCAL_REPO_DIR}/\n\n# create new repo for local.\nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/base/  \nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/epel/    \n\nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/extras/  \nsudo createrepo ${LOCAL_REPO_DIR}/extras/\nsudo createrepo -g comps.xml ${LOCAL_REPO_DIR}/updates/  \nsudo createrepo ${LOCAL_REPO_DIR}/updates/\n</code></pre></p> <p>Create NGINX configuration. <pre><code>sudo vi /etc/nginx/conf.d/repos.conf \n</code></pre></p> <p>, and add the following configuration. <pre><code>server {\n        listen   80;\n        server_name  chango-private-yum-local-repo.chango.private;\n        root   /data/var/www/html/repos;\n        location / {\n                index  index.php index.html index.htm;\n                autoindex on;\n        }\n}\n</code></pre></p> <p>Remove NGINX default configuration. <pre><code>sudo rm -rf /etc/nginx/conf.d/default.conf;\n</code></pre></p> <p>Start NGINX. <pre><code>sudo systemctl start nginx;\n</code></pre></p> <p>Now, you can see the list of RPMs. <pre><code>http://[local-yum-repo-ip]/\n</code></pre></p> <p>If internet is available to the host of local yum repository, create crontab to update local yum repository.</p> <pre><code>sudo vi /etc/cron.daily/update-localrepos\n</code></pre> <p>Add the following job. <pre><code>#!/bin/bash\n##specify all local repositories in a single variable\nLOCAL_REPOS=\u201dbase epel extras updates\u201d\n##a loop to update repos one at a time \nfor REPO in ${LOCAL_REPOS}; do\nreposync -g -l -d -m --repoid=$REPO --newest-only --download-metadata --download_path=/data/var/www/html/repos/\ncreaterepo -g comps.xml /data/var/www/html/repos/$REPO/  \ncreaterepo /data/var/www/html/repos/$REPO/\n</code></pre></p> <p>, and add permission. <pre><code>sudo chmod 755 /etc/cron.daily/update-localrepos\n</code></pre></p>"},{"location":"install/install-admin/#on-the-nodes-of-chango-admin-node-and-chango-component-nodes","title":"On the Nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>","text":"<p>As yum client nodes to local yum repository, you need to configure the followings on the nodes of <code>Chango Admin Node</code> and <code>Chango Component Nodes</code>  to look up remote local yum repository server.</p> <p>Move current yum repo configurations.</p> <pre><code>sudo mkdir -p ~/old-repos\nsudo mv /etc/yum.repos.d/*.repo ~/old-repos/\n</code></pre> <p>Create configuration of local yum repository.</p> <pre><code>sudo vi /etc/yum.repos.d/local-repos.repo\n</code></pre> <p>Add the following configuration. <pre><code>[local-base]\nname=CentOS Base\nbaseurl=http://[local-yum-repo-ip]/base/\ngpgcheck=0\nenabled=1\n\n[local-epel]\nname=CentOS Epel\nbaseurl=http://[local-yum-repo-ip]/epel/\ngpgcheck=0\nenabled=1\n\n[local-extras]\nname=CentOS Extras\nbaseurl=http://[local-yum-repo-ip]/extras/\ngpgcheck=0\nenabled=1\n\n[local-updates]\nname=CentOS Updates\nbaseurl=http://[local-yum-repo-ip]/updates/\ngpgcheck=0\nenabled=1\n</code></pre></p> <p>Replace <code>[local-yum-repo-ip]</code> with the ip address of local yum repository.</p> <p>Refresh yum repository.</p> <pre><code>sudo yum repolist all\n</code></pre> <p>Test yum install, for example.</p> <pre><code>sudo yum install nginx\n</code></pre>"},{"location":"install/install-admin/#download-chango-private-distribution","title":"Download Chango Private Distribution","text":"<p>If you want to install Chango Private in public, then, before downloading Chango Private distribution,  you need to be logged in as <code>sudo</code> user created before on <code>Chango Admin Node</code>, for example.</p> <pre><code>sudo su - chango;\n</code></pre> <p>Download Chango Private distribution. <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-libs/releases/download/chango-private-deps/chango-private-2.3.0.tar.gz\n</code></pre></p> <p>And tar the file and move to the installation directory of Chango Private.</p> <pre><code>tar zxvf chango-private-2.3.0.tar.gz \ncd chango-private-2.3.0/ansible/\n</code></pre> <p>Download all Chango Components.</p> <pre><code>./download-component-files.sh\n</code></pre> <p>If it takes long time to download chango components, then run the following to download in background.</p> <pre><code># download chango components in background.\nnohup ./download-component-files.sh &gt; out.log &amp;;\n\n# tail output log.\ntail -f out.log;\n</code></pre> <p>NOTE: For installing Chango Private in disconnected environment, after downloading chango component files,  you need to package the whole distribution directory with downloaded component files to the file(for example, tar.gz) which needs to be transferred to your node in which internet is not available.</p>"},{"location":"install/install-admin/#install-chango-admin_1","title":"Install Chango Admin","text":"<p>Now, you are logged in as <code>sudo</code> user on <code>Chango Admin Node</code>.</p> <p>NOTE: Make sure that current user is <code>sudo</code> user created before to access <code>Chango Component Nodes</code> and the self host of <code>Chango Admin Node</code> with password-less SSH connection.</p> <p>Run the following to install Chango Admin.</p> <p><pre><code>./start-chango-private.sh;\n</code></pre> , which will </p> <ul> <li>install Postgresql database.</li> <li>mount attached disks as logical volume on PostgreSQL database host.</li> <li>install Chango Admin on the current host.</li> </ul> <p>Enter values for the prompts, for example.</p> <pre><code>Installing ansible in virtual environment...\nAnsible installed...\nReady to install Chango Admin...\nEnter FQDN of the target host name on which PostgreSQL database will be installed: \nchango-comp-3.chango.private\n\nEnter comma-separated disk paths for LVM mount on the target host of PostgreSQL database (for example, '/dev/sdb,/dev/sdc'): \n/dev/sdb,/dev/sdc\n\nEnter current host name on which Chango Admin will be installed: \nchango-admin.chango.private\n</code></pre> <ul> <li><code>chango-comp-3.chango.private</code> is for PostgreSQL database host which MUST BE one of <code>Chango Component Nodes</code>, NOT <code>Chango Admin Node</code>.</li> <li><code>/dev/sdb,/dev/sdc</code> is for LVM raw disks in comma separated list on PostgreSQL database host.</li> <li><code>chango-admin.chango.private</code> is for current host name of <code>Chango Admin Node</code>.</li> </ul> <p>To enter the raw disks, run <code>lsblk</code> for example.</p> <pre><code>lsblk;\n\nNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\n\n...\n\nsdb      8:16   0   512G  0 disk \nsdc      8:32   0   512G  0 disk \n</code></pre> <p>After installing Chango Admin successfully, you need to get randomly genterated password of <code>admin</code>.</p> <pre><code>cat /export/chango-private-admin-logs/admin.log | grep \"randomly generated password\";\n</code></pre> <p>Output looks like this.</p> <pre><code>2023-10-27 03:53:59,863 INFO com.cloudcheflabs.changoprivate.admin.config.AdminUserConfigurer:32 [main] randomly generated password for user 'admin': b3b3c51fde594d7abdd38fbc3eb2a581\n</code></pre> <p>For example, <code>b3b3c51fde594d7abdd38fbc3eb2a581</code> is the password of <code>admin</code>.</p> <p>The URL of Chango Admin UI is, for example. <pre><code>http://chango-admin.chango.private:8123/\n</code></pre></p> <p>NOTE: <code>8123</code> for Admin UI and <code>8080</code> for NGINX proxy to Admin on <code>Chango Admin Node</code> need to be open.</p> <p>Make sure that the host DNS entries for <code>Chango Admin Node</code> and <code>Chango Component Nodes</code> are added to the host file(for example, <code>/etc/hosts</code>) on your local PC,  for example. <pre><code>[ip-address]  chango-admin.chango.private\n\n[ip-address]  chango-comp-1.chango.private\n[ip-address]  chango-comp-2.chango.private\n[ip-address]  chango-comp-3.chango.private\n[ip-address]  chango-comp-4.chango.private\n</code></pre> , where <code>[ip-address]</code> can be public IP address for online environment or private IP address for offline environment.</p> <p>Login as <code>admin</code> with the randomly generated password. You can change the password of <code>admin</code> in Chango Admin UI <code>Account</code> -&gt; <code>Profile</code> later.</p>"},{"location":"install/install-admin/#after-installing-chango-admin","title":"After installing Chango Admin","text":"<p>After installing Chango Admin, several shell files will be created which can be used for your need later.</p> <ul> <li><code>reinstall-admin.sh</code>: Reinstall LVM mount, PostgreSQL database, and Chango Admin with the same configurations you entered for prompts.</li> <li><code>restart-admin.sh</code>: Restart Chango Admin.</li> <li><code>uninstall-admin.sh</code>: Uninstall Chango Admin.</li> <li><code>uninstall-lvm.sh</code>: Unmount LVM on the host of PostgreSQL database.</li> <li><code>uninstall-postgresql.sh</code>: Uninstall PostgreSQL database.</li> </ul> <p>NOTE: If you have installed <code>Chango Components</code> by <code>Chango Admin</code> later,  DO NOT use <code>reinstall-admin.sh</code>, <code>uninstall-lvm.sh</code>, and <code>uninstall-postgresql.sh</code>  which will destroy all the installation of Chango Private!</p> <p>When the installation of Chango Admin failed with some reasons, you need to run the above shell. If you entered wrong values for the prompts which affects installation failure and want to install Chango Admin again, then run the following sequence.</p> <ul> <li><code>uninstall-admin.sh</code></li> <li><code>uninstall-postgresql.sh</code></li> <li><code>uninstall-lvm.sh</code></li> </ul> <p>, and then, install Chango Admin again with running <code>start-chango-private.sh</code>.</p> <p>If you changed the host and port of NGINX for <code>Chango Admin</code> after adding TLS configuration to NGINX,  then, add the variables to <code>--extra-vars</code> in <code>restart-admin.sh</code>, for example.</p> <pre><code>cpadmin_nginx_scheme=https \\\ncpadmin_nginx_host=[your-nginx-host] \\\ncpadmin_nginx_port=[your-nginx-port] \\\n</code></pre> <p>The revised <code>restart-admin.sh</code> looks like this, for example. <pre><code>python3 -m venv venv;\nsource /home/chango/venv/bin/activate;\n\nansible-playbook -i admin.inv run.yml \\\n--extra-vars \"\\\nexec_user=chango \\\ntarget_hosts=changoprivate-admin-hosts \\\nrole_name=changoprivate-admin \\\ncpadmin_ansible_path=/data/chango/cp-dist/chango-private-2.3.0/ansible \\\nrun_option=restart \\\ncpadmin_nginx_scheme=https \\\ncpadmin_nginx_host=[your-nginx-host] \\\ncpadmin_nginx_port=[your-nginx-port] \\\n\"\n</code></pre></p> <p>Run the revised <code>restart-admin.sh</code>.</p>"},{"location":"install/install-component/","title":"Install Chango Components","text":"<p>There are several components supported by Chango Private.</p> <p>See What is Chango Private? for more details.</p> <p>NOTE: If Chango Private initialization failed or you want to reset Chango Private, then you need to move to  <code>http://[admin-host]:8123/cp-reset.html</code> to reset Chango Private.</p>"},{"location":"install/install-component/#initialize-chango-private","title":"Initialize Chango Private","text":"<p>If you have not initialized Chango Private, you will get the following picture to initialze Chango Private.</p> <p></p> <p>There are mandatory Chango Components like <code>MySQL</code>, <code>Object Storage</code>, <code>Chango Authorizer</code>, and  <code>Chango REST Catalog</code> which must be installed when Chango Private is initialized.  The other optional compoents can be installed after Chango Private initialization.</p>"},{"location":"install/install-component/#configure-hosts-and-ssh-private-key","title":"Configure Hosts and SSH Private Key","text":"<p>All the <code>Chango Component Nodes</code> need to be registered, and SSH private key on <code>Chango Admin Node</code> need to be added to access <code>Chango Component Nodes</code> from  the host of <code>Chango Admin Node</code>.</p> <p>NOTE: Take a note that you need to register all the nodes of <code>Chango Component Nodes</code> except <code>Chango Admin Node</code>.</p> <p>Get SSH private key on <code>Chango Admin Node</code> with the following command, and paste it to the text box <code>SSH Private Key</code>.</p> <pre><code>cat ~/.ssh/id_rsa\n</code></pre> <p></p>"},{"location":"install/install-component/#configure-lvm","title":"Configure LVM","text":"<p>Raw disks attached to the <code>Chango Component Nodes</code> in comma separated list need to be added to mount as logical volume.</p> <p>If the disks attached are same on all the nodes, use this.</p> <p></p> <p>If the disks attached are different on every node, use this.</p> <p></p>"},{"location":"install/install-component/#configure-mysql","title":"Configure MySQL","text":"<p><code>MySQL</code> is used by open source components like <code>Apache Superset</code> and <code>Apache Ozone</code> in Chango Private. Select the host on which <code>MySQL</code> will be installed.</p> <p></p>"},{"location":"install/install-component/#configure-object-storage","title":"Configure Object Storage","text":"<p>Select the options for object storage. <code>Apache Ozone</code> is default object storage provided by Chango Private,  which will be used in disconnected environment in most cases. In public, you may select the external object storage like AWS S3, MinIO, OCI Object Storage.</p> <p>If Apache Ozone is used as object storage, enter the values this below. </p> <p>If external object storage is used, enter values of S3 credentials. </p>"},{"location":"install/install-component/#configure-chango-authorizer","title":"Configure Chango Authorizer","text":"<p><code>Chango Authorizer</code> is used to authenticate and authorize all the data access to Chango.</p> <p></p>"},{"location":"install/install-component/#configure-chango-rest-catalog","title":"Configure Chango REST Catalog","text":"<p><code>Chango REST Catalog</code> is used as data catalog in Chango.</p> <p></p> <p>For now, the components you configured are mandatory.  After configuring mandatory components, you can skip configuration.</p> <p></p> <p>You can install other optional components later after finishing Chango Private initialization.</p>"},{"location":"install/install-component/#install-configured-components","title":"Install Configured Components","text":"<p>Install all the configured components.</p> <p></p> <p>When the installation is finished, press the button of <code>Installation Finished</code>.</p> <p></p> <p>Then, you will move to the main page.</p> <p></p>"},{"location":"install/install-component/#show-log","title":"Show Log","text":"<p>You can see current log produced by installed components.</p> <p></p> <p>Click the host name of components in <code>Status</code> to show log.</p> <p></p>"},{"location":"install/install-component/#chango-authorizer","title":"Chango Authorizer","text":""},{"location":"install/install-component/#scale-server","title":"Scale Server","text":"<p>Chango Authorizer can be scaled out or unscaled.</p> <p></p> <p>First, select hosts for scaling out or unscaling Authorizer servers, and then, press the button of <code>Scale Out Server</code> to scale out Authorizer servers or press the button of <code>Unscale Server</code> to unscale Authorizer servers.</p>"},{"location":"install/install-component/#chango-rest-catalog","title":"Chango REST Catalog","text":""},{"location":"install/install-component/#scale-server_1","title":"Scale Server","text":"<p>Chango REST Catalog can be scaled out or unscaled.</p> <p></p> <p>First, select hosts for scaling out or unscaling REST Catalog servers, and then, press the button of <code>Scale Out Server</code> to scale out REST Catalog servers or press the button of <code>Unscale Server</code> to unscale REST Catalog servers.</p>"},{"location":"install/install-component/#apache-kafka","title":"Apache Kafka","text":"<p><code>Apache Kafka</code> is used as event streaming platform in Chango Private. Multiple Kafka clusters are supported by Chango, that is, you can install kafka clusters as many as you want.</p>"},{"location":"install/install-component/#install-kafka","title":"Install Kafka","text":"<p>If you want to install <code>Apache Kafka</code>, press <code>Go to Install</code> button.</p> <p>NOTE: If you have not installed any kafka cluster, then, enter <code>default</code> for the cluster name.</p> <p></p> <p>After installing kafka, you will see kafka page like this.</p> <p></p> <p>Because Chango Private supports multiple kafka clusters, you can install another kafka cluster.</p> <p>NOTE: Because you have already installed default kafka cluster, you can enter anything for the cluster name.</p> <p></p> <p>After installing another kafka cluster, new created kafka cluster will be shown in tab list.</p> <p></p>"},{"location":"install/install-component/#scale-broker","title":"Scale Broker","text":"<p>Kafka Broker can be scaled out or unscaled.</p> <p></p> <p>First, select hosts for scaling out or unscaling kafka brokers, and then, press the button of <code>Scale Out Broker</code> to scale out brokers  or press the button of <code>Unscale Broker</code> to unscale brokers.</p>"},{"location":"install/install-component/#configure-kafka","title":"Configure Kafka","text":"<p>You can update kafka configurations like heap memory and <code>server.properties</code>.</p> <p>First, select kafka cluster which you want to configure.</p> <p></p> <p>After modifying configuration, press <code>Update</code> to update the selected kafka cluster.</p> <p></p>"},{"location":"install/install-component/#apache-spark","title":"Apache Spark","text":"<p><code>Apache Spark</code> is used as computing engine to run batch and streaming jobs in Chango.</p>"},{"location":"install/install-component/#install-spark","title":"Install Spark","text":"<p>Select hosts to install master, worker and history server of Spark. </p> <p>After installing Spark, you will see the spark page like this. </p>"},{"location":"install/install-component/#scale-worker","title":"Scale Worker","text":"<p>You can scale out and unscale spark workers.</p> <p></p>"},{"location":"install/install-component/#ui","title":"UI","text":"<p>There are URL links to get Spark Master UI and Spark History Server UI in <code>UI</code> of Spark Page.</p> <p>Spark Master UI looks as below.</p> <p></p> <p>Spark History Server UI looks like this.</p> <p></p>"},{"location":"install/install-component/#trino","title":"Trino","text":"<p><code>Trino</code> is used as query engine to run interactive and long running ETL query in Chango.  Chango Private provides multiple trino clusters, so, you can install trino clusers as many as you want.</p>"},{"location":"install/install-component/#install-trino","title":"Install Trino","text":"<p>Enter <code>default</code> for the cluser name if default trino cluster is not installed. </p> <p>After installing default trino cluster, trino page looks like this. </p> <p>If you want to install another trino cluster, enter any name for cluster name. </p> <p>After installing another trino cluster, new created trino cluster will be shown in the cluster tab list. </p>"},{"location":"install/install-component/#scale-worker_1","title":"Scale Worker","text":"<p>You can scale out and unscale trino workers.</p> <p></p>"},{"location":"install/install-component/#configure-trino","title":"Configure Trino","text":"<p>You can update trino memory configurations and catalogs.</p> <p>To update memory properties in trino, select trino cluster. </p> <p>Update catalogs in trino. </p> <p>You can also add catalogs. </p>"},{"location":"install/install-component/#ui_1","title":"UI","text":"<p>You can get Trino UI clicking link in <code>UI</code> of the selected trino cluster.</p> <p></p>"},{"location":"install/install-component/#chango-trino-gateway","title":"Chango Trino Gateway","text":"<p><code>Chango Trino Gateway</code> is used to route trino queries to the backend trino clusters in Chango.</p> <p>In addition, <code>Chango Trino Gateway</code> also provides the following functions. - authenticate trino users and authorize the queries run by trino users. - activate and deactivate the backend trino clusters.</p>"},{"location":"install/install-component/#install-chango-trino-gateway","title":"Install Chango Trino Gateway","text":"<p>Select hosts for <code>Chango Trino Gateway</code> servers, host for NGINX proxy, and host for Redis cache. </p> <p>It looks as below after installing it. </p>"},{"location":"install/install-component/#scale-server_2","title":"Scale Server","text":"<p>Chango Trino Gateway can be scaled out or unscaled.</p> <p></p> <p>First, select hosts for scaling out or unscaling Trino Gateway servers, and then, press the button of <code>Scale Out Server</code> to scale out Trino Gateway servers or press the button of <code>Unscale Server</code> to unscale Trino Gateway servers.</p>"},{"location":"install/install-component/#apache-superset","title":"Apache Superset","text":"<p><code>Apache Superset</code> is used as BI tool in Chango.</p>"},{"location":"install/install-component/#install-superset","title":"Install Superset","text":"<p>Select host for superset server. </p>"},{"location":"install/install-component/#ui_2","title":"UI","text":"<p>You can get Superset UI clicking link in <code>UI</code> of superset page. </p>"},{"location":"install/install-component/#azkaban","title":"Azkaban","text":"<p><code>Azkaban</code> is used as workflow to integrate all the batch jobs like spark ETL jobs and trino ETL jobs in Chango.</p>"},{"location":"install/install-component/#install-azkaban","title":"Install Azkaban","text":"<p>Select host for web and hosts for executors. </p>"},{"location":"install/install-component/#ui_3","title":"UI","text":"<p>You can get Azkaban UI clicking link in <code>UI</code> of azkaban page. </p>"},{"location":"install/install-component/#azkaban-cli","title":"Azkaban CLI","text":"<p><code>Azkaban CLI</code> is CLI to create and update azkaban project on Azkaban.</p>"},{"location":"install/install-component/#install-azkaban-cli","title":"Install Azkaban CLI","text":"<p>Select hosts for azkaban CLI. </p>"},{"location":"install/install-component/#dbt","title":"dbt","text":"<p><code>dbt</code> is CLI tool to transform data in Chango. In most cases, it will be used to run trino queries.</p>"},{"location":"install/install-component/#install-dbt","title":"Install dbt","text":"<p>Select hosts for dbt. </p>"},{"location":"install/install-component/#chango-data-api","title":"Chango Data API","text":"<p>Chango provides data ingestion especially for streaming events. <code>Chango Data API</code> is used to collect streaming events and produce them to kafka.</p>"},{"location":"install/install-component/#install-chango-data-api","title":"Install Chango Data API","text":"<p>Select hosts for Chango Data API servers and host for NGINX proxy. </p>"},{"location":"install/install-component/#scale-server_3","title":"Scale Server","text":"<p>You can scale out and unscale <code>Chango Data API</code> servers. </p>"},{"location":"install/install-component/#chango-streaming","title":"Chango Streaming","text":"<p><code>Chango Streaming</code> is a spark streaming job and used to consume streaming events from kafka and save them to Iceberg table in Chango.</p>"},{"location":"install/install-component/#install-chango-streaming","title":"Install Chango Streaming","text":"<p>Enter spark configurations for <code>Chango Streaming</code> job. </p> <p>After installation, you will see the driver host of <code>Chango Streaming</code> spark job. </p> <p>In spark master UI, <code>Chango Streaming</code> job will be found. </p>"},{"location":"install/install-component/#install-chango-streaming-tx","title":"Install Chango Streaming Tx","text":"<p>This is transactional spark streaming jobs. Enter spark configurations for <code>Chango Streaming Tx</code> job.</p> <p>You need to enter iceberg <code>schema</code> and <code>table</code> to which streaming messages will be saved.</p> <p></p> <p>After installation, you will see chango streaming tx applications in running status in spark ui.</p>"},{"location":"install/install-component/#spark-thrift-server","title":"Spark Thrift Server","text":"<p>Spark Thrift Server is a spark streaming job to run spark sql queries. Clients can connect to spark thrift server,  for example through JDBC/Thrift.</p>"},{"location":"install/install-component/#install-spark-thrift-server","title":"Install Spark Thrift Server","text":"<p>Enter spark configurations for <code>Spark Thrift Server</code> job. </p> <p>After installation, you will see spark thrift server in running status in spark ui.</p>"},{"location":"install/install-component/#spark-sql-runner","title":"Spark SQL Runner","text":"<p>Spark SQL Runner is a spark streaming job which exposes REST API to run spark sql queries requested by clients through REST.</p>"},{"location":"install/install-component/#install-spark-sql-runner","title":"Install Spark SQL Runner","text":"<p>Enter spark configurations for <code>Spark SQL Runner</code> job. </p> <p>After installation, you will see spark sql runner in running status in spark ui.</p>"},{"location":"install/install-component/#chango-query-exec","title":"Chango Query Exec","text":"<p><code>Chango Query Exec</code> is a REST application to execute trino ETL queries to transform data in Chango.  It may be used as alternative to dbt to transform data in Chango.</p>"},{"location":"install/install-component/#install-chango-query-exec","title":"Install Chango Query Exec","text":"<p>Select hosts for Chango Query Exec servers and host for NGINX proxy. </p>"},{"location":"install/install-component/#scale-server_4","title":"Scale Server","text":"<p>You can scale out and unscale <code>Chango Query Exec</code> servers. </p>"},{"location":"install/run-first-ctas/","title":"Run First Trino Query","text":"<p>This is an example to run queries to Trino through <code>Chango Trino Gateway</code>.</p>"},{"location":"install/run-first-ctas/#create-cluster-group","title":"Create Cluster Group","text":"<p>First, you need to create <code>Cluster Group</code> in <code>Trino Gateway</code>. Go to <code>Settings</code> -&gt; <code>Trino Gateway</code>. Click <code>Create Cluster Group</code> in <code>Cluster Groups</code> section.</p> <p></p> <p>And, enter cluster group name <code>bi</code>. </p>"},{"location":"install/run-first-ctas/#create-trino-user-and-register-trino-cluster","title":"Create Trino User and Register Trino Cluster","text":"<p>In order to create trino user and register trino cluster, first select cluster group created before. </p> <p>Click <code>Create Trino User</code> in <code>Trino Users</code> section, and enter user name with password.</p> <p></p> <p>To register trino cluster, click <code>Register Trino Cluster</code> in <code>Trino Clusters</code> section.  Select trino cluster which trino queries will be routed to.</p> <p></p>"},{"location":"install/run-first-ctas/#create-privileges","title":"Create Privileges","text":"<p>To add privileges to the cluster group, go to <code>Settings</code> -&gt; <code>Security</code>. And select role in <code>Roles</code> section.</p> <p></p> <p>Add privilege of <code>*</code> for READ type. </p> <p>Add privilege of <code>*</code> for WRITE type. </p> <p><code>*</code> means all data access to Chango is allowed.</p> <p>But, note that all data access above is not secure, so you need to set the values in fine-grained manner in your production environment, for example.</p> <pre><code>information_schema.*       READ\ntpch.*                     READ\niceberg.*                  READ\niceberg.*                  WRITE\n</code></pre> <p>NOTE: <code>information_schema.*</code> needs to be added for trino connector of superset.</p>"},{"location":"install/run-first-ctas/#get-endpoint-of-chango-trino-gateway","title":"Get Endpoint of Chango Trino Gateway","text":"<p>You need endpoint of <code>Chango Trino Gateway</code> to which clients will connect to run queries.</p> <p>Go to <code>Components</code> -&gt; <code>Chango Trino Gateway</code>. Get the endpoint in <code>Endpoint</code> section. </p>"},{"location":"install/run-first-ctas/#run-first-ctas-query-with-superset","title":"Run First CTAS Query with Superset","text":"<p>To run queries to trino, Superset will be used.</p>"},{"location":"install/run-first-ctas/#login-to-superset","title":"Login to Superset","text":"<p>To move to <code>Superset</code> UI, go to <code>Components</code> -&gt; <code>Apache Superset</code>. Click UI URL to move to Superset UI.</p> <p>First, login as <code>admin</code> with default password <code>SupersetPass1#</code>.</p>"},{"location":"install/run-first-ctas/#add-trino-database","title":"Add Trino Database","text":"<p>You need to add trino database in superset.</p> <p>You need to enter trino url in <code>SQLAlchemy URI *</code> with the following convention. <pre><code>trino://[user]:[password]@[trino-gateway-endpoint-without-scheme]\n</code></pre> For example, you can enter like this. <pre><code>trino://trino:trino123@chango-private-3.chango.private:443\n</code></pre> Replace <code>chango-private-3.chango.private:443</code> with your trino gateway endpoint without <code>https://</code>.</p> <p></p> <p>And you need to add the following in <code>Extra</code> to disable TLS validation. <pre><code>{\n    \"metadata_params\": {},\n    \"engine_params\": {\n          \"connect_args\":{\n              \"http_scheme\": \"https\",\n              \"verify\": false\n        }\n     },\n    \"metadata_cache_timeout\": {},\n    \"schemas_allowed_for_csv_upload\": []\n}\n</code></pre> </p> <p>Check the options of <code>Allow CREATE TABLE AS</code>, <code>Allow CREATE VIEW AS</code>, <code>Allow DML</code>.</p> <p>Finally, press <code>Save</code>.</p>"},{"location":"install/run-first-ctas/#run-ctas-query","title":"Run CTAS Query","text":"<p>Run CTAS query which selects rows from <code>tpch.sf1000.lineitem</code> table and insert them to new created iceberg table <code>iceberg.iceberg_db.test_ctas</code>. <pre><code>-- create schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- ctas.\nCREATE TABLE IF NOT EXISTS iceberg.iceberg_db.test_ctas \nAS\nSELECT\n    *\nFROM tpch.sf1000.lineitem limit 1000\n;\n</code></pre> </p> <p>And select rows from created iceberg table. <pre><code>-- select.\nselect * from iceberg.iceberg_db.test_ctas limit 1000;\n</code></pre> </p> <p>Congratulations!</p>"},{"location":"intro/intro/","title":"What is Chango Private?","text":"<p>Chango Private is a Data Lakehouse Platform for both online and disconnected environment.</p>"},{"location":"intro/intro/#chango-private-data-lakehouse-platform","title":"Chango Private Data Lakehouse Platform","text":"<p>In <code>Ingestion</code> layer:</p> <ul> <li><code>Spark</code> and <code>Trino</code> with <code>dbt</code> or <code>Chango Query Exec</code> will be used as data integration tool.</li> <li><code>Kafka</code> is used as event streaming platform to handle streaming events.</li> <li><code>Chango Ingestion</code> will be used to insert incoming streaming events to Chango directly.</li> </ul> <p>In <code>Storage</code> layer:</p> <ul> <li>Chango Private supports Apache Ozone as object storage by default and external S3 compatible object storage like AWS S3, MinIO, OCI Object Storage.</li> <li>Data lakehouse format is <code>Iceberg</code> table format in Chango.</li> </ul> <p>In <code>Transformation</code> layer:</p> <ul> <li><code>Spark</code> and <code>Trino</code> with <code>dbt</code> or <code>Chango Query Exec</code> will be used to run ETL jobs.</li> </ul> <p>In <code>Analytics</code> layer:</p> <ul> <li><code>Trino</code> is used as query engine to explore all the data in Chango.</li> <li><code>BI</code> tools like <code>Apache Superset</code> will connect to <code>Trino</code> to run queries through <code>Chango Trino Gateway</code>.</li> </ul> <p>In <code>Management</code> layer:</p> <ul> <li><code>Azkaban</code> is used as workflow. All the batch jobs like ETL will be integrated with <code>Azkaban</code>.</li> <li><code>Chango REST Catalog</code> is Iceberg REST Catalog and used as data catalog in Chango.</li> <li>Chango Private supports storage security to control data access based on RBAC in Chango. <code>Chango Authorizer</code> will be used for it.</li> <li><code>Chango Trino Gateway</code> is an implementation of Trino Gateway concept. <code>Chango Trino Gateway</code> provides several features like authentication, authorization, smart query routing(routing to less exhausted trino clusters), trino cluster activation/deactivation. For more details, see Chango Trino Gateway.</li> <li><code>Chango Spark SQL Runner</code> exposes REST API to which clients send spark sql queries using REST to execute spark queries.</li> <li><code>Chango Thrift Server</code> exposes JDBC/Thrift to which clients send spark sql queries using JDBC/Thrift to execute spark queries.</li> </ul>"},{"location":"user-guide/azkaban-change-pw/","title":"Change Login Password in Azkaban","text":"<p>The default login user and password of <code>Azkaban</code> is <code>azkaban</code> and <code>azkaban</code>. The default password should be changed.</p>"},{"location":"user-guide/azkaban-change-pw/#change-login-password","title":"Change Login Password","text":"<p>Enter Azkaban web server host via SSH, and move to the Azakban web server directory.</p> <pre><code>sudo su - azkaban;\ncd /usr/lib/azkaban/web;\n</code></pre> <p>There is a user management file <code>conf/azkaban-users.xml</code>.</p> <pre><code>&lt;azkaban-users&gt;\n  &lt;user groups=\"azkaban\" password=\"azkaban\" roles=\"admin\" username=\"azkaban\"/&gt;\n  &lt;user password=\"metrics\" roles=\"metrics\" username=\"metrics\"/&gt;\n\n  &lt;role name=\"admin\" permissions=\"ADMIN\"/&gt;\n  &lt;role name=\"metrics\" permissions=\"METRICS\"/&gt;\n&lt;/azkaban-users&gt;\n</code></pre> <p>The value of <code>password</code> in the following line needs to be modified.</p> <pre><code>&lt;user groups=\"azkaban\" password=\"azkaban\" roles=\"admin\" username=\"azkaban\"/&gt;\n</code></pre> <p>After modifying <code>conf/azkaban-users.xml</code>, restart <code>Azkaban</code> in Chango Admin UI.</p>"},{"location":"user-guide/azkaban-cli/","title":"Create Azkaban Project using CLI","text":"<p>Azkaban Project file is a zip file in which workflow files are contained.  Azkaban project file can be uploaded either via Azkaban Web UI or via Azkaban CLI to create workflow in Azkaban.</p>"},{"location":"user-guide/azkaban-cli/#create-azkaban-project-file","title":"Create Azkaban Project File","text":"<p>First, create flow file, for example, <code>spark-pi-client.flow</code>.</p> <pre><code>---\nconfig:\n  failure.emails: admin@your-domain.com\n\nnodes:\n  - name: Start\n    type: noop\n\n  - name: SparkPi\n    type: command\n    config:\n      command: ssh spark@chango-private-3.chango.private \"/home/spark/spark-pi-client.flow\"\n    dependsOn:\n      - Start\n\n  - name: End\n    type: noop\n    dependsOn:\n      - SparkPi\n</code></pre> <p>And create project file <code>flow20.project</code>.</p> <pre><code>azkaban-flow-version: 2.0\n</code></pre> <p>Finally, package as zip file. <pre><code>zip spark-pi-client.zip spark-pi-client.flow flow20.project \n</code></pre></p>"},{"location":"user-guide/azkaban-cli/#upload-azkaban-project-file-with-azkaban-cli","title":"Upload Azkaban Project File with Azkaban CLI","text":"<p>First, access to the host where <code>Azkaban CLI</code> is installed.</p> <p>Login as azkaban cli user with activating python virtual environment.</p> <pre><code>sudo su - azkabancli;\nsource venv/bin/activate;\nazkaban --help;\n</code></pre> <p>Create azkaban project with the name of <code>spark-pi-client</code> with CLI.</p> <p><pre><code>azkaban upload -c -p spark-pi-client -u azkaban@http://[azkaban-web-host]:28081 ./spark-pi-client.zip\n</code></pre> Default password for the user <code>azkaban</code> is <code>azkaban</code>.</p> <p>To update azkaban project, parameter <code>-c</code> needs to be removed from the above command.</p>"},{"location":"user-guide/chango-cdc/","title":"Change Data Capture Using Chango CDC","text":"<p>Chango CDC is Change Data Capture application to catch CDC data of database  and send CDC data to Chango.</p>"},{"location":"user-guide/chango-cdc/#build-and-package-chango-cdc","title":"Build and Package Chango CDC","text":"<p>Embedded Debezium is used by <code>Chango CDC</code>. So you need to choose which Debezium version will be used for your database, see the release notice of Debezium. After that, choose the concrete maven dependency version of Debezium connector. For example, maven dependency version of Debezium connector for PostgreSQL can be found here.</p> <p>Package <code>Chango CDC</code> distribution with Debezium maven dependency version. Please note that Java 11 and Maven 3 are required to build <code>Chango CDC</code>. <pre><code>git clone -b branch-1.0.1 https://github.com/cloudcheflabs/chango-cdc.git\ncd chango-cdc;\n\nexport CHANGO_CDC_VERSION=1.0.1\nexport DEBEZIUM_VERSION=1.9.7.Final\n\n./package-dist.sh \\\n--version=${CHANGO_CDC_VERSION} \\\n--debezium.version=${DEBEZIUM_VERSION} \\\n;\n</code></pre></p>"},{"location":"user-guide/chango-cdc/#install-chango-cdc","title":"Install Chango CDC","text":"<p>You may use <code>Chango CDC</code> package which was built for yourself previously.  For this example, the pre-built <code>Chango CDC</code> will be used. Download Chango CDC.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-cdc/releases/download/1.0.1/chango-cdc-1.0.1-debezium-1.9.7.Final-linux-x64.tar.gz\n</code></pre> <p>Untar and move to Chango CDC directory.</p> <pre><code>tar zxvf chango-cdc-1.0.1-debezium-1.9.7.Final-linux-x64.tar.gz\ncd chango-cdc-1.0.1-debezium-1.9.7.Final-linux-x64/\n</code></pre>"},{"location":"user-guide/chango-cdc/#configure-chango-cdc","title":"Configure Chango CDC","text":"<p>Modify <code>conf/configuation.yml</code>. For example, it looks like this.</p> <pre><code>chango:\n  token: eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJjZGM0MTFkYzIzMzg2NjU0ZGZkYTdjYjk4OTMzNjA1NWNiNyIsImV4cCI6MTcwNjY1OTE5OSwiaWF0IjoxNzAxMzU2NDEyfQ.-WjO6mpNV5QM5t1jwLmBD8tBuRNOxrzcREU6RqLJtHGD0u_TGi28NWG9lFYA-ZKQ-nDwGbr6Nf_MXaUeeO2VAw\n  dataApiUrl: http://chango-private-1.chango.private:80\n  schema: cdc_db\n  table: student\n  batchSize: 10000\n  interval: 1000\n  tx: true\n\ndebezium:\n  connector: |-\n    name=postgres-connector\n    connector.class=io.debezium.connector.postgresql.PostgresConnector\n    offset.storage=org.apache.kafka.connect.storage.FileOffsetBackingStore\n    offset.storage.file.filename=/tmp/chango-cdc/offset-student.dat\n    offset.flush.interval.ms=60000\n    topic.prefix=cdc\n    schema.history.internal=io.debezium.storage.file.history.FileSchemaHistory\n    schema.history.internal.file.filename=/tmp/chango-cdc/schemahistory-student.dat\n    database.server.name=postgresql-server\n    database.server.id=pg-1\n    database.hostname=localhost\n    database.port=5432\n    database.user=anyuser\n    database.password=anypassword\n    database.dbname=studentdb\n    table.include.list=public.student\n</code></pre> <ul> <li><code>chango.token</code>: Chango Credential which is necessary to access <code>Chango Data API</code>. See Get Chango Credential.</li> <li><code>chango.dataApiUrl</code>: <code>Chango Data API</code> endpoint URL.</li> <li><code>chango.schema</code>: Schema of Iceberg catalog in Chango.</li> <li><code>chango.table</code>: Iceberg Table in Chango.</li> </ul> <p>In this example, we will use PostgreSQL from which CDC data will be caught and sent to Chango.</p> <ul> <li><code>debezium.connector</code>: Properties of Debezium connector for PostgreSQL.</li> </ul> <p>If you want to use another database, consult Debezium Connectors.</p> <p>NOTE: If <code>chango.tx</code> is set to <code>true</code>, then you need to install transactional chango streaming beforehand.</p>"},{"location":"user-guide/chango-cdc/#install-postgresql","title":"Install PostgreSQL","text":"<p>PostgreSQL will be installed as docker container.</p> <p>Assumed that docker and docker compose are installed on your machine, create docker compose file.</p> <pre><code>cat &lt;&lt;EOF &gt; docker-compose.yml\nversion: \"3.5\"\n\nservices:\n  postgres:\n    container_name: postgres\n    image: debezium/postgres:9.6\n    ports:\n      - 5432:5432\n    environment:\n      - POSTGRES_DB=studentdb\n      - POSTGRES_USER=anyuser\n      - POSTGRES_PASSWORD=anypassword\nEOF\n</code></pre> <p>Run docker compose.</p> <pre><code>docker-compose up -d;\n</code></pre> <p>And then, enter the docker container of PostgreSQL to create a table.</p> <pre><code>docker exec -it postgres sh;\n</code></pre> <p>Connect PostgreSQL database in it.</p> <pre><code>psql -U anyuser -d studentdb;\n</code></pre> <p>Create table <code>student</code>.</p> <pre><code>CREATE TABLE public.student\n(\n    id integer NOT NULL,\n    address character varying(255),\n    email character varying(255),\n    name character varying(255),\n    CONSTRAINT student_pkey PRIMARY KEY (id)\n);\n</code></pre>"},{"location":"user-guide/chango-cdc/#create-iceberg-table","title":"Create Iceberg Table","text":"<p>You need to create Iceberg table in Chango using trino clients like <code>Superset</code>.</p> <pre><code>-- create iceberg schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.cdc_db;\n\n-- create iceberg table.\nCREATE TABLE iceberg.cdc_db.student (\n    address varchar,\n    day varchar,\n    email varchar,\n    id bigint,\n    month varchar,\n    name varchar,\n    op varchar,\n    ts bigint,\n    year varchar \n)\nWITH (\n    partitioning=ARRAY['year', 'month', 'day'],\n    format = 'PARQUET'\n);\n</code></pre> <p>In addition to the original fields of PostgreSQL table, fields <code>year</code>, <code>month</code>, <code>day</code>, <code>ts</code> and <code>op</code> are required for partitioning and small files compaction. If fields <code>year</code>, <code>month</code>, <code>day</code>, <code>ts</code> and <code>op</code> exist in the original PostgreSQL table, then <code>_</code> will be appended to the original fields of PostgreSQL table.</p> <p>Take a note that the type of field <code>id</code> in original PostgreSQL table is <code>integer</code>, but the type of field <code>id</code> in Iceberg table in Chango is <code>bigint</code>.</p> <p>NOTE: The sequence  of table column names in lower case must be alphanumeric in ascending order.</p>"},{"location":"user-guide/chango-cdc/#run-chango-cdc","title":"Run Chango CDC","text":"<p>Move to Chango CDC directory, and run Chango CDC.</p> <pre><code>bin/start-chango-cdc.sh\n</code></pre> <p>You can check the log file <code>/tmp/chango-cdc/chango-cdc.log</code>.</p> <p>If you want to stop Chango CDC, run the following.</p> <pre><code>bin/stop-chango-cdc.sh\n</code></pre>"},{"location":"user-guide/chango-cdc/#run-postgresql-cud-queries","title":"Run PostgreSQL CUD Queries","text":"<p>In PostgreSQL docker container, run the CUD queries.</p> <pre><code>INSERT INTO STUDENT(ID, NAME, ADDRESS, EMAIL) VALUES('1','Kidong Lee','Seoul','kidong@example.com');\n\nUPDATE STUDENT SET EMAIL='kidong2@example.com', NAME='Kidong2 Lee' WHERE ID = 1; \n\nDELETE FROM STUDENT WHERE ID = 1;\n</code></pre> <p>Let's check if CDC data has been saved in Iceberg table in Chango with running the following query in <code>Superset</code>.</p> <pre><code>select *, from_unixtime(ts/1000) from iceberg.cdc_db.student order by ts desc;\n</code></pre> <p></p>"},{"location":"user-guide/chango-log/","title":"Reading Log Files Using Chango Log","text":"<p>Chango Log is a log agent to read local log files and send logs  to Chango to analyze logs.</p>"},{"location":"user-guide/chango-log/#create-iceberg-table","title":"Create Iceberg Table","text":"<p>Before sending logs to Chango, you need to create Iceberg table for logs with trino clients like <code>Superset</code>.</p> <pre><code>-- create schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.logs_db;\n\n-- create iceberg table.\nCREATE TABLE iceberg.logs_db.logs (\n    day varchar,\n    fileName varchar,\n    filePath varchar,\n    hostAddress varchar,\n    hostName varchar,\n    lineNumber bigint,\n    message varchar,\n    month varchar,\n    readableTs varchar,\n    ts bigint,\n    year varchar \n)\nWITH (\n    partitioning=ARRAY['year', 'month', 'day'],\n    format = 'PARQUET'\n);\n</code></pre>"},{"location":"user-guide/chango-log/#install-chango-log","title":"Install Chango Log","text":"<p>Download <code>Chango Log</code> distribution package.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-log/releases/download/1.0.1/chango-log-1.0.1-linux-x64.tar.gz;\n</code></pre> <p>Untar Chango Log file and move to Chango Log directory.</p> <pre><code>tar zxvf chango-log-1.0.1-linux-x64.tar.gz;\ncd chango-log-1.0.1-linux-x64;\n</code></pre>"},{"location":"user-guide/chango-log/#configure-chango-log","title":"Configure Chango Log","text":"<p>Modify <code>conf/configuration.yml</code>.</p> <pre><code>chango:\n  token: any-chango-credential\n  dataApiUrl: http://any-data-api-endpoint\n  schema: logs_db\n  table: logs\n  batchSize: 10000\n  interval: 1000\n  tx: false\n\nlogs:\n  - path: /log-dir-1\n    file: rest*.log\n  - path: /log-dir-2\n    file: admin*.log\n  - path: /log-dir-3\n\ntask:\n  log:\n    interval: 20000\n    threads: 3\n\nrocksdb:\n  directory: /tmp/rocksdb\n</code></pre> <p>You need to configure the following properties to send logs to Chango.</p> <ul> <li><code>chango.token</code>: Chango Credential which is necessary to access <code>Chango Data API</code>. See Get Chango Credential.</li> <li><code>chango.dataApiUrl</code>: <code>Chango Data API</code> endpoint URL.</li> </ul> <p>You can configure multiple log directories in which all the log files will be read.</p> <ul> <li><code>logs[*].path</code>: Log directory path.</li> <li><code>logs[*].file</code>: Log file name pattern. All the log files matched with this pattern will be read in the log directory. If this property is omitted, all the log files will be read without pattern matching.</li> </ul>"},{"location":"user-guide/chango-log/#run-chango-log","title":"Run Chango Log","text":"<p>Start <code>Chango Log</code> to read local log files and send logs to Chango.</p> <pre><code>bin/start-chango-log.sh\n</code></pre> <p>You can see the logs of <code>Chango Log</code> in the path <code>/tmp/chango-log/chango-log.log</code>.</p> <p>To stop <code>Chango Log</code>, run the following.</p> <pre><code>bin/stop-chango-log.sh\n</code></pre>"},{"location":"user-guide/chango-log/#run-queries-to-analyze-logs","title":"Run Queries to Analyze Logs","text":"<p>Run queries to analyze logs saved in Iceberg table <code>logs</code>.</p> <p>For example.</p> <pre><code>select * from iceberg.logs_db.logs where fileName = 'admin.log' order by lineNumber asc limit 100000;\n</code></pre> <p>If the query is run with <code>Superset</code>, then it looks like this.</p> <p></p>"},{"location":"user-guide/connect-trino-gw/","title":"Connect to Chango Trino Gateway with Trino Clients","text":"<p>Using Trino Clients like Trino CLI and Apache Superset, you can connect to <code>Chango Trino Gateway</code>.</p> <p>Before proceeding to this example, you need to see Run First Trino Query  how to create <code>Cluster Group</code> in <code>Chango Trino Gateway</code> with storage security.</p>"},{"location":"user-guide/connect-trino-gw/#conect-to-chango-trino-gateway-with-apache-superset","title":"Conect to Chango Trino Gateway with Apache Superset","text":"<p>You need to follow the instruction shown in Run First Trino Query to  connect to <code>Chango Trino Gateway</code> using Apache Superset.</p>"},{"location":"user-guide/connect-trino-gw/#connect-to-chango-trino-gateway-with-trino-cli","title":"Connect to Chango Trino Gateway with Trino CLI","text":""},{"location":"user-guide/connect-trino-gw/#install-trino-cli","title":"Install Trino CLI","text":"<p>Run the following to install Trino CLI.</p> <pre><code>mkdir -p ~/trino-cli;\ncd ~/trino-cli;\n\nexport TRINO_VERSION=435\ncurl -L -O https://repo1.maven.org/maven2/io/trino/trino-cli/${TRINO_VERSION}/trino-cli-${TRINO_VERSION}-executable.jar;\nmv trino-cli-${TRINO_VERSION}-executable.jar trino\nchmod +x trino\n\n./trino --version;\n</code></pre>"},{"location":"user-guide/connect-trino-gw/#connect-to-chango-trino-gateway","title":"Connect to Chango Trino Gateway","text":"<p>Let's say you have created trino user <code>trino</code> with proper password in the previous section, and the endpoint of  <code>Chango Trino Gateway</code> is <code>https://chango-private-1.chango.private:443</code>.  Run the following to connect to <code>Chango Trino Gateway</code>, and enter the password for the user <code>trino</code>.</p> <pre><code>./trino --server https://chango-private-1.chango.private \\\n--user trino \\\n--insecure \\\n--password;\n</code></pre> <p>If there is no problem occurred to connect, then you will get the following with running <code>show catalogs</code>.</p> <pre><code>trino&gt; show catalogs;\n Catalog \n---------\n iceberg \n jmx     \n system  \n tpcds   \n tpch    \n(5 rows)\n\nQuery 20231108_052020_00744_pdmmv, FINISHED, 1 node\nSplits: 1 total, 1 done (100.00%)\n0.04 [0 rows, 0B] [0 rows/s, 0B/s]\n</code></pre>"},{"location":"user-guide/cred/","title":"Get Chango Credential","text":"<p>To get Chango Credential, you need to create new role with proper privileges for credentials in most cases.</p> <p>Go to <code>Settings</code> -&gt; <code>Security</code>.</p>"},{"location":"user-guide/cred/#create-role","title":"Create Role","text":"<p>Click <code>Create Role</code> in <code>Roles</code> section. Enter role name.</p> <p></p>"},{"location":"user-guide/cred/#create-credential","title":"Create Credential","text":"<p>First, select role you created, and click <code>Create Credential</code> in <code>Credentials</code> section.</p> <p></p>"},{"location":"user-guide/cred/#create-privileges","title":"Create Privileges","text":"<p>To add data access control, you need to create privileges with storage path.</p> <p>Create privileges for READ and WRITE type clicking <code>Create Privileges</code> in <code>Privileges</code> section.</p> <p>For READ privileges. </p> <p>For WRITE privileges. </p>"},{"location":"user-guide/cred/#list-credential","title":"List Credential","text":"<p>You will get credential created before from the list of credentials of role in <code>Credentials</code> section.</p> <p></p>"},{"location":"user-guide/dbt/","title":"Run dbt Model","text":"<p>This is an example to run <code>dbt</code> model in Chango Private.</p>"},{"location":"user-guide/dbt/#configure-trino-connection","title":"Configure Trino Connection","text":"<p>You need to configure trino connection in <code>profiles.yml</code> in dbt. The following is a template to connect to <code>Chango Trino Gateway</code> from dbt. <pre><code>trino:\n  target: dev\n  outputs:\n    dev:\n      type: trino\n      method: ldap\n      user: trino\n      password: xxx\n      host: chango-private-5.chango.private\n      port: 443\n      database: iceberg\n      schema: silver\n      threads: 8\n      http_scheme: https\n      session_properties:\n        query_max_run_time: 5d\n        exchange_compression: True\n</code></pre> The target catalog is <code>iceberg</code> and schema is <code>silver</code>. <code>host</code> is the host name of <code>Chango Trino Gateway</code> endpoint which  can be found in <code>Endpoint</code> section in <code>Components</code> -&gt; <code>Chango Trino Gateway</code>.</p> <p><code>user</code> and <code>password</code> can be created in <code>Settings</code> -&gt; <code>Trino Gateway</code>. For more details, see here.</p>"},{"location":"user-guide/dbt/#run-dbt-model-using-git-sync","title":"Run dbt Model using <code>git-sync</code>","text":"<p><code>dbt</code> is installed as docker container in Chango Private. To see on which hosts <code>dbt</code> is installed, go to <code>Components</code> -&gt; <code>dbt</code>. First, you need to access <code>dbt</code> host to run dbt model. </p> <p>Assumed that your <code>dbt</code> model files are source controlled by git repo, you can run <code>dbt</code> model with this command.</p> <pre><code>docker exec -it dbt \\\n/bin/sh -c '\n/git-sync \\\n-repo [git-repo-url] \\\n-branch [branch] \\\n-username [user] \\\n-password [password] \\\n-root [root-dir] \\\n-one-time &amp;&amp; \\\ncd [root-dir]/[git-repo-name].git/[dbt-model-dir] &amp;&amp; \\\ndbt run \\\n--profiles-dir ../ \\\n--project-dir ./ \\\n-m models/[model-sql-file]\n'\n</code></pre> <p><code>git-sync</code> already installed in <code>dbt</code> docker image will be used to pull <code>dbt</code> model files from git repo with the following command. <pre><code>/git-sync \\\n-repo [git-repo-url] \\\n-branch [branch] \\\n-username [user] \\\n-password [password] \\\n-root [root-dir] \\\n-one-time &amp;&amp; \\\n</code></pre></p> <p>And run dbt model like this. <pre><code>dbt run \\\n--profiles-dir ../ \\\n--project-dir ./ \\\n-m models/[model-sql-file]\n</code></pre></p>"},{"location":"user-guide/kafka-topic/","title":"Create Kafka Topic","text":"<p>To create kafka topic in Chango Private, first access to the host of kafka nodes.</p> <p>Login as kafka user and move to kafka installation directory.</p> <pre><code># login as kafka.\nsudo su - kafka;\n\n# move to kafka installation directory.\ncd /usr/lib/kafka;\n</code></pre> <p>Run the command to create topic.</p> <pre><code>export JAVA_HOME=/usr/lib/jdk\n\nbin/kafka-topics \\\n--create \\\n--bootstrap-server kafka-host-1:9092,kafka-host-2:9092,kafka-host-3:9092 \\\n--topic [topic] \\\n--replication-factor 3 \\\n--partitions 10 \\\n--config cleanup.policy=compact\n</code></pre> <p><code>[topic]</code> is topic name to create.</p>"},{"location":"user-guide/livy/","title":"Run Spark Applications using Livy","text":"<p>Livy exposes REST API to run spark applications remotely. </p>"},{"location":"user-guide/livy/#run-spark-batch-application","title":"Run Spark Batch Application","text":"<p>You need to know Livy endpoint to which spark job configuration should be sent in the menu of <code>Spark</code> beforehand.</p> <p>Let's run spark example PI application using Livy.</p> <pre><code>cat &lt;&lt;EOF &gt; /home/opc/run-livy-batch.json\n{\n   \"file\":\"file:///usr/lib/spark/examples/jars/spark-examples_2.12-3.4.3.jar\",\n   \"className\":\"org.apache.spark.examples.SparkPi\",\n   \"args\": [\"100\"],\n   \"driverMemory\":\"1G\",\n   \"driverCores\": 1,\n   \"executorMemory\":\"1G\",\n   \"executorCores\":1,\n   \"numExecutors\":1,\n   \"name\":\"PI Example New Named 3\",\n   \"conf\":{\n      \"spark.master\":\"spark://chango-private-1.chango.private:7777\"\n   }\n}\nEOF\n\n# convert pretty json to oneline json.\ncat /home/opc/run-livy-batch.json | jq -c &gt; /home/opc/run-livy-batch-oneline.json\n\n# call livy api.\ncurl -H \"Content-Type: application/json\" \\\n-XPOST \\\n\"http://chango-private-1.chango.private:8998/batches\" \\\n-d @/home/opc/run-livy-batch-oneline.json \\\n;\n</code></pre> <p>The livy endpoint above needs to be replaced with your current livy endpoint shown in <code>Spark</code> UI page.</p>"},{"location":"user-guide/livy/#get-batch-state","title":"Get Batch State","text":"<p>You can get batch state of spark applications run by Livy.</p> <pre><code>curl -H \"Content-Type: application/json\" \\\n-XGET \\\n\"http://chango-private-1.chango.private:8998/batches/0/state\" \\\n;\n</code></pre>"},{"location":"user-guide/livy/#get-batch-log","title":"Get Batch Log","text":"<p>If you want to see the log of spark application, call the following.</p> <pre><code>curl -H \"Content-Type: application/json\" \\\n-XGET \\\n\"http://chango-private-1.chango.private:8998/batches/0/log\" \\\n;\n</code></pre>"},{"location":"user-guide/livy/#delete-batch","title":"Delete Batch","text":"<p>In order to delete spark application job, run the following rest call.</p> <pre><code>curl -H \"Content-Type: application/json\" \\\n-XDELETE \\\n\"http://chango-private-1.chango.private:8998/batches/0\" \\\n;\n</code></pre>"},{"location":"user-guide/maintain-iceberg/","title":"Maintain Iceberg Tables","text":"<p>Everytime committing data to iceberg tables, many files will be created like metadata,  snapshots which need to be maintained, for example.</p> <ul> <li>remove old metadata files.</li> <li>expire snapshots.</li> <li>compact small files.</li> </ul> <p>Chango provides ways to maintain iceberg tables with ease.</p>"},{"location":"user-guide/maintain-iceberg/#iceberg-maintenance-overview","title":"Iceberg Maintenance Overview","text":"<p><code>Chango Spark SQL Runner</code> and <code>Chango Spark Thrift Server</code> are spark streaming jobs running on spark cluster  and will execute spark sql queries sent by clients through REST or JDBC/Thrift. In particular, iceberg maintenance spark sql queries need to be sent to either <code>Chango Spark SQL Runner</code> or <code>Chango Spark Thrift Server</code>  to maintain iceberg tables in Chango.</p> <p><code>Chango Spark SQL Runner</code> exposes REST API to which clients can send spark sql queries through REST,  <code>Chango Spark Thrift Server</code> exposes JDBC/Thrift interface to which clients can send spark sql queries through JDBC/Thrift.</p> <p>All the incoming queries in <code>Chango Spark SQL Runner</code> and <code>Chango Spark Thrift Server</code> will be checked by <code>Chango Authorizer</code>  if the queries is allowed to be run. This access control provided by Chango is RBAC with fain-grained access control like  catalog, schema and table level to Iceberg.</p> <p>Take a note that <code>Chango Spark SQL Runner</code> and <code>Chango Thrift Server</code> will connect to <code>Chango REST Catalog</code> as data catalog.</p>"},{"location":"user-guide/maintain-iceberg/#create-chango-credential","title":"Create Chango Credential","text":"<p>Before sending spark sql queries, you need to create Chango credential. First, create a role like <code>spark-sql</code> and credential and privileges.</p> <p></p> <p>Privileges need to be added like this.</p> <pre><code>iceberg.*               READ\niceberg.*               WRITE\n</code></pre>"},{"location":"user-guide/maintain-iceberg/#send-spark-sql-queries-to-chango-spark-sql-runner","title":"Send Spark SQL Queries to Chango Spark SQL Runner","text":"<p>Clients like <code>curl</code> can send iceberg maintenance spark queries to <code>Chango Spark SQL Runner</code> through REST. For example, the spark sql queries below which are small files compaction, snapshot expiration and old metadata files removal in iceberg tables  will be sent.</p> <pre><code>### request.\n# access token for authorizer.\nexport ACCESS_TOKEN=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJzcGFyay1zcWxkN2JkMDllNmM3Y2Q0ZDQ5OTI2NDRhYTViYzc4ZTExNCIsImV4cCI6MTcyMTg2NTU5OSwiaWF0IjoxNzE0MDAyMzY1fQ.y-JvgmeKkUcE6V538B9O2989fE-8IauYLNAQC4eOmziW7B_taeN9OdHGXZzTwrWYEd4vC3lo2R0EAjSh9IDp_w\n\n# spark queries.\ncat &lt;&lt;EOF &gt; queries.sql\n-- compact small files.\nCALL iceberg.system.rewrite_data_files(table =&gt; 'iceberg.iceberg_db.test_ctas',strategy =&gt; 'binpack', options =&gt; map('min-input-files','2'));\n\n-- expire snapshot.\nCALL iceberg.system.expire_snapshots('iceberg.iceberg_db.test_ctas');\n\n-- remove old metadata files.\nALTER TABLE iceberg.iceberg_db.test_ctas SET TBLPROPERTIES ('write.metadata.delete-after-commit.enabled'='true','write.metadata.previous-versions-max'='100');\nEOF\n\n# request.\ncurl -XPOST \\\n-H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n-H \"Content-Type: application/x-www-form-urlencoded\" \\\nhttp://&lt;spark-sql-runner-host&gt;:29080/v1/spark-sql/execute \\\n-d \"query=$(cat ./queries.sql)\" \\\n;\n</code></pre> <p><code>&lt;spark-sql-runner-host&gt;</code> needs to be replaced with the current driver host of Chango Spqrk SQL Runner.</p>"},{"location":"user-guide/maintain-iceberg/#send-spark-sql-queries-to-chango-spark-thrift-server","title":"Send Spark SQL Queries to Chango Spark Thrift Server","text":"<p>Chango Spark Thrift Server exposes JDBC/Thrift interface to run spark sql queries.  For example, superset can connect using Thrift, and java applications can connect using JDBC.</p>"},{"location":"user-guide/maintain-iceberg/#connect-chango-spark-thrift-server-using-superset","title":"Connect Chango Spark Thrift Server using Superset","text":"<p>First, create Hive Connector in Superset.</p> <p></p> <p>Add the following URL as <code>SQLAlchemy URI</code> in superset ui. <pre><code>hive://&lt;sts-host&gt;:10000/iceberg\n</code></pre></p> <p><code>&lt;sts-host&gt;</code> needs to be replaced with the current driver host of spark thrift server.</p> <p>Check <code>Allow CREATE TABLE AS</code>, <code>Allow CREATE VIEW AS</code> and <code>Allow DML</code>.</p> <p>And you need to enter the following json value to <code>Extra</code>.</p> <pre><code>{\n   \"metadata_params\":{\n   },\n   \"engine_params\":{\n      \"connect_args\":{\n         \"username\":\"&lt;chango-credential&gt;\",\n         \"auth\": \"NONE\"\n      }\n   },\n   \"metadata_cache_timeout\":{  \n   },\n   \"schemas_allowed_for_csv_upload\":[\n   ]\n}\n</code></pre> <p><code>&lt;chango-credential&gt;</code> needs to be set with the chango credential created previously.</p> <p>And, run iceberg maintenance spark sql queries as follows.</p> <pre><code>-- compact small files.\nCALL iceberg.system.rewrite_data_files(table =&gt; 'iceberg.iceberg_db.test_ctas',strategy =&gt; 'binpack', options =&gt; map('min-input-files','2'));\n\n-- expire snapshot.\nCALL iceberg.system.expire_snapshots('iceberg.iceberg_db.test_ctas');\n\n-- remove old metadata files.\nALTER TABLE iceberg.iceberg_db.test_ctas SET TBLPROPERTIES ('write.metadata.delete-after-commit.enabled'='true','write.metadata.previous-versions-max'='100');\n</code></pre> <p></p>"},{"location":"user-guide/maintain-iceberg/#connect-chango-spark-thrift-server-using-beeline","title":"Connect Chango Spark Thrift Server using Beeline","text":"<p>If spark with spark thrift server support is installed, beeline can be used to connect Chango Spark Thrift Server.</p> <pre><code>export ACCESS_TOKEN=&lt;chango-credential&gt;\n\n$SPARK_HOME/bin/beeline -u jdbc:hive2://&lt;sts-host&gt;:10000/iceberg -n $ACCESS_TOKEN\n</code></pre> <p><code>&lt;chango-credential&gt;</code> and <code>&lt;sts-host&gt;</code> need to be replaced.</p>"},{"location":"user-guide/maintain-iceberg/#connect-chango-spark-thrift-server-using-jdbc","title":"Connect Chango Spark Thrift Server using JDBC","text":"<p>Java applications can send iceberg maintenance spark sql queries to Chango Spark Thrift Server using JDBC.</p> <p>First, add hive jdbc dependency.</p> <p><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;\n    &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt;\n    &lt;version&gt;2.3.9&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> The following java codes shows to send iceberg snapshot expiration query.</p> <pre><code>        String url = \"jdbc:hive2://&lt;sts-host&gt;:10000/iceberg\";\nString token = \"&lt;chango-credential&gt;\";\nString query = \"CALL iceberg.system.expire_snapshots('iceberg.iceberg_db.test_ctas')\";\n// properties\nProperties properties = new Properties();\nproperties.setProperty(\"user\", token);\nConnection con = DriverManager.getConnection(url, properties);\nStatement stmt = con.createStatement();\nboolean ret = stmt.execute(query);\n</code></pre> <p><code>&lt;chango-credential&gt;</code> and <code>&lt;sts-host&gt;</code> need to be replaced.</p>"},{"location":"user-guide/query-exec/","title":"Data Tranformation Using Chango Query Exec","text":"<p><code>Chango Query Exec</code> is a REST application to execute trino ETL queries requested through REST call to transform  data in Chango. </p> <p><code>dbt</code> is a popular tool to transform data with running trino ETL queries. But <code>Chango Query Exec</code> is a more simpler way  to transform data in Chango. You just send trino queries for ETL jobs to <code>Chango Query Exec</code> explicitly through REST using, for example <code>curl</code>. Such ETL jobs using <code>Chango Query Exec</code> can be integrated with Workflow like <code>Azkaban</code> with ease.</p>"},{"location":"user-guide/query-exec/#date-functions","title":"Date Functions","text":"<p>There are date functions supported by <code>Chango Query Exec</code>.</p>"},{"location":"user-guide/query-exec/#nowinmillis","title":"nowInMillis()","text":"<p>Current Time in Millis.</p> <pre><code>CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.ctas\nAS\nSELECT\n*\nFROM anycatalog.anyschema.anytable\nWHERE ts &lt; #{ nowInMillis() }\n;\n</code></pre>"},{"location":"user-guide/query-exec/#nowformattedstring-format","title":"nowFormatted(String format)","text":"<p>Current date with date format.</p> <pre><code>CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.ctas\nAS\nSELECT\n*\nFROM anycatalog.anyschema.anytable\nWHERE dateAt &lt; '#{ nowFormatted(\"YYYY-MM-dd\") }'\n;\n</code></pre>"},{"location":"user-guide/query-exec/#nowplusinmillisint-years-int-months-int-days-int-hours-int-minutes-int-weeks","title":"nowPlusInMillis(int years, int months, int days, int hours, int minutes, int weeks)","text":"<p>Add time amount to current date.</p> <pre><code>CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.ctas\nAS\nSELECT\n*\nFROM anycatalog.anyschema.anytable\nWHERE where ts &lt; #{ nowPlusInMillis(0, 0, 1, 0, 0, 0) }\n;\n</code></pre>"},{"location":"user-guide/query-exec/#nowplusformattedint-years-int-months-int-days-int-hours-int-minutes-int-weeks-string-format","title":"nowPlusFormatted(int years, int months, int days, int hours, int minutes, int weeks, String format)","text":"<p>Formatted plus date time.</p> <pre><code>CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.ctas\nAS\nSELECT\n*\nFROM anycatalog.anyschema.anytable\nWHERE where dateAt &gt;= '#{ nowPlusFormatted(0, 0, 0, 0, 0, 0, \"YYYY-MM-dd\") }' and dateAt &lt; '#{ nowPlusFormatted(0, 0, 1, 0, 0, 0, \"YYYY-MM-dd\") }'\n;\n</code></pre>"},{"location":"user-guide/query-exec/#nowminusinmillisint-years-int-months-int-days-int-hours-int-minutes-int-weeks","title":"nowMinusInMillis(int years, int months, int days, int hours, int minutes, int weeks)","text":"<p>Substract time amount from current time.</p> <pre><code>CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.ctas\nAS\nSELECT\n*\nFROM anycatalog.anyschema.anytable\nWHERE where ts &gt;= #{ nowMinusInMillis(0, 0, 1, 0, 0, 0) }\n;\n</code></pre>"},{"location":"user-guide/query-exec/#nowminusformattedint-years-int-months-int-days-int-hours-int-minutes-int-weeks-string-format","title":"nowMinusFormatted(int years, int months, int days, int hours, int minutes, int weeks, String format)","text":"<p>Formatted minus date time.</p> <pre><code>CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.ctas\nAS\nSELECT\n*\nFROM anycatalog.anyschema.anytable\nWHERE where dateAt &gt;= '#{ nowMinusFormatted(0, 0, 1, 0, 0, 0, \"YYYY-MM-dd\") }' and dateAt &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 0, 0, \"YYYY-MM-dd\") }'\n;\n</code></pre>"},{"location":"user-guide/query-exec/#send-simple-etl-query","title":"Send Simple ETL Query","text":"<p>This is a simple ETL query file called <code>exec-queries.sql</code>.</p> <pre><code>-- create schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- ctas.\nCREATE TABLE IF NOT EXISTS iceberg.iceberg_db.metrics\nAS\nSELECT\n    *\nFROM postgresql.public.metrics \nwhere format_datetime(ts, 'YYYY-MM-dd HH:mm:ss.SSS') &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 10, 0, \"YYYY-MM-dd HH:mm:ss.SSS\") }'\nlimit 10000\n;\n</code></pre> <p>It will create iceberg schema and CTAS table. </p> <p>Take a look at the function of <code>nowMinusFormatted(0, 0, 0, 0, 10, 0, \"YYYY-MM-dd HH:mm:ss.SSS\")</code>  which will be replaced with the date before 10 minutes of current time by <code>Chango Query Exec</code>.</p> <p>Send this simple trino query to <code>Chango Query Exec</code> using <code>curl</code>.</p> <pre><code>export ACCESS_TOKEN=&lt;access-token&gt;\n\ncurl -XPOST -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\nhttp://&lt;chango-query-exec-endpoint&gt;/v1/trino/exec-query \\\n-d \"uri=&lt;trino-gateway-endpoint&gt;\" \\\n-d \"user=&lt;trino-user&gt;\" \\\n-d \"password=&lt;trino-password&gt;\" \\\n-d \"query=$(base64 -w 0 ./exec-queries.sql)\" \\\n;\n</code></pre> <ul> <li><code>&lt;access-token&gt;</code>: Chango Credential. See Get Chango Credential.</li> <li><code>&lt;trino-gateway-endpoint&gt;</code>: Chango Trino Gateway Endpoint without https scheme.</li> <li><code>&lt;trino-user&gt;</code>: Trino User.</li> <li><code>&lt;trino-password&gt;</code>: Trino Password.</li> <li><code>&lt;chango-query-exec-endpoint&gt;</code>: Chango Query Exec Endpoint without http scheme.</li> </ul>"},{"location":"user-guide/query-exec/#send-query-flow","title":"Send Query Flow","text":"<p>You may send DAG like query flow to <code>Chango Query Exec</code>.</p> <p>Let's create query flow file <code>exec-flow.yaml</code>.</p> <pre><code>uri: &lt;trino-gateway-endpoint&gt;\nuser: &lt;trino-user&gt;\npassword: &lt;trino-password&gt;\nqueries:\n  - id: query-0\n    description: |-\n      Query 0 description\n    depends: NONE\n    query: |-\n      -- drop table.\n      DROP TABLE iceberg.iceberg_db.metrics\n  - id: query-1\n    description: |-\n      Query 1 description\n    depends: query-0\n    query: |-\n      -- create schema.\n      CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n  - id: query-2\n    description: |-\n      Query 2 description\n    depends: query-0\n    query: |-\n      -- create schema.\n      CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n  - id: query-3\n    description: |-\n      Query 3 description\n    depends: query-1,query-2\n    query: |-\n      -- ctas.\n      CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.metrics\n      AS\n      SELECT\n      *\n      FROM postgresql.public.metrics\n      where format_datetime(ts, 'YYYY-MM-dd HH:mm:ss.SSS') &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 10, 0, \"YYYY-MM-dd HH:mm:ss.SSS\") }'\n      limit 10000\n      ;\n  - id: query-4\n    description: |-\n      Query 4 description\n    depends: query-3\n    query: |-\n      -- create schema.\n      CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n      -- ctas.\n      CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.metrics\n      AS\n      SELECT\n      *\n      FROM postgresql.public.metrics\n      where format_datetime(ts, 'YYYY-MM-dd HH:mm:ss.SSS') &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 50, 0, \"YYYY-MM-dd HH:mm:ss.SSS\") }'\n      limit 10000\n      ;\n</code></pre> <p>This is a query flow example. You don't have to see the details of the individual queries.</p> <p>You need define unique ids for <code>queries[*].id</code>, and you may add dependencies with <code>queries[*].depends</code>. </p> <p>Send query flow to <code>Chango Query Exec</code>.</p> <pre><code>curl -XPOST -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\nhttp://&lt;chango-query-exec-endpoint&gt;/v1/trino/exec-query-flow \\\n-d \"flow=$(base64 -w 0 ./exec-flow.yaml)\" \\\n;\n</code></pre>"},{"location":"user-guide/rest-in-spark/","title":"Create Iceberg REST Catalog in Spark","text":"<p>This shows how to create Iceberg REST catalog in spark applications in Chango Private.</p> <p>If spark applications need to access Iceberg tables using <code>Chango REST Catalog</code>, Chango credential and S3 credential are necessary.</p> <p>Let's see the following codes to create Iceberg REST Catalog in Spark. <pre><code>        String s3AccessKey = \"...\";\nString s3SecretKey = \"...\";\nString s3Endpoint = \"...\";\nString s3Region = \"...\";\nString restUrl = \"...\";\nString warehouse = \"...\";\nString token = \"...\";\nSparkConf sparkConf = new SparkConf().setAppName(\"Run Spark with Iceberg REST Catalog\");\n// set aws system properties.\nSystem.setProperty(\"aws.region\", (s3Region != null) ? s3Region : \"us-east-1\");\nSystem.setProperty(\"aws.accessKeyId\", s3AccessKey);\nSystem.setProperty(\"aws.secretAccessKey\", s3SecretKey);\n// iceberg rest catalog.\nsparkConf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\");\nsparkConf.set(\"spark.sql.catalog.rest\", \"org.apache.iceberg.spark.SparkCatalog\");\nsparkConf.set(\"spark.sql.catalog.rest.catalog-impl\", \"org.apache.iceberg.rest.RESTCatalog\");\nsparkConf.set(\"spark.sql.catalog.rest.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\");\nsparkConf.set(\"spark.sql.catalog.rest.uri\", restUrl);\nsparkConf.set(\"spark.sql.catalog.rest.warehouse\", warehouse);\nsparkConf.set(\"spark.sql.catalog.rest.token\", token);\nsparkConf.set(\"spark.sql.catalog.rest.s3.endpoint\", s3Endpoint);\nsparkConf.set(\"spark.sql.catalog.rest.s3.path-style-access\", \"true\");\nsparkConf.set(\"spark.sql.defaultCatalog\", \"rest\");\nSparkSession spark = SparkSession\n.builder()\n.config(sparkConf)\n.enableHiveSupport()\n.getOrCreate();\nConfiguration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\nhadoopConfiguration.set(\"fs.s3a.endpoint\", s3Endpoint);\nif(s3Region != null) {\nhadoopConfiguration.set(\"fs.s3a.endpoint.region\", s3Region);\n}\nhadoopConfiguration.set(\"fs.s3a.access.key\", s3AccessKey);\nhadoopConfiguration.set(\"fs.s3a.secret.key\", s3SecretKey);\nhadoopConfiguration.set(\"fs.s3a.path.style.access\", \"true\");\nhadoopConfiguration.set(\"fs.s3a.change.detection.mode\", \"warn\");\nhadoopConfiguration.set(\"fs.s3a.change.detection.version.required\", \"false\");\nhadoopConfiguration.set(\"fs.s3a.multiobjectdelete.enable\", \"true\");\nhadoopConfiguration.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\");\nhadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\");\n</code></pre></p> <ul> <li><code>s3AccessKey</code>: S3 Access Key.</li> <li><code>s3SecretKey</code>: S3 Secret Key.</li> <li><code>s3Endpoint</code>: S3 Endpoint.</li> <li><code>s3Region</code>: S3 Region.</li> <li><code>restUrl</code>: Endpoint of <code>Chango REST Catalog</code>.</li> <li><code>warehouse</code>: Warehouse Path.</li> <li><code>token</code>: Chango Credential. </li> </ul> <p>To get S3 Credential, see Get S3 Credential.  For the value of <code>warehouse</code>, it can be <code>s3a://[bucket]/warehouse-rest</code> where <code>[bucket]</code> is found in your S3 credential list.</p> <p>To get Endpoint URL of <code>Chango REST Catalog</code>, go to <code>Components</code> -&gt; <code>Chango REST Catalog</code>.  The URL will be shown in <code>Endpoint</code> section.</p> <p>To get Chango Credential, see Get Chango Credential.</p>"},{"location":"user-guide/s3-cred/","title":"Get S3 Credential","text":"<p>Chango provides object storage as storage. You can select either <code>Apache Ozone</code> or <code>External S3 compatible Object Storage</code> like AWS S3, MinIO and OCI Object Storage  in Chango.</p> <p>Applications using Spark and Trino need to have S3 Credential to access to Chango Storage.</p> <p>Go to <code>Settings</code> -&gt; <code>S3 Credential</code>.</p>"},{"location":"user-guide/s3-cred/#create-s3-credential","title":"Create S3 Credential","text":"<p>If <code>Apache Ozone</code> is being used as object storage in Chango Private, you can create new S3 Credential as you want.</p> <p>Press <code>Create</code> button, and enter description. </p> <p>Then, the created S3 credential will be shown in the list. </p>"},{"location":"user-guide/schema-reg/","title":"Configure Schema Registry URL","text":"<p>Schema Registry is schema registry server for kafka to handle schemas of kafka messages.  For example, if you want to send Avro messages to kafka, then first, you need to register Avro schema to Schema Registry.</p> <p>To send Avro messages to Kafka using Schema Registry, see here for more details.</p> <p>The following property needs to be configured to Kafka producer and consumer.</p> <pre><code>schema.registry.url=http://[schema-registry-host]:8081\n</code></pre> <p>To find the host of Schema Registry, go to <code>Components</code> -&gt; <code>Apache Kafka</code>.  The host name will be found in <code>Schema Registry Host</code> of <code>Status</code> section.</p>"},{"location":"user-guide/spark-azkaban/","title":"Run Spark Jobs with Azkaban","text":"<p><code>Azkaban</code> is used as Workflow in Chango Private.  All the batch jobs like spark jobs and trino ETL jobs using dbt can be integrated with <code>Azkaban</code> in Chango.</p> <p>This example shows how spark jobs can be integrated with Azkaban.</p>"},{"location":"user-guide/spark-azkaban/#run-spark-jobs-on-spark-nodes","title":"Run Spark Jobs on Spark Nodes","text":"<p>Let's run spark pi example job on spark nodes which can be either spark master or spark workers.</p> <p>First, login as <code>spark</code> user.</p> <pre><code>sudo su - spark;\n</code></pre> <p>Run Spark PI example job.</p> <pre><code>export JAVA_HOME=/usr/lib/jdk\nexport SPARK_VERSION=3.4.0\nexport SPARK_HOME=/usr/lib/spark\nexport SPARK_MASTER=spark://chango-private-1.chango.private:7777\n\nspark-submit \\\n--master ${SPARK_MASTER} \\\n--deploy-mode client \\\n--name spark-pi \\\n--class org.apache.spark.examples.SparkPi \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n--num-executors 1 \\\nfile://${SPARK_HOME}/examples/jars/spark-examples_2.12-${SPARK_VERSION}.jar 100;\n</code></pre> <p>Make sure the env value of <code>SPARK_MASTER</code> is set correctly.</p>"},{"location":"user-guide/spark-azkaban/#run-sparks-jobs-from-azkaban-executor-nodes","title":"Run Sparks Jobs from Azkaban Executor Nodes","text":"<p>In order to integrate spark jobs with Azkaban, spark jobs must be run from Azkaban executor nodes via SSH.</p> <p>Let's say, Azkaban executor nodes are <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code>, and Spark Node is <code>chango-private-3.chango.private</code>.</p>"},{"location":"user-guide/spark-azkaban/#password-less-ssh-access-to-spark-node-from-azkaban-executor-nodes","title":"Password-less SSH Access to Spark Node from Azkaban Executor Nodes","text":"<p>To access Spark Node from Azkaban executor nodes via SSH, we need to configure as follows.</p> <p>Copy ssh public key on all the azkaban executor nodes <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code></p> <pre><code>sudo su - azkaban;\ncat ~/.ssh/id_rsa.pub;\n</code></pre> <p>Paste ssh public key of Azkaban executor nodes to <code>authorized_keys</code> on spark node <code>chango-private-3.chango.private</code>.</p> <pre><code>sudo su - spark;\nmkdir -p ~/.ssh\nvi ~/.ssh/authorized_keys\n</code></pre> <p>Add permission on Spark Node <code>chango-private-3.chango.private</code></p> <pre><code>chmod 600 ~/.ssh/authorized_keys\nchmod 700 ~/.ssh\n</code></pre> <p>Test if password-less ssh access to Spark node <code>chango-private-3.chango.private</code> from Azkaban executor nodes <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code>. Move to Azkaban executor nodes, and run the following. <pre><code>sudo su - azkaban;\nssh spark@chango-private-3.chango.private;\n</code></pre></p>"},{"location":"user-guide/spark-azkaban/#run-spark-pi-example-remotely","title":"Run Spark PI Example Remotely","text":"<p>Create spark job run shell file <code>run-spark-pi-client.sh</code> on Spark node <code>chango-private-3.chango.private</code>.</p> <p>Login as <code>spark</code> user on Spark node. <pre><code>sudo su - spark;\n</code></pre></p> <p>Create file <code>run-spark-pi-client.sh</code>.</p> <pre><code>export JAVA_HOME=/usr/lib/jdk\nexport SPARK_VERSION=3.4.0\nexport SPARK_HOME=/usr/lib/spark\nexport SPARK_MASTER=spark://chango-private-1.chango.private:7777\n\nspark-submit \\\n--master ${SPARK_MASTER} \\\n--deploy-mode client \\\n--name spark-pi \\\n--class org.apache.spark.examples.SparkPi \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n--num-executors 1 \\\nfile://${SPARK_HOME}/examples/jars/spark-examples_2.12-${SPARK_VERSION}.jar 100;\n</code></pre> <p>Add permission. <pre><code>chmod +x run-spark-pi-client.sh;\n</code></pre></p> <p>On Azkaban executor nodes <code>chango-private-1.chango.private</code> and <code>chango-private-2.chango.private</code>, run spark job remotely.</p> <pre><code>sudo su - azkaban;\nssh spark@chango-private-3.chango.private \"/home/spark/run-spark-pi-client.sh\"\n</code></pre> <p>The output looks like this.</p> <pre><code>client mode..\n23/11/08 00:49:17 INFO SparkContext: Running Spark version 3.4.0\n23/11/08 00:49:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/11/08 00:49:17 INFO ResourceUtils: ==============================================================\n23/11/08 00:49:17 INFO ResourceUtils: No custom resources configured for spark.driver.\n23/11/08 00:49:17 INFO ResourceUtils: ==============================================================\n23/11/08 00:49:17 INFO SparkContext: Submitted application: Spark Pi\n23/11/08 00:49:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -&gt; name: cores, amount: 1, script: , vendor: , memory -&gt; name: memory, amount: 1024, script: , vendor: , offHeap -&gt; name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -&gt; name: cpus, amount: 1.0)\n23/11/08 00:49:17 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n23/11/08 00:49:17 INFO ResourceProfileManager: Added ResourceProfile id: 0\n23/11/08 00:49:17 INFO SecurityManager: Changing view acls to: spark\n23/11/08 00:49:17 INFO SecurityManager: Changing modify acls to: spark\n23/11/08 00:49:17 INFO SecurityManager: Changing view acls groups to: \n23/11/08 00:49:17 INFO SecurityManager: Changing modify acls groups to: \n23/11/08 00:49:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY\n23/11/08 00:49:18 INFO Utils: Successfully started service 'sparkDriver' on port 44612.\n23/11/08 00:49:18 INFO SparkEnv: Registering MapOutputTracker\n23/11/08 00:49:18 INFO SparkEnv: Registering BlockManagerMaster\n23/11/08 00:49:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n23/11/08 00:49:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n23/11/08 00:49:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n23/11/08 00:49:18 INFO DiskBlockManager: Created local directory at /export/spark/local/blockmgr-f486810b-b6fc-4973-8c23-491e7c8f43c6\n23/11/08 00:49:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n23/11/08 00:49:18 INFO SparkEnv: Registering OutputCommitCoordinator\n23/11/08 00:49:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n23/11/08 00:49:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n23/11/08 00:49:18 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n23/11/08 00:49:18 INFO SparkContext: Added JAR file:///usr/lib/spark/examples/jars/spark-examples_2.12-3.4.0.jar at spark://chango-private-3.chango.private:44612/jars/spark-examples_2.12-3.4.0.jar with timestamp 1699404557683\n23/11/08 00:49:18 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://chango-private-1.chango.private:7777...\n23/11/08 00:49:18 INFO TransportClientFactory: Successfully created connection to chango-private-1.chango.private/10.0.0.188:7777 after 26 ms (0 ms spent in bootstraps)\n\n...\n\n23/11/08 00:49:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n23/11/08 00:49:23 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.565842 s\nPi is roughly 3.1414131141413115\n23/11/08 00:49:23 INFO SparkContext: SparkContext is stopping with exitCode 0.\n23/11/08 00:49:23 INFO SparkUI: Stopped Spark web UI at http://chango-private-3.chango.private:4041\n23/11/08 00:49:23 INFO StandaloneSchedulerBackend: Shutting down all executors\n23/11/08 00:49:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down\n23/11/08 00:49:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n23/11/08 00:49:23 INFO MemoryStore: MemoryStore cleared\n23/11/08 00:49:23 INFO BlockManager: BlockManager stopped\n23/11/08 00:49:23 INFO BlockManagerMaster: BlockManagerMaster stopped\n23/11/08 00:49:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n23/11/08 00:49:23 INFO SparkContext: Successfully stopped SparkContext\n23/11/08 00:49:23 INFO ShutdownHookManager: Shutdown hook called\n23/11/08 00:49:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-c2184120-a77d-40f1-9dba-6b10c726697a\n23/11/08 00:49:23 INFO ShutdownHookManager: Deleting directory /export/spark/local/spark-ae6bef98-6f06-48ca-8b9d-10c1a2cd2c87\n23/11/08 00:49:23 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...\n23/11/08 00:49:23 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.\n23/11/08 00:49:23 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n</code></pre>"},{"location":"user-guide/spark-azkaban/#integrate-spark-jobs-with-azkaban","title":"Integrate Spark Jobs with Azkaban","text":"<p>There is a CLI tool for Azkaban to create and update azkaban project.  We will use Azkaban CLI to create and update azkaban projects here.</p>"},{"location":"user-guide/spark-azkaban/#experience-sample-projects","title":"Experience Sample Projects","text":"<p>Let's create azkaban sample projects.</p> <p>Login as <code>azkabancli</code> user on Azkaban CLI node.</p> <pre><code>sudo su - azkabancli;\nsource venv/bin/activate;\nazkaban -v;\n</code></pre> <p>Download sample projects.</p> <pre><code>wget https://github.com/azkaban/azkaban/raw/master/az-examples/flow20-projects/basicFlow20Project.zip;\nwget https://github.com/azkaban/azkaban/raw/master/az-examples/flow20-projects/embeddedFlow20Project.zip;\n</code></pre> <p>Create a project on Azkaban Web Server using Azkaban CLI. Enter password <code>azkaban</code>.</p> <pre><code>azkaban upload -c -p basicFlow20Project -u azkaban@http://chango-private-1.chango.private:28081 ./basicFlow20Project.zip;\n</code></pre> <p>After login with the default password <code>azkaban</code> for the user <code>azkaban</code> in Azkaban Web UI, you will see the following picture.</p> <p></p> <p>Execute the flow.</p> <p></p> <p>Let's create another sample project.</p> <pre><code>azkaban upload -c -p embeddedFlow20Project -u azkaban@http://chango-private-1.chango.private:28081 ./embeddedFlow20Project.zip;\n</code></pre> <p></p> <p>Execute the flow.</p> <p></p>"},{"location":"user-guide/spark-azkaban/#create-azkaban-project-for-spark-job","title":"Create Azkaban Project for Spark Job","text":"<p>Create flow file <code>spark-pi-client.flow</code>.</p> <pre><code>---\nconfig:\n  failure.emails: admin@cloudchef-labs.com\n\nnodes:\n  - name: Start\n    type: noop\n\n  - name: SparkPi\n    type: command\n    config:\n      command: ssh spark@chango-private-3.chango.private \"/home/spark/run-spark-pi-client.sh\"\n    dependsOn:\n      - Start\n\n  - name: End\n    type: noop\n    dependsOn:\n      - SparkPi\n</code></pre> <p>Create project file <code>flow20.project</code>.</p> <pre><code>azkaban-flow-version: 2.0\n</code></pre> <p>Package as zip file. <pre><code>zip spark-pi-client.zip spark-pi-client.flow flow20.project \n</code></pre></p> <p>Upload project file.</p> <pre><code>azkaban upload -c -p spark-pi-client -u azkaban@http://chango-private-1.chango.private:28081 ./spark-pi-client.zip\n</code></pre> <p>Now, you will see that the project for spark job has been created.</p> <p></p> <p>Execute spark project flow.</p> <p></p> <p></p> <p>If spark job has been run in <code>client</code> mode, you will see the spark job logs like this.</p> <p></p> <p>You will see the execution history like this.</p> <p></p>"},{"location":"user-guide/standalone-query-exec/","title":"Data Transformation Using Standalone Chango Query Exec","text":"<p><code>Chango Query Exec</code> standalone distribution can be downloaded and installed.  It can be used as standalone <code>Chango Query Exec</code> which is not controlled by <code>Chango Admin</code>.</p>"},{"location":"user-guide/standalone-query-exec/#date-functions","title":"Date Functions","text":"<p>Date Functions supported by <code>Chango Query Exec</code> can be found here.</p>"},{"location":"user-guide/standalone-query-exec/#install-standalone-chango-query-exec","title":"Install Standalone Chango Query Exec","text":"<p>Download <code>Chango Query Exec</code> standalone distribution. <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-libs/releases/download/chango-private-deps/chango-query-exec-2.3.0-linux-x64.tar.gz\n</code></pre></p> <p>Untar and move to query exec directory. <pre><code>tar zxvf chango-query-exec-2.3.0-linux-x64.tar.gz;\ncd chango-query-exec-2.3.0-linux-x64/\n</code></pre></p> <p>Start <code>Chango Query Exec</code>.</p> <pre><code>bin/start-chango-query-exec.sh\n</code></pre> <p>You can see the log file of <code>Chango Query Exec</code>.</p> <pre><code>tail -f /tmp/chango-private-query-exec-logs/query-exec.log\n</code></pre> <p>To stop <code>Chango Query Exec</code>.</p> <pre><code>bin/stop-chango-query-exec.sh\n</code></pre>"},{"location":"user-guide/standalone-query-exec/#send-simple-etl-query","title":"Send Simple ETL Query","text":"<p>Create a file <code>exec-queries.sql</code>.</p> <pre><code>-- create schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- ctas.\nCREATE TABLE IF NOT EXISTS iceberg.iceberg_db.metrics\nAS\nSELECT\n    *\nFROM postgresql.public.metrics \nwhere format_datetime(ts, 'YYYY-MM-dd HH:mm:ss.SSS') &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 10, 0, \"YYYY-MM-dd HH:mm:ss.SSS\") }'\nlimit 10000\n;\n</code></pre> <p>Send queries to <code>Chango Query Exec</code>.</p> <pre><code>curl -XPOST \\\nhttp://localhost:28291/v1/trino/exec-query \\\n-d \"uri=chango-private-1.chango.private:18080\" \\\n-d \"user=trino\" \\\n-d \"ssl=false\" \\\n-d \"query=$(base64 -w 0 ./exec-queries.sql)\" \\\n;\n</code></pre> <p>Parameters for this API.</p> <ul> <li><code>uri</code>: Trino URI.</li> <li><code>user</code>: Trino User.</li> <li><code>password</code>: Trino Password. Optional.</li> <li><code>query</code>: Trino Queries. Base64 encoded.</li> <li><code>ssl</code>: SSL enabled or not. Default is <code>true</code>. Optional.</li> <li><code>ssl_verification</code>: SSL Verification enabled or not. Default is <code>false</code>. Optional.</li> </ul>"},{"location":"user-guide/standalone-query-exec/#send-query-flow","title":"Send Query Flow","text":"<p>Let's create more complex queries like DAG, <code>exec-flow.yaml</code>.</p> <pre><code>uri: chango-private-1.chango.private:18080\nuser: trino\nssl: false\nsslVerification: false\nqueries:\n  - id: query-0\n    description: |-\n      Query 0 description\n    depends: NONE\n    query: |-\n      -- drop table.\n      DROP TABLE iceberg.iceberg_db.metrics\n  - id: query-1\n    description: |-\n      Query 1 description\n    depends: query-0\n    query: |-\n      -- create schema.\n      CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n  - id: query-2\n    description: |-\n      Query 2 description\n    depends: query-0\n    query: |-\n      -- create schema.\n      CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n  - id: query-3\n    description: |-\n      Query 3 description\n    depends: query-1,query-2\n    query: |-\n      -- ctas.\n      CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.metrics\n      AS\n      SELECT\n      *\n      FROM postgresql.public.metrics\n      where format_datetime(ts, 'YYYY-MM-dd HH:mm:ss.SSS') &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 10, 0, \"YYYY-MM-dd HH:mm:ss.SSS\") }'\n      limit 10000\n      ;\n  - id: query-4\n    description: |-\n      Query 4 description\n    depends: query-3\n    query: |-\n      -- create schema.\n      CREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n      -- ctas.\n      CREATE TABLE IF NOT EXISTS iceberg.iceberg_db.metrics\n      AS\n      SELECT\n      *\n      FROM postgresql.public.metrics\n      where format_datetime(ts, 'YYYY-MM-dd HH:mm:ss.SSS') &lt; '#{ nowMinusFormatted(0, 0, 0, 0, 50, 0, \"YYYY-MM-dd HH:mm:ss.SSS\") }'\n      limit 10000\n      ;\n</code></pre> <p>The individual queries used in this example are not useful. You should take a look at how to construct DAG like query flow in this example.</p> <p>You can define the following properties except <code>queries</code> in the query flow.</p> <ul> <li><code>uri</code>: Trino URI.</li> <li><code>user</code>: Trino User.</li> <li><code>password</code>: Trino Password. Optional.</li> <li><code>ssl</code>: SSL enabled or not. Default is <code>true</code>. Optional.</li> <li><code>sslVerification</code>: SSL Verification enabled or not. Default is <code>false</code>. Optional.</li> </ul> <p>Send query flow to <code>Chango Query Exec</code>.</p> <pre><code>curl -XPOST \\\nhttp://localhost:28291/v1/trino/exec-query-flow \\\n-d \"flow=$(base64 -w 0 ./exec-flow.yaml)\" \\\n;\n</code></pre>"},{"location":"user-guide/streaming/","title":"Send Streaming Events","text":"<p><code>Chango Data API</code> server will collect all the incoming streaming events and produce them to kafka.  <code>Chango Streaming</code> will consume events from kafka and save events to Iceberg table in Chango directly. </p> <p>That is, if you send streaming events to <code>Chango Data API</code> server, then all the streaming events will be inserted to Chango automatically. <code>Chango Client</code> is used to send streaming events to <code>Chango Data API</code> server with ease.</p> <p>NOTE: Before sending streaming events, make sure that <code>Chango Data API</code> and <code>Chango Straming</code> are installed.</p>"},{"location":"user-guide/streaming/#add-chango-client-library-to-classpath","title":"Add Chango Client Library to Classpath","text":"<p>You can add the following maven dependency to your project.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;co.cloudcheflabs.chango&lt;/groupId&gt;\n  &lt;artifactId&gt;chango-client&lt;/artifactId&gt;\n  &lt;version&gt;2.0.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>You can also download Chango Client jar file to add it to your application classpath.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-client/releases/download/2.0.2/chango-client-2.0.2-executable.jar;\n</code></pre>"},{"location":"user-guide/streaming/#create-iceberg-table-before-sending-json-events","title":"Create Iceberg Table before Sending JSON Events","text":"<p>Before sending json as streaming events to <code>Chango Data API</code> server, Iceberg table needs to be created beforehand with  trino clients like Trino CLI and Apache Superset.</p> <p>It is good practice that Iceberg table for streaming events needs to be partitioned with date, for example, <code>year</code>, <code>month</code> and <code>day</code>. In addition, timestamp column like <code>ts</code> also needs to be added to Iceberg table.  Actually, with <code>ts</code> column, Chango Private will compact small data, manifest, and position delete files and expire snapshots to improve query performance.</p> <p>Especially, in order to compact small files and add partitioning in Iceberg table in Chango, you need to follow the rules.</p> <p>The name of timestamp column must be <code>ts</code> whose type is <code>bigint</code> which is equivalent to <code>long</code> in java.  Column names <code>year</code>, <code>month</code>, <code>day</code> will be used as partitioning columns.</p> <ul> <li><code>ts</code>:  the number of milliseconds since 1970-01-01 00:00:00.</li> <li><code>year</code>: year with the format of <code>yyyy</code> which is necessary for partitioning.</li> <li><code>month</code>: month of the year with the format of <code>MM</code> which is necessary for partitioning.</li> <li><code>day</code>: day of the month with the format of <code>dd</code> which is necessary for partitioning.</li> </ul> <p>For example, create <code>logs</code> table with partitioning and timestamp.</p> <pre><code>-- create iceberg schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- create iceberg table.\nCREATE TABLE iceberg.iceberg_db.logs (\n    day varchar,\n    level varchar,\n    message varchar,\n    month varchar,\n    ts bigint,\n    year varchar \n)\nWITH (\n    partitioning=ARRAY['year', 'month', 'day'],\n    format = 'PARQUET'\n);\n</code></pre> <p>NOTE: The sequence  of table column names in lower case must be alphanumeric in ascending order.</p> <p>You can create Iceberg table with Superset provided by Chango Private like this.</p> <p></p>"},{"location":"user-guide/streaming/#send-json-events-with-chango-client","title":"Send JSON Events with Chango Client","text":"<p>It is very simple to use Chango Client API in your code. </p> <p>You can construct <code>ChangoClient</code> instance like this. <pre><code>        ChangoClient changoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval\n);\n</code></pre></p> <ul> <li><code>token</code> : Data access credential issued by <code>Chango Authorizer</code>.</li> <li><code>dataApiServer</code> : Endpoint URL of <code>Chango Data API</code>.</li> <li><code>schema</code>: Target Iceberg schema which needs to be created before sending json data to chango.</li> <li><code>table</code>: Target Iceberg table which also needs to be created beforehand.</li> <li><code>batchSize</code> : The size of json list which will be sent to chango in batch mode and in gzip format.</li> <li><code>interval</code> : Json data will be queued internally in chango client. The queued json list will be sent in this period whose unit is milliseconds.</li> </ul> <p>In order to get <code>token</code>, see Get Chango Credential.</p> <p>To get the endpoint of <code>Chango Data API</code>, go to <code>Components</code> -&gt; <code>Chango Data API</code>.</p> <ul> <li>Get URL in <code>Endpoint</code> section.</li> </ul> <p>And send JSON events.</p> <pre><code>        // send json.\nchangoClient.add(json);\n</code></pre> <p>Let's see the full codes to send JSON events.</p> <pre><code>import co.cloudcheflabs.chango.client.component.ChangoClient;\nimport co.cloudcheflabs.chango.client.util.JsonUtils;\nimport org.joda.time.DateTime;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport java.util.HashMap;\nimport java.util.Map;\npublic class SendLogsToDataAPI {\nprivate static Logger LOG = LoggerFactory.getLogger(SendLogsToDataAPI.class);\n@Test\npublic void sendLogs() throws Exception {\nString token = System.getProperty(\"token\");\nString dataApiServer = System.getProperty(\"dataApiServer\");\nString table = System.getProperty(\"table\");\nint batchSize = 10000;\nlong interval = 1000;\nString schema = \"iceberg_db\";\nChangoClient changoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval\n);\nlong count = 0;\nwhile (true) {\nint MAX = 50 * 1000;\nfor (int i = 0; i &lt; MAX; i++) {\nMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();\nDateTime dt = DateTime.now();\nString year = String.valueOf(dt.getYear());\nString month = padZero(dt.getMonthOfYear());\nString day = padZero(dt.getDayOfMonth());\nlong ts = dt.getMillis(); // in milliseconds.\nmap.put(\"level\", \"INFO\");\nmap.put(\"message\", \"any log message ... [\" + count + \"]\");\nmap.put(\"ts\", ts);\nmap.put(\"year\", year);\nmap.put(\"month\", month);\nmap.put(\"day\", day);\nString json = JsonUtils.toJson(map);\ntry {\n// send json.\nchangoClient.add(json);\ncount++;\n} catch (Exception e) {\nLOG.error(e.getMessage());\n// reconstruct chango client.\nchangoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval\n);\nLOG.info(\"Chango client reconstructed.\");\nThread.sleep(1000);\n}\n}\nThread.sleep(10 * 1000);\nLOG.info(\"log [{}] sent...\", count);\n}\n}\nprivate String padZero(int value) {\nString strValue = String.valueOf(value);\nif(strValue.length() == 1) {\nstrValue = \"0\" + strValue;\n}\nreturn strValue;\n}\n}\n</code></pre> <p>Take a look at the value of <code>ts</code> must be the number of milliseconds since 1970-01-01 00:00:00.</p> <p>If you want to ingest events to Chango transactionally, you need to use transactional chango client like this.</p> <pre><code>        ChangoClient changoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval,\ntransactional\n);\n</code></pre> <p>Before you use transactional chango client, you need to install transactional chango streaming.</p>"},{"location":"user-guide/streaming/#run-query-in-iceberg-table-for-streaming-events","title":"Run Query in Iceberg Table for Streaming Events","text":"<p>You can run queries in iceberg table <code>logs</code> to which streaming events are inserted.</p> <pre><code>-- select with partitioning columns.\nselect *, from_unixtime(ts/1000) from iceberg.iceberg_db.logs where year = '2023' and month = '11' and day = '07' limit 1000;\n</code></pre> <p></p>"},{"location":"user-guide/tx-streaming/","title":"Transactional Streaming","text":"<p>Chango Streaming Tx is a spark streaming job  to support exactly-once delivery in contrast to  Chango Streaming which supports at-most once delivery.</p> <p><code>Chango Streaming Tx</code> like <code>Chango Streaming</code> is a streaming consumer and works with <code>Chango Data API</code> which is a streaming producer. So, the combination of <code>Chango Streaming Tx</code> like <code>Chango Streaming</code> and <code>Chango Data API</code> is called <code>Chango Ingestion</code>. </p> <p>Chango provides the following tools as chango clients to ingest messages to Chango quickly and easily.</p> <ul> <li>Chango Client</li> <li>Chango CLI</li> <li>Chango Log</li> <li>Chango CDC</li> </ul> <p>These chango clients can send messages which will be saved to iceberg tables transactionally in Chango.</p>"},{"location":"user-guide/tx-streaming/#use-transactional-chango-client-api","title":"Use Transactional Chango Client API","text":"<p>You need to use the following Chango Client API in java codes to send messages transactionally.</p> <pre><code>        ChangoClient changoClient = new ChangoClient(\ntoken,\ndataApiServer,\nschema,\ntable,\nbatchSize,\ninterval,\ntransactional\n);\n</code></pre>"},{"location":"user-guide/tx-streaming/#configure-chango-cli","title":"Configure Chango CLI","text":"<p>You need to add <code>--tx</code> argument to Chango CLI to upload JSON files transactionally.</p> <pre><code>chango upload json local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--file /home/chango/multi-line-json.json \\\n--batch-size 150000 \\\n--tx \\\n;\n</code></pre>"},{"location":"user-guide/tx-streaming/#configure-chango-log","title":"Configure Chango Log","text":"<p>Configure <code>conf/configuration.yml</code> with setting <code>tx</code> to <code>true</code> to send logs transactionally in <code>Chango Log</code>.</p> <pre><code>chango:\n  token: any-chango-credential\n  dataApiUrl: http://any-data-api-endpoint\n  schema: logs_db\n  table: logs\n  batchSize: 10000\n  interval: 1000\n  tx: true\n</code></pre>"},{"location":"user-guide/tx-streaming/#configure-chango-cdc","title":"Configure Chango CDC","text":"<p>Configure <code>conf/configuration.yml</code> with setting <code>tx</code> to <code>true</code> to send CDC data transactionally in <code>Chango CDC</code>.</p> <pre><code>chango:\n  token: any-chango-credential\n  dataApiUrl: http://any-data-api-endpoint\n  schema: cdc_db\n  table: student\n  batchSize: 10000\n  interval: 1000\n  tx: true\n</code></pre>"},{"location":"user-guide/upload-excel/","title":"Upload Excel","text":"<p>You can upload Excel files with either Chango CLI or Admin UI.</p> <p>This example will show how to upload Excel files with Admin UI.</p> <p>NOTE: Before uploading Excel, make sure that <code>Chango Data API</code> and <code>Chango Straming</code> are installed.</p>"},{"location":"user-guide/upload-excel/#get-chango-data-api-endpoint","title":"Get Chango Data API Endpoint","text":"<p>You need to get the endpoint of <code>Chango Data API</code> in <code>Components</code> -&gt; <code>Chango Data API</code>. </p> <p></p>"},{"location":"user-guide/upload-excel/#get-chango-credential","title":"Get Chango Credential","text":"<p>You need to get Chango Credential to have privileges to upload Excel file. Go to <code>Settings</code> -&gt; <code>Security</code>.</p> <p>First, create a role <code>upload-excel</code>.</p> <p></p> <p>Crate a credential for the role <code>upload-excel</code>.</p> <p></p> <p>Create privileges with storage path <code>iceberg.excel_db.*</code> for the role <code>upload-excel</code>.</p> <p> </p>"},{"location":"user-guide/upload-excel/#upload-excel-file","title":"Upload Excel File","text":"<p>Before uploading Excel file, make sure that the header columns exist in the Excel file. Go to <code>Upload Files</code> -&gt; <code>Upload Excel</code>.</p> <p>Assumed that you want to insert uploaded Excel rows into the table <code>test_excel</code> in schema <code>excel_db</code> of catalog <code>iceberg</code>,  Enter the values as follows.</p> <p></p> <p>To select rows of uploaded Excel, run the query with Superset like this.</p> <pre><code>-- select rows in uploaded excel.\nselect * from iceberg.excel_db.test_excel;\n</code></pre> <p></p>"},{"location":"user-guide/upload/","title":"Upload Files","text":"<p>Using <code>Chango CLI</code>, CSV, JSON, and Excel Files can be uploaded whose data will be inserted to Iceberg table in Chango automatically.</p> <p>NOTE: Before uploading files, make sure that <code>Chango Data API</code> and <code>Chango Straming</code> are installed.</p>"},{"location":"user-guide/upload/#install-chango-cli","title":"Install Chango CLI","text":"<p>Download chango client jar and install it as Chango CLI. <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-client/releases/download/2.0.2/chango-client-2.0.2-executable.jar;\ncp chango-client-2.0.2-executable.jar ~/bin/chango;\nchmod +x ~/bin/chango;\n</code></pre></p>"},{"location":"user-guide/upload/#initialize-chango-cli","title":"Initialize Chango CLI","text":"<p>Credential needs to be entered when Chango CLI is initialized. To Get Chango Credential, see Get Chango Credential. <pre><code>chango init\n</code></pre></p> <p>After credential entered, output looks like this. <pre><code>chango init\n\nEnter Token: xxxxxxxx\nInitialization success!\n</code></pre></p>"},{"location":"user-guide/upload/#upload-json","title":"Upload JSON","text":"<p>You can upload individual JSON file or all the JSON files in directory.</p> <p>NOTE: Before uploading JSON files, target Iceberg table needs to be created beforehand.  See here for more details.</p> <p>Upload JSON file.</p> <pre><code>chango upload json local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--file /home/chango/multi-line-json.json \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>schema</code> : iceberg schema created before.</li> <li><code>table</code>: iceberg table where json data will be ingested in chango.</li> <li><code>file</code> : local json file path.</li> <li><code>batch-size</code> : list of json in gzip will be sent in batch mode. The size of json list.</li> </ul> <p>Upload all JSON files in directory.</p> <pre><code>chango upload json local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--directory /home/chango/json-files \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>directory</code>: the directory where json files are located which will be sent to chango.</li> </ul> <p><code>[endpoint-of-data-api]</code> is the endpoint of <code>Chango Data API</code>, go to <code>Components</code> -&gt; <code>Chango Data API</code>, and get URL in <code>Endpoint</code> section.</p> <p>If you want to upload json transactionally, you need to add <code>--tx</code> parameter.</p> <pre><code>chango upload json local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--file /home/chango/multi-line-json.json \\\n--batch-size 150000 \\\n--tx \\\n;\n</code></pre>"},{"location":"user-guide/upload/#upload-excel","title":"Upload Excel","text":"<p>You don't have to create Iceberg table before uploading Excel to chango. Chango will create Iceberg table automatically.</p> <p>NOTE: Take a note that the header of Excel must exist.</p> <p>You can also create Iceberg table(for example, Iceberg table with the definition of partition columns) before uploading Excel to chango.</p> <pre><code>chango upload excel local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table excel_to_json \\\n--file /home/chango/data/excel-to-json.xlsx \\\n;\n</code></pre>"},{"location":"user-guide/upload/#upload-csv","title":"Upload CSV","text":"<p>As like Excel, you don\u2019t have to create Iceberg table beforehand. Chango will create Iceberg table automatically.</p> <p>NOTE: Take a note that the header of CSV must exist.</p> <p>You can also define a Iceberg table(for example, Iceberg table with the definition of partition columns) before uploading CSV to chango.</p> <p>The following example is to upload CSV with the separator of comma.</p> <pre><code>chango upload csv local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--file /home/chango/data/csv-to-json-comma.csv \\\n;\n</code></pre> <ul> <li><code>separator</code> is the separator of csv data. If csv data is tab separated, the value is <code>TAB</code> . But for another separators, you need to type a separator value, for instance,  <code>,</code> , <code>|</code> , or something else.</li> <li><code>is-single-quote</code> is if the escaped value of csv data is single quoted or not. Default is <code>false</code>.</li> </ul> <p>For tab separated CSV, use the following.</p> <pre><code>chango upload csv local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table csv_to_json_tab \\\n--separator TAB \\\n--is-single-quote false \\\n--file /home/chango/data/csv-to-json-tab.csv \\\n;\n</code></pre> <ul> <li>parameter <code>separator</code> value must be <code>TAB</code>.</li> </ul> <p>You can upload multiple CSV files located in the directory.</p> <pre><code>chango upload csv local \\\n--data-api-server [endpoint-of-data-api] \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--directory /home/chango/local-csvs \\\n;\n</code></pre>"}]}